{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAVENT DONE AT AND ATAE MODELS \n",
    "# ALSO 3 CHANNEL AND 5 CHANNEL CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from Attention import Attention\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Embedding, LSTM,Bidirectional,Dense\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from skopt.plots import plot_histogram, plot_objective_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skopt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = Real(low=0.2, high=0.9,\n",
    "                         name='dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_num_hidden_layers = Integer(low=1, high=5, name='num_hidden_layers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_batch_size = Categorical(categories=[8,16,32,64],name='batch_size')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_num_dense_nodes = Integer(low=5, high=512, name='num_dense_nodes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out = Integer(low=100, high=400, name='lstm_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_activation = Categorical(categories=['relu', 'sigmoid','tanh'],name='activation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [dim_learning_rate,\n",
    "              dim_num_hidden_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation,\n",
    "              dropout,\n",
    "              para_batch_size,\n",
    "              lstm_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_parameters = [0.0006973763486468701, 1, 300, 'relu',0.2,8,300]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pickle.load(open(\"D:/PythonCodes/Sentiment-Analysis/Data/all_data.dat\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_sentiment', 'h_aspect_encoding', 'sentence', 'p_sentiment', 'lable_encoding', 'h_target', 'p_aspect', 'p_aspect_encoding', 'vocab_size', 'embedding_matrix', 'aspect', 'p_target', 'target', 'h_sentence', 'h_sentiment', 'h_aspect', 'input_aspect', 'p_sentence'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX= a['sentence']\n",
    "aspect=a['aspect']\n",
    "test_headline_X=a['h_sentence']\n",
    "test_post_X=a['p_sentence']\n",
    "h_aspect=a['h_aspect']\n",
    "p_aspect=a['p_aspect']\n",
    "label_encoding=a['lable_encoding']\n",
    "h_target=a['h_target']\n",
    "target=a['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_pred(pred):\n",
    "    return [label_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_encoded = get_class_from_pred(aspect)\n",
    "h_aspect_encoding=a['h_aspect_encoding']\n",
    "p_aspect_encoding=a['p_aspect_encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=11\n",
    "vocab_size=a['vocab_size']\n",
    "embedding_matrix=a['embedding_matrix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Model+Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(learning_rate,dropout,lstm_out,num_hidden_layers,num_dense_nodes,em,em_trainable_flag,em_dim,activation):\n",
    "     \n",
    "#   model=Sequential()\n",
    "#    model.add(Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag))\n",
    "#    model.add(Bidirectional(LSTM(lstm_out, return_sequences=True,recurrent_dropout=dropout)))\n",
    "#    model.add(Attention())\n",
    "    \n",
    "#    for i in range(num_hidden_layers):\n",
    "#        model.add(Dense(num_dense_nodes,activation=activation))\n",
    "        \n",
    "#    model.add(Dense(27,activation='softmax'))\n",
    "\n",
    "\n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "        \n",
    "    a = Bidirectional(LSTM(300, return_sequences=True,recurrent_dropout=dropout))(context)\n",
    "    \n",
    "    #print(a.shape)\n",
    "\n",
    "    alpha = Attention()(a)\n",
    "    \n",
    "    x=Dense(int((2*lstm_out+27)/2),activation='relu')(alpha)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "    \n",
    "    model= Model(inputs=input_context ,outputs=out) \n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_best_model = 'our_model_attention.keras'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = dict()\n",
    "key=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=parameters)\n",
    "\n",
    "def fitness(learning_rate,num_hidden_layers,num_dense_nodes,activation,dropout,batch_size,lstm_out):\n",
    "        global key\n",
    "        global record\n",
    "\n",
    "        # Print the hyper-parameters.\n",
    "        print('-----------------------------combination no={0}------------------'.format(key))\n",
    "        print('learning rate===> '.format(learning_rate))\n",
    "        print('num_hidden_layers:', num_hidden_layers)\n",
    "        print('num_dense_nodes:', num_dense_nodes)\n",
    "        print('activation:', activation)\n",
    "        print('lstm_out:', lstm_out)\n",
    "        print('dropout===>',dropout)\n",
    "        print('batch_size===> ',batch_size)\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "        acc=[]\n",
    "        f1_head=[]\n",
    "        f1_post=[]\n",
    "        \n",
    "        #for train,test in kfold.split(dataX,aspect):\n",
    "        model = define_model(learning_rate=learning_rate,\n",
    "                             num_hidden_layers=num_hidden_layers,\n",
    "                             num_dense_nodes=num_dense_nodes,\n",
    "                             activation=activation,\n",
    "                             lstm_out=lstm_out,\n",
    "                             dropout=dropout,\n",
    "                             em='embedding_matrix',\n",
    "                             em_trainable_flag=False,\n",
    "                             em_dim=300)\n",
    "        \n",
    "        #graph = tf.get_default_graph()\n",
    "    \n",
    "\n",
    "        history = model.fit(x=dataX,\n",
    "                            y=aspect,\n",
    "                            epochs=1,\n",
    "                            batch_size=batch_size\n",
    "                            )\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        accuracy = history.history['acc'][-1]\n",
    "\n",
    "        # Print the classification accuracy.\n",
    "        print()\n",
    "        print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        _,accuracy_1= model.evaluate(x=test_headline_X , y=h_aspect_encoding)\n",
    "\n",
    "        pred1 = model.predict(test_headline_X)\n",
    "        pred_class = get_class_from_pred(pred1)\n",
    "        f1_1=f1_score(h_aspect,pred_class,average='macro')\n",
    "\n",
    "        _,accuracy_2 = model.evaluate(x=test_post_X , y=p_aspect_encoding)\n",
    "\n",
    "\n",
    "        pred2= model.predict(test_post_X)\n",
    "        pred_class = get_class_from_pred(pred2)\n",
    "        f1_2=f1_score(p_aspect,pred_class,average='macro')\n",
    "\n",
    "        print(\"Test Headline\")\n",
    "        print(\"F1 Score: \",f1_1)\n",
    "        print(\"Accuracy: \",accuracy_1)\n",
    "        print()\n",
    "\n",
    "        print(\"Test Post\")\n",
    "        print(\"F1 Score: \",f1_2)\n",
    "        print(\"Accuracy: \",accuracy_2)\n",
    "        print()\n",
    "\n",
    "\n",
    "        f1_head.append(f1_1)\n",
    "        f1_post.append(f1_2)\n",
    "\n",
    "        ###########################\n",
    "\n",
    "        record[key] = {'parameters':[learning_rate,num_hidden_layers,num_dense_nodes,activation,dropout,batch_size,lstm_out],\n",
    "               'accuracy_Train':accuracy,\n",
    "               'f1_score_headlines':f1_1,\n",
    "               'f1_score_post':f1_2,\n",
    "               'accuracy_Headline_Test':accuracy_1,\n",
    "               'accuracy_Post_Test':accuracy_2,\n",
    "\n",
    "              }\n",
    "\n",
    "        pickle.dump(record, open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Data\\record_parameters.dat\",\"wb\"))\n",
    "\n",
    "        \n",
    "        key+=1\n",
    "\n",
    "        del model\n",
    "        \n",
    "        print(\"model deleted\")\n",
    "\n",
    "        K.clear_session()\n",
    "        \n",
    "        print(\"Session cleared \")\n",
    "        \n",
    "        return -accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#fitness(x=default_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------combination no=0------------------\n",
      "learning rate===> \n",
      "num_hidden_layers: 1\n",
      "num_dense_nodes: 300\n",
      "activation: relu\n",
      "lstm_out: 300\n",
      "dropout===> 0.2\n",
      "batch_size===>  8\n",
      "\n",
      "Epoch 1/1\n",
      "1173/1173 [==============================] - 23s 20ms/step - loss: 2.1885 - acc: 0.4339\n",
      "\n",
      "Accuracy: 43.39%\n",
      "\n",
      "93/93 [==============================] - 1s 7ms/step\n",
      "64/99 [==================>...........] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Headline\n",
      "F1 Score:  0.06690439139865055\n",
      "Accuracy:  0.03225806447607215\n",
      "\n",
      "Test Post\n",
      "F1 Score:  0.18065268065268067\n",
      "Accuracy:  0.0\n",
      "\n",
      "model deleted\n",
      "Session cleared \n",
      "-----------------------------combination no=1------------------\n",
      "learning rate===> \n",
      "num_hidden_layers: 3\n",
      "num_dense_nodes: 134\n",
      "activation: tanh\n",
      "lstm_out: 389\n",
      "dropout===> 0.6150142450871858\n",
      "batch_size===>  16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = search_result.space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space.point_to_dict(search_result.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result.fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+LSTM_ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para_filter_size = Integer(low=1,high=6,name = 'filter_size')\n",
    "para_n_filters = Categorical(categories=[100,200,300,400],name='n_filters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default_parameters_2=[0.0006973763486468701, 1, 300, 'relu', 0.2, 8, 300,3,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters_2=[dim_learning_rate,\n",
    "              dim_num_hidden_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation,\n",
    "              dropout,\n",
    "              para_batch_size,\n",
    "              lstm_out,\n",
    "              para_filter_size,\n",
    "              para_n_filters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def define_model_1(learning_rate,dropout,lstm_out,num_hidden_layers,num_dense_nodes,em,em_trainable_flag,em_dim,activation,n_filters,kernel_size):\n",
    "     \n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv1D(n_filters,kernel_size=3,activation=activation))\n",
    "    model.add(LSTM(lstm_out, return_sequences=True,recurrent_dropout=dropout))\n",
    "    model.add(Attention())\n",
    "    \n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(num_dense_nodes,activation=activation))\n",
    "        \n",
    "    model.add(Dense(27,activation='softmax'))\n",
    "        \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_best_model = 'cnn_lstm_attention.keras'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "record = dict()\n",
    "key=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_class_from_pred(pred):\n",
    "    return [label_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@use_named_args(dimensions=parameters_2)\n",
    "def fitness(learning_rate,num_hidden_layers,num_dense_nodes,activation,dropout,batch_size,epoch,lstm_out,filter_size,n_filters):\n",
    "    global key\n",
    "    global record\n",
    "    \n",
    "    # Print the hyper-parameters.\n",
    "    print('-----------------------------combination no={0}------------------'.format(key))\n",
    "    print('learning rate===> '.format(learning_rate))\n",
    "    print('num_hidden_layers:', num_hidden_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    print('lstm_out:', lstm_out)\n",
    "    print('dropout===>'.format(dropout))\n",
    "    print('batch_size===> ',batch_size)\n",
    "    print('filter_size===> ',filter_size)\n",
    "    print('num_filters===> 'n_filters)\n",
    "\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    acc=[]\n",
    "    f1_Score=[]\n",
    "    for train,test in kfold.split(dataX,aspect):\n",
    "        model = define_model_1(learning_rate=learning_rate,\n",
    "                             num_hidden_layers=num_hidden_layers,\n",
    "                             num_dense_nodes=num_dense_nodes,\n",
    "                             activation=activation,\n",
    "                             lstm_out=lstm_out,\n",
    "                             dropout=dropout,\n",
    "                             em='embedding_matrix',\n",
    "                             em_trainable_flag=False,\n",
    "                             em_dim=300,\n",
    "                             n_filters=n_filters,\n",
    "                             kernel_size=filter_size)\n",
    "\n",
    "  \n",
    "   \n",
    "        history = model.fit(x=dataX,\n",
    "                            y=aspect,\n",
    "                            epochs=10,\n",
    "                            batch_size=batch_size,\n",
    "                            )\n",
    "\n",
    "        pred1 = model.predict(test_headline_X)\n",
    "        pred_class = get_class_from_pred(pred1)\n",
    "        f1=f1_score(h_aspect,pred_class,average='macro')\n",
    "        \n",
    "        accuracy = history.history['acc'][-1]\n",
    "\n",
    "        # Print the classification accuracy.\n",
    "        print()\n",
    "        print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "        print(\"F1 Score: \",f1)\n",
    "        print()\n",
    "        \n",
    "        f1_Score.append(f1)\n",
    "        acc.append(accuracy)\n",
    "        \n",
    "        record[key] = {'parameters':[learning_rate,num_hidden_layers,num_dense_nodes,activation,dropout,batch_size,lstm_out,filter_size,n_filters],'f1_score':f1,'accuracy':accuracy}\n",
    "        \n",
    "        with open('record.json', 'w') as fout:\n",
    "               json.dump(record,fout,indent=4)\n",
    "    \n",
    "        key+=1\n",
    "    \n",
    "        del model\n",
    "\n",
    "        K.clear_session()\n",
    "        return -accuracy\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed=7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitness(x=default_parameters_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters_2,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space = search_result.space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space.point_to_dict(search_result.x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search_result.fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+ Bi Directional LSTM_ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para_filter_size = Integer(low=1,high=6,name = 'filter_size')\n",
    "para_n_filters = Categorical(categories=[100,200,300,400],name='n_filters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default_parameters_2=[0.0006973763486468701, 1, 300, 'relu', 0.2, 8, 10, 300,3,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters_2=[dim_learning_rate,\n",
    "              dim_num_hidden_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation,\n",
    "              dropout,\n",
    "              para_batch_size,\n",
    "              para_epoch,\n",
    "              lstm_out,\n",
    "              para_filter_size,\n",
    "              para_n_filters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def define_model_2(learning_rate,dropout,lstm_out,num_hidden_layers,num_dense_nodes,em,em_trainable_flag,em_dim,activation,n_filters,kernel_size):\n",
    "     \n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv1D(n_filters,kernel_size=3,activation=activation))\n",
    "    model.add(Bidirectional(LSTM(lstm_out, return_sequences=True,recurrent_dropout=dropout)))\n",
    "    model.add(Attention())\n",
    "    \n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(num_dense_nodes,activation=activation))\n",
    "        \n",
    "    model.add(Dense(27,activation='softmax'))\n",
    "        \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_best_model = 'cnn_lstm_attention.keras'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "record = dict()\n",
    "key=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_class_from_pred(pred):\n",
    "    return [label_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@use_named_args(dimensions=parameters_2)\n",
    "def fitness(learning_rate,num_hidden_layers,num_dense_nodes,activation,dropout,batch_size,epoch,lstm_out,filter_size,n_filters):\n",
    "    global key\n",
    "    global record\n",
    "    \n",
    "    # Print the hyper-parameters.\n",
    "    print('-----------------------------combination no={0}------------------'.format(key))\n",
    "    print('learning rate===> '.format(learning_rate))\n",
    "    print('num_hidden_layers:', num_hidden_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    print('lstm_out:', lstm_out)\n",
    "    print('dropout===>'.format(dropout))\n",
    "    print('batch_size===> ',batch_size)\n",
    "    print('epoch===> ',epoch)\n",
    "    print('filter_size===> ',filter_size)\n",
    "    print('num_filters===> 'n_filters)\n",
    "\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    acc=[]\n",
    "    f1_Score=[]\n",
    "    for train,test in kfold.split(dataX,aspect):\n",
    "        model = define_model_2(learning_rate=learning_rate,\n",
    "                             num_hidden_layers=num_hidden_layers,\n",
    "                             num_dense_nodes=num_dense_nodes,\n",
    "                             activation=activation,\n",
    "                             lstm_out=lstm_out,\n",
    "                             dropout=dropout,\n",
    "                             em='embedding_matrix',\n",
    "                             em_trainable_flag=False,\n",
    "                             em_dim=300,\n",
    "                             n_filters=n_filters,\n",
    "                             kernel_size=filter_size)\n",
    "\n",
    "  \n",
    "   \n",
    "        history = model.fit(x=dataX,\n",
    "                            y=aspect,\n",
    "                            epochs=epoch,\n",
    "                            batch_size=batch_size,\n",
    "                            )\n",
    "\n",
    "        pred1 = model.predict(test_headline_X)\n",
    "        pred_class = get_class_from_pred(pred1)\n",
    "        f1=f1_score(h_aspect,pred_class,average='macro')\n",
    "        \n",
    "        accuracy = history.history['val_acc'][-1]\n",
    "\n",
    "        # Print the classification accuracy.\n",
    "        print()\n",
    "        print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "        print(\"F1 Score: \",f1)\n",
    "        print()\n",
    "        \n",
    "        f1_Score.append(f1)\n",
    "        acc.append(accuracy)\n",
    "        \n",
    "        record[key] = {'parameters':[learning_rate,num_hidden_layers,num_dense_nodes,activation,dropout,batch_size,epoch,lstm_out,filter_size,n_filters],'f1_score':f1,'accuracy':accuracy}\n",
    "        \n",
    "        with open('record.json', 'w') as fout:\n",
    "               json.dump(record,fout,indent=4)\n",
    "    \n",
    "        key+=1\n",
    "    \n",
    "        del model\n",
    "\n",
    "        K.clear_session()\n",
    "        return -accuracy\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed=7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitness(x=default_parameters_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters_2,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space = search_result.space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space.point_to_dict(search_result.x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search_result.fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def define_model_3(learning_rate,dropout,lstm_out,em,em_trainable_flag, em_dim,activation,num_dense_nodes):\n",
    "    \n",
    "    input_context = Input(shape=(max_length,),name='Context')\n",
    "    input_target = Input(shape=(max_length,),name='Target')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "\n",
    "    context=embedding(input_context)\n",
    "    target= embedding(input_target)\n",
    "    \n",
    " \n",
    "    H_c , _ , _ = LSTM( lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"LSTM_C\")(context)\n",
    "    H_t , _ , _ = LSTM( lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"LSTM_T\")(target)\n",
    "\n",
    "    \n",
    "    c_avg = GlobalAveragePooling1D(name='POOL_C')(H_c)  \n",
    "    t_avg = GlobalAveragePooling1D(name='POOL_T')(H_t)\n",
    "    \n",
    "\n",
    "    #c_r = Lambda(lambda x: one_step_attention(x[0],x[1],11))([H_c,t_avg])\n",
    "    #t_r = Lambda(lambda x: one_step_attention(x[0],x[1],11))([H_t,c_avg])\n",
    "    \n",
    "    c_ = RepeatVector(11)(t_avg)\n",
    "    c_ = concatenator([H_c, c_])\n",
    "    c_ = densor(c_)\n",
    "    c_ = activator(c_)\n",
    "    c = dotor([c_, H_c])    \n",
    "    c_r= Lambda(lambda x: K.sum(x, axis=1))(c)\n",
    "    \n",
    "    t_ = RepeatVector(11)(c_avg)\n",
    "    t_ = concatenator([H_t, t_])\n",
    "    t_ = densor(t_)\n",
    "    t_ = activator(t_)\n",
    "    t = dotor([t_, H_t])    \n",
    "    t_r= Lambda(lambda x: K.sum(x, axis=1))(t)\n",
    "\n",
    "    d = concatenate(inputs=[c_r , t_r])\n",
    "    \n",
    "    \n",
    "    x=Dense(num_dense_nodes,activation=activation)(d)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "   \n",
    "    IAN_model= Model(inputs=[input_context,input_target],outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    IAN_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "    return  IAN_model\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_best_model = 'ian_model.keras'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "record = dict()\n",
    "key=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_class_from_pred(pred):\n",
    "    return [label_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@use_named_args(dimensions=parameters)\n",
    "def fitness(learning_rate,num_hidden_layers,num_dense_nodes,activation,dropout,batch_size,epoch,lstm_out):\n",
    "    global key\n",
    "    global record\n",
    "    \n",
    "    # Print the hyper-parameters.\n",
    "    print('-----------------------------combination no={0}------------------'.format(key))\n",
    "    print('learning rate===> '.format(learning_rate))\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    print('lstm_out:', lstm_out)\n",
    "    print('dropout===>'.format(dropout))\n",
    "    print('batch_size===> ',batch_size)\n",
    "    print('epoch===> ',epoch)\n",
    "\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    acc=[]\n",
    "    f1_Score=[]\n",
    "    for train,test in kfold.split(dataX,aspect):\n",
    "        model = define_model_3(learning_rate=learning_rate,\n",
    "                             num_dense_nodes=num_dense_nodes,\n",
    "                             activation=activation,\n",
    "                             lstm_out=lstm_out,\n",
    "                             dropout=dropout,\n",
    "                             em='embedding_matrix',\n",
    "                             em_trainable_flag=False,\n",
    "                             em_dim=300)\n",
    "\n",
    "  \n",
    "   \n",
    "        history = model.fit(x=[dataX,target],\n",
    "                            y=aspect,\n",
    "                            epochs=epoch,\n",
    "                            batch_size=batch_size,\n",
    "                            )\n",
    "\n",
    "        pred1 = model.predict([test_headline_X,h_target])\n",
    "        pred_class = get_class_from_pred(pred1)\n",
    "        f1=f1_score(h_aspect,pred_class,average='macro')\n",
    "        \n",
    "        accuracy = history.history['val_acc'][-1]\n",
    "\n",
    "        # Print the classification accuracy.\n",
    "        print()\n",
    "        print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "        print(\"F1 Score: \",f1)\n",
    "        print()\n",
    "        \n",
    "        f1_Score.append(f1)\n",
    "        acc.append(accuracy)\n",
    "        \n",
    "        record[key] = {'parameters':[learning_rate,num_dense_nodes,activation,dropout,batch_size,epoch,lstm_out],'f1_score':f1,'accuracy':accuracy}\n",
    "        \n",
    "        with open('record.json', 'w') as fout:\n",
    "               json.dump(record,fout,indent=4)\n",
    "    \n",
    "        key+=1\n",
    "    \n",
    "        del model\n",
    "\n",
    "        K.clear_session()\n",
    "        return -accuracy\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed=7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitness(x=default_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space = search_result.space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space.point_to_dict(search_result.x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search_result.fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ian + Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def define_model_4(learning_rate,dropout,lstm_out,em,em_trainable_flag, em_dim, num_dense_nodes,activation):\n",
    "    \n",
    "    input_context = Input(shape=(max_length,),name='Context')\n",
    "    input_target = Input(shape=(max_length,),name='Target')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "\n",
    "    context=embedding(input_context)\n",
    "    target= embedding(input_target)\n",
    "    \n",
    " \n",
    "    H_c , _ , _,_,_ = Bidirectional(LSTM( lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"LSTM_C\"))(context)\n",
    "    H_t , _ , _ ,_,_= Bidirectional(LSTM( lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"LSTM_T\"))(target)\n",
    "\n",
    "    \n",
    "    c_avg = GlobalAveragePooling1D(name='POOL_C')(H_c)  \n",
    "    t_avg = GlobalAveragePooling1D(name='POOL_T')(H_t)\n",
    "    \n",
    "\n",
    "    #c_r = Lambda(lambda x: one_step_attention(x[0],x[1],11))([H_c,t_avg])\n",
    "    #t_r = Lambda(lambda x: one_step_attention(x[0],x[1],11))([H_t,c_avg])\n",
    "    \n",
    "    c_ = RepeatVector(11)(t_avg)\n",
    "    c_ = concatenator([H_c, c_])\n",
    "    c_ = densor(c_)\n",
    "    c_ = activator(c_)\n",
    "    c = dotor([c_, H_c])    \n",
    "    c_r= Lambda(lambda x: K.sum(x, axis=1))(c)\n",
    "    \n",
    "    t_ = RepeatVector(11)(c_avg)\n",
    "    t_ = concatenator([H_t, t_])\n",
    "    t_ = densor(t_)\n",
    "    t_ = activator(t_)\n",
    "    t = dotor([t_, H_t])    \n",
    "    t_r= Lambda(lambda x: K.sum(x, axis=1))(t)\n",
    "\n",
    "    d = concatenate(inputs=[c_r , t_r])\n",
    "    \n",
    "    \n",
    "    x=Dense(num_dense_nodes,activation=activation)(d)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "   \n",
    "    IAN_model= Model(inputs=[input_context,input_target],outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    IAN_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "    return  IAN_model\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_best_model = 'ian_bilistm_model.keras'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "record = dict()\n",
    "key=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_class_from_pred(pred):\n",
    "    return [label_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@use_named_args(dimensions=parameters)\n",
    "def fitness(learning_rate,num_hidden_layers,num_dense_nodes,activation,dropout,batch_size,epoch,lstm_out):\n",
    "    global key\n",
    "    global record\n",
    "    \n",
    "    # Print the hyper-parameters.\n",
    "    print('-----------------------------combination no={0}------------------'.format(key))\n",
    "    print('learning rate===> '.format(learning_rate))\n",
    "#    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    print('lstm_out:', lstm_out)\n",
    "    print('dropout===>'.format(dropout))\n",
    "    print('batch_size===> ',batch_size)\n",
    "    print('epoch===> ',epoch)\n",
    "\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    acc=[]\n",
    "    f1_Score=[]\n",
    "    for train,test in kfold.split(dataX,aspect):\n",
    "        model = define_model_4(learning_rate=learning_rate,\n",
    "                             num_dense_nodes=num_dense_nodes,\n",
    "                             activation=activation,\n",
    "                             lstm_out=lstm_out,\n",
    "                             dropout=dropout,\n",
    "                             em='embedding_matrix',\n",
    "                             em_trainable_flag=False,\n",
    "                             em_dim=300)\n",
    "\n",
    "  \n",
    "   \n",
    "        history = model.fit(x=[dataX,target],\n",
    "                            y=aspect,\n",
    "                            epochs=epoch,\n",
    "                            batch_size=batch_size,\n",
    "                            )\n",
    "\n",
    "        pred1 = model.predict([test_headline_X,h_target])\n",
    "        pred_class = get_class_from_pred(pred1)\n",
    "        f1=f1_score(h_aspect,pred_class,average='macro')\n",
    "        \n",
    "        accuracy = history.history['val_acc'][-1]\n",
    "\n",
    "        # Print the classification accuracy.\n",
    "        print()\n",
    "        print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "        print(\"F1 Score: \",f1)\n",
    "        print()\n",
    "        \n",
    "        f1_Score.append(f1)\n",
    "        acc.append(accuracy)\n",
    "        \n",
    "        record[key] = {'parameters':[learning_rate,num_dense_nodes,activation,dropout,batch_size,epoch,lstm_out],'f1_score':f1,'accuracy':accuracy}\n",
    "        \n",
    "        with open('record.json', 'w') as fout:\n",
    "               json.dump(record,fout,indent=4)\n",
    "    \n",
    "        key+=1\n",
    "    \n",
    "        del model\n",
    "\n",
    "        K.clear_session()\n",
    "        return -accuracy\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed=7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitness(x=default_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space = search_result.space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space.point_to_dict(search_result.x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search_result.fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
