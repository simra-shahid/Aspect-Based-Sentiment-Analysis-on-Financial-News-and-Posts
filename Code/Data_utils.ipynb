{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "#from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk import pos_tag\n",
    "from string import punctuation,digits\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from keras.utils import to_categorical \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    list_punctuation = list(punctuation)\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,'')\n",
    "    return s\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    sentence = sentence.replace('#',' ')\n",
    "    sentence = sentence.replace(\"'s\",' ')\n",
    "    sentence = sentence.replace(\"-\",' ')\n",
    "    tokens = sentence.split()\n",
    "    tokens = [remove_punctuation(w) for w in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    tokens = [w.translate(remove_digits) for w in tokens]\n",
    "    tokens = [w.strip() for w in tokens]\n",
    "    tokens = [w for w in tokens if w!=\"\"]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_google_word2vec(file_name):\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)\n",
    "\n",
    "\n",
    "def build_embedding_matrix(vocab_size, embed_dim,tokenizer):\n",
    "    \n",
    "    embedding_matrix_file_name='Finance_embedding_matrix_1.dat'\n",
    "    if os.path.exists(embedding_matrix_file_name):\n",
    "        print('loading embedding_matrix:', embedding_matrix_file_name)\n",
    "        embedding_matrix = pickle.load(open(embedding_matrix_file_name, 'rb'))\n",
    "    else:\n",
    "        print('loading word vectors...')\n",
    "        fname = 'D:\\Jupyter notebooks\\Word Embeddings\\GoogleNews-vectors-negative300.bin' \n",
    "        model=load_google_word2vec(fname)\n",
    "        embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            try:\n",
    "                embedding_vector = model[word]\n",
    "            except KeyError:\n",
    "                embedding_vector = None\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i]=embedding_vector\n",
    "        \n",
    "        pickle.dump(embedding_matrix, open(embedding_matrix_file_name, 'wb'))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing Finance dataset...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_data(fname):\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            foo = json.load(f)\n",
    "            sentence_l=[]\n",
    "            target_l=[]\n",
    "            aspect_l=[]\n",
    "            sentiment_l=[]\n",
    "            for key in foo.keys():\n",
    "                for info in foo[key]['info']:\n",
    "                    sentence=foo[key]['sentence']\n",
    "                                #print(sentence)\n",
    "                    sentence = [clean_sentence(x) for x in sentence.split(\" \")]\n",
    "                                #print(sentence)\n",
    "                    sentence=' '.join(sentence)\n",
    "                                #print(sentence)\n",
    "                    target= info['target'].lower()\n",
    "                    \n",
    "                    sentiment_score = info['sentiment_score']\n",
    "                                    #print(sentiment_score)\n",
    "                    aspect= info['aspects']\n",
    "                                    #print(\"Aspect \"+ aspect)\n",
    "                                    #sentiment_score = rescale(sentiment_score,[-1,1],[0,1])\n",
    "                                    #print(sentiment_score)\n",
    "\n",
    "                    \n",
    "                    sentence=re.sub(' +', ' ',sentence)\n",
    "                    sentence=sentence.strip()\n",
    "                    \n",
    "                    sentence_l.append(sentence)\n",
    "                    target_l.append(target)\n",
    "                    sentiment_l.append(sentiment_score)\n",
    "                    aspect_l.append(aspect)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        return sentence_l,target_l,sentiment_l,aspect_l\n",
    "\n",
    "print(\"preparing Finance dataset...\")\n",
    "fname = {\n",
    "            'finance': {\n",
    "                'train': 'train_data.json',\n",
    "                'test':  'test.json',\n",
    "                'validation_test' : 'validation_test.json'\n",
    "            }\n",
    "\n",
    "\n",
    "        }\n",
    "sentence,target,sentiment,aspect=prepare_data(fname['finance']['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_sentence,v_target,v_sentiment,v_aspect=prepare_data(fname['finance']['validation_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(series,old_range,new_range):\n",
    "    m = interp1d(old_range,new_range)\n",
    "    return [float(m(x)) for x in series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = rescale(sentiment,[-1,1],[0,1])\n",
    "v_sentiment = rescale(v_sentiment,[-1,1],[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  convert_lables (trainY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(trainY)\n",
    "    temp1 = le.transform(trainY)\n",
    "    return to_categorical(temp1,27),le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_level1= []\n",
    "aspect_level2=[]\n",
    "for asp in aspect:\n",
    "    try:\n",
    "        asp=asp.lstrip(\"['\")\n",
    "        asp=asp.rstrip(\"']\")\n",
    "\n",
    "        l=asp.split(\"/\")\n",
    "        \n",
    "        aspect_level1.append(l[0])\n",
    "        aspect_level2.append(l[1])\n",
    "    except:\n",
    "        print(asp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Corporate': ['Technical Analysis',\n",
       "  'Rumors',\n",
       "  'Reputation',\n",
       "  'Legal',\n",
       "  'Sales',\n",
       "  'M&A',\n",
       "  'Regulatory',\n",
       "  'Financial',\n",
       "  'Dividend Policy',\n",
       "  'Strategy',\n",
       "  'Risks',\n",
       "  'Company Communication',\n",
       "  'Appointment'],\n",
       " 'Economy': ['Central Banks', 'Trade'],\n",
       " 'Market': ['Currency', 'Volatility', 'Conditions', 'Market'],\n",
       " 'Stock': ['Technical Analysis',\n",
       "  'IPO',\n",
       "  'Coverage',\n",
       "  'Buyside',\n",
       "  'Options',\n",
       "  'Price Action',\n",
       "  'Fundamentals',\n",
       "  'Insider Activity',\n",
       "  'Signal']}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_level_pairs(l1,l2):\n",
    "    level_pair = dict()\n",
    "    for pair in zip(l1,l2):\n",
    "        if pair[1] in level_pair.keys():\n",
    "            level_pair[pair[1]].append(pair[0])\n",
    "        else:\n",
    "            level_pair[pair[1]] = [pair[0]]\n",
    "    for _ in level_pair.keys():\n",
    "        level_pair[_] = list(set(level_pair[_]))\n",
    "    return level_pair\n",
    "L2_L1_pair = get_level_pairs(aspect_level1,aspect_level2)\n",
    "L1_L2_pair = get_level_pairs(aspect_level2,aspect_level1)\n",
    "\n",
    "L1_L2_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stock', 'Coverage']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_aspect[0][0].split('/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_level1_v= []\n",
    "aspect_level2_v=[]\n",
    "for asp in v_aspect:\n",
    "    try:\n",
    "        l=asp[0].split(\"/\")\n",
    "        aspect_level1_v.append(l[0])\n",
    "        aspect_level2_v.append(l[1])\n",
    "    except:\n",
    "        print(asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coverage',\n",
       " 'Price Action',\n",
       " 'Price Action',\n",
       " 'Insider Activity',\n",
       " 'Insider Activity',\n",
       " 'Financial',\n",
       " 'Company Communication',\n",
       " 'Market',\n",
       " 'Signal',\n",
       " 'Financial']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_level2_v[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Price Action',\n",
       " 'Price Action',\n",
       " 'Price Action',\n",
       " 'Appointment',\n",
       " 'Strategy',\n",
       " 'Price Action',\n",
       " 'Market',\n",
       " 'Financial',\n",
       " 'Price Action',\n",
       " 'Strategy']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_level2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 27)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_label_level_1 = len(set(aspect_level1))\n",
    "n_label_level_2 = len(set(aspect_level2))\n",
    "n_label_level_1,n_label_level_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 19)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_n_label_level_1 = len(set(aspect_level1_v))\n",
    "val_n_label_level_2 = len(set(aspect_level2_v))\n",
    "val_n_label_level_1,val_n_label_level_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1173"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aspect_level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY,lable_encoding = convert_lables(aspect_level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY,lable_encoding = convert_lables(aspect_level2_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "dataX=sentence\n",
    "tokenizer = create_tokenizer(dataX)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "first=[]\n",
    "for m in range(len(target)):\n",
    "    j=target[m].split() \n",
    "    first.append(j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=11\n",
    "i = encode_text(tokenizer, sentence, max_length)\n",
    "t = encode_text(tokenizer, first, 1)\n",
    "AS = encode_text(tokenizer, aspect, 2)\n",
    "t_total=encode_text(tokenizer, target, 5)\n",
    "VT_total=encode_text(tokenizer, v_target, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1478,  292,   38,   48,  445,    7,  293,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_I=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  14,    0,    0,    0,    0],\n",
       "       [  84,    0,    0,    0,    0],\n",
       "       [1461,    0,    0,    0,    0],\n",
       "       [1462,    0,    0,    0,    0],\n",
       "       [ 578,    0,    0,    0,    0],\n",
       "       [ 579,    0,    0,    0,    0],\n",
       "       [  95,    0,    0,    0,    0],\n",
       "       [ 847,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_total[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0],\n",
       "       [1247,    0,    0,    0,    0],\n",
       "       [ 782,    0,    0,    0,    0],\n",
       "       [ 110,    0,    0,    0,    0],\n",
       "       [1238,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT_total[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment= [float(x) for x in sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1462, 1462, 1462, 1462, 1462, 1462, 1462, 1462, 1462, 1462, 1462])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "t_=np.tile(t, 11)\n",
    "t_[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vt=[]\n",
    "for i in range(len(v_target)):\n",
    "    j=v_target[i].split() \n",
    "    first_vt.append(j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dh',\n",
       " 'shop',\n",
       " 'plx',\n",
       " 'coty',\n",
       " 'ge',\n",
       " 'ko',\n",
       " 'glencore',\n",
       " 'sky',\n",
       " 'berkshire',\n",
       " 'glaxo']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dh',\n",
       " 'shop',\n",
       " 'plx',\n",
       " 'coty',\n",
       " 'ge',\n",
       " 'ko',\n",
       " 'glencore',\n",
       " 'sky',\n",
       " 'berkshire',\n",
       " 'glaxo']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_vt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_i=encode_text(tokenizer, v_sentence, max_length)\n",
    "v_t=encode_text(tokenizer, first_vt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0],\n",
       "       [   0],\n",
       "       [   0],\n",
       "       [   0],\n",
       "       [1247],\n",
       "       [ 782],\n",
       "       [ 110],\n",
       "       [1238],\n",
       "       [ 204],\n",
       "       [1822]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_t[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [1247, 1247, 1247, 1247, 1247, 1247, 1247, 1247, 1247, 1247, 1247],\n",
       "       [ 782,  782,  782,  782,  782,  782,  782,  782,  782,  782,  782],\n",
       "       [ 110,  110,  110,  110,  110,  110,  110,  110,  110,  110,  110],\n",
       "       [1238, 1238, 1238, 1238, 1238, 1238, 1238, 1238, 1238, 1238, 1238],\n",
       "       [ 204,  204,  204,  204,  204,  204,  204,  204,  204,  204,  204],\n",
       "       [1822, 1822, 1822, 1822, 1822, 1822, 1822, 1822, 1822, 1822, 1822]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_t_=np.tile(v_t,11)\n",
    "\n",
    "v_sentiment= [float(x) for x in v_sentiment]\n",
    "\n",
    "v_t_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding_matrix: Finance_embedding_matrix_1.dat\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = build_embedding_matrix(vocab_size, 300,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3324"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval('embedding_matrix'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3324\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data={\n",
    "    \n",
    "    'sentence' : SENTENCE_I,\n",
    "    'target'   : t,\n",
    "    'embedding_matrix' :embedding_matrix,\n",
    "    'train_sentiment': np.array(sentiment),\n",
    "    'v_sentence' : v_i,\n",
    "    'v_target'   : v_t_,\n",
    "    'v_sentiment': np.array(v_sentiment),\n",
    "    'vocab_size' : vocab_size,\n",
    "    'target_for_IAN' : t_total,\n",
    "    'v_target_IAN' : VT_total,\n",
    "    'aspect': trainY,\n",
    "    'v_aspect':testY\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "pickle.dump(all_data, open(r\"D:\\Sentiment-Analysis\\all_data.dat\",\"wb\"))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the train , test , Validation_test_alike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open('train_data.json', encoding='utf-8') as f:\n",
    "                foo = json.load(f)\n",
    "\n",
    "                for key in foo.keys():\n",
    "                    for info in foo[key]['info']:\n",
    "                        sentence=foo[key]['sentence']\n",
    "                        #print(info['target']+ \" ## \"+ sentence)\n",
    "                        #print(sentence)\n",
    "                        sentence = [clean_sentence(x) for x in sentence.split(\" \")]\n",
    "                        #print(sentence)\n",
    "                        \n",
    "                        sentence=' '.join(sentence)\n",
    "                        #print(sentence)\n",
    "                        \n",
    "                        sentence=' '.join(sentence.split())\n",
    "                        \n",
    "                        target= info['target']\n",
    "                        #print(\"Target = \"+ target)\n",
    "                        \n",
    "                         \n",
    "                        print(info['target'].lower()+ \" ## \"+ sentence)\n",
    "                        text_left, _, text_right = [s.lower().strip() for s in sentence.partition(target)]\n",
    "                        #print( text_left + text_right)\n",
    "                        text_raw_indices =tokenizer.text_to_sequence(text_left + \" \" + target + \" \" + text_right)\n",
    "                        #print(text_raw_indices)\n",
    "                        \n",
    "                        text_raw_without_target_indices = tokenizer.text_to_sequence(text_left + \" \" + text_right)\n",
    "                        #print(text_raw_without_target_indices)\n",
    "                        text_left_indices = tokenizer.text_to_sequence(text_left)\n",
    "                        #print(text_left_indices)\n",
    "                        text_left_with_target_indices = tokenizer.text_to_sequence(text_left + \" \" + target)\n",
    "                        #print(text_left_with_target_indices)\n",
    "                        text_right_indices = tokenizer.text_to_sequence(text_right, reverse=True)\n",
    "                        #print( text_right_indices)\n",
    "                        text_right_with_target_indices = tokenizer.text_to_sequence(\" \" + target + \" \" + text_right, reverse=True)\n",
    "                        #print(text_right_with_target_indices)\n",
    "                        target_indices = tokenizer.text_to_sequence(target)\n",
    "                        #print(target_indices)\n",
    "                        \n",
    "                        #if data=='train' :\n",
    "                        sentiment_score = info['sentiment_score']\n",
    "                            #print(sentiment_score)\n",
    "                        aspect= info['aspects']\n",
    "                            #print(\"Aspect \"+ aspect)\n",
    "                            #sentiment_score = rescale(sentiment_score,[-1,1],[0,1])\n",
    "                            #print(sentiment_score)\n",
    "\n",
    "\n",
    "\n",
    "                        data = {\n",
    "                                    'text_raw_indices': text_raw_indices,\n",
    "                                    'text_raw_without_target_indices': text_raw_without_target_indices,\n",
    "                                    'text_left_indices': text_left_indices,\n",
    "                                    'text_left_with_target_indices': text_left_with_target_indices,\n",
    "                                    'text_right_indices': text_right_indices,\n",
    "                                    'text_right_with_target_indices': text_right_with_target_indices,\n",
    "                                    'target_indices': target_indices,\n",
    "                                    \n",
    "                        }\n",
    "                        \n",
    "                        #if data=='train': \n",
    "                        data['polarity']= sentiment_score\n",
    "                        \n",
    "                       \n",
    " '''                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Combining test_headline and test_post\n",
    "\n",
    "import json\n",
    "def load_test_data():\n",
    "    with open(r'D:\\Sentiment-Analysis\\test\\task1_post_ABSA_test.json', encoding='utf-8') as f1:\n",
    "        foo1 = json.load(f1)\n",
    "        with open(r'D:\\Sentiment-Analysis\\test\\task1_headline_ABSA_test.json', encoding='utf-8') as f2:\n",
    "                foo2= json.load(f2)\n",
    "                test = {**foo1, **foo2}\n",
    "                data = json.dumps(test)\n",
    "                with open(r\"D:\\Sentiment-Analysis\\test.json\",\"w\") as f:\n",
    "                    f.write(data)\n",
    "                \n",
    "#load_test_data()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "def validation_headline():\n",
    "    df = pd.read_csv(r'D:\\Datasets\\FinanceHeadlineDataset\\gold_standard\\test_headlines_samples - Sheet1.tsv', sep='\\t')\n",
    "    df.head()\n",
    "    d={}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        target = re.sub('\\d+',\"\", row[\"id\"])\n",
    "        target=target.lstrip(\"_\")\n",
    "        inner_d={}\n",
    "        inner_d_1={}\n",
    "        inner_d['info']=[]\n",
    "\n",
    "        inner_d['sentence']=row['sentence']\n",
    "\n",
    "        inner_d_1[\"target\"]=target\n",
    "        inner_d_1[\"sentiment_score\"]=row[\"sentiment_scores\"]\n",
    "        aspect_h=row[\"aspect\"].split(\"/\")\n",
    "        a=aspect_h[0]+\"/\"+aspect_h[1]\n",
    "        aspect=[]\n",
    "        aspect.append(a)\n",
    "        inner_d_1[\"aspects\"]=aspect\n",
    "        inner_d['info'].append(inner_d_1)\n",
    "\n",
    "        d[row[\"id\"]]=inner_d\n",
    "    return d\n",
    "\n",
    "def validation_posts():\n",
    "    df = pd.read_csv(r'D:\\Datasets\\FinanceHeadlineDataset\\gold_standard\\test_set_post - Sheet1.tsv', sep='\\t')\n",
    "    df.head()\n",
    "    d={}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        target = re.sub('\\d+',\"\", row[\"id\"])\n",
    "        target=target.lstrip(\"$\")\n",
    "        inner_d={}\n",
    "        inner_d_1={}\n",
    "        inner_d['info']=[]\n",
    "\n",
    "        inner_d['sentence']=row['sentence']\n",
    "\n",
    "        inner_d_1[\"target\"]=target\n",
    "        inner_d_1[\"sentiment_score\"]=row[\"sentiment_score\"]\n",
    "        a=row[\"aspect_category_1\"]+\"/\"+row[\"aspect_category_2\"]\n",
    "        aspect=[]\n",
    "        aspect.append(a)\n",
    "        inner_d_1[\"aspects\"]=aspect\n",
    "        inner_d['info'].append(inner_d_1)\n",
    "\n",
    "        d[row[\"id\"]]=inner_d\n",
    "    return d\n",
    "foo1=validation_headline()\n",
    "foo2=validation_posts()\n",
    "validation_test = {**foo1, **foo2}    \n",
    "data = json.dumps(validation_test)\n",
    "with open(r\"D:\\Sentiment-Analysis\\validation_test.json\",\"w\") as f:\n",
    "                    f.write(data)\n",
    "                    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
