{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import codecs\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import f1_score,accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pickle.load(open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Code\\train_data_initial.dat\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(train_data,open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Code\\train_data_initial_1.dat\",\"wb\"),protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rumors</td>\n",
       "      <td>ab inbrev latest bid said unlikely win COMPANY...</td>\n",
       "      <td>0.127</td>\n",
       "      <td>latest bid said unlikely to win sabmiller's ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>COMPANY shakes board two new business chiefs t...</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>shakes up board with two new business chiefs, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>press serco set appoint roy gardner ex COMPANY...</td>\n",
       "      <td>0.195</td>\n",
       "      <td>set to appoint roy gardner, ex-centrica, as ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>press COMPANY set appoint roy gardner ex centr...</td>\n",
       "      <td>0.122</td>\n",
       "      <td>set to appoint roy gardner, ex-centrica, as ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strategy</td>\n",
       "      <td>barclays sells benchmark indices unit COMPANY</td>\n",
       "      <td>0.281</td>\n",
       "      <td>sells benchmark indices unit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        aspect                                           sentence sentiment  \\\n",
       "0       Rumors  ab inbrev latest bid said unlikely win COMPANY...     0.127   \n",
       "1  Appointment  COMPANY shakes board two new business chiefs t...    -0.074   \n",
       "2  Appointment  press serco set appoint roy gardner ex COMPANY...     0.195   \n",
       "3  Appointment  press COMPANY set appoint roy gardner ex centr...     0.122   \n",
       "4     Strategy      barclays sells benchmark indices unit COMPANY     0.281   \n",
       "\n",
       "                                             snippet  \n",
       "0  latest bid said unlikely to win sabmiller's ap...  \n",
       "1  shakes up board with two new business chiefs, ...  \n",
       "2  set to appoint roy gardner, ex-centrica, as ch...  \n",
       "3  set to appoint roy gardner, ex-centrica, as ch...  \n",
       "4                       sells benchmark indices unit  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1173"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rumors</td>\n",
       "      <td>ab inbrev latest bid said unlikely win COMPANY...</td>\n",
       "      <td>0.127</td>\n",
       "      <td>latest bid said unlikely to win sabmiller's ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>COMPANY shakes board two new business chiefs t...</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>shakes up board with two new business chiefs, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>press serco set appoint roy gardner ex COMPANY...</td>\n",
       "      <td>0.195</td>\n",
       "      <td>set to appoint roy gardner, ex-centrica, as ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>press COMPANY set appoint roy gardner ex centr...</td>\n",
       "      <td>0.122</td>\n",
       "      <td>set to appoint roy gardner, ex-centrica, as ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strategy</td>\n",
       "      <td>barclays sells benchmark indices unit COMPANY</td>\n",
       "      <td>0.281</td>\n",
       "      <td>sells benchmark indices unit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        aspect                                           sentence sentiment  \\\n",
       "0       Rumors  ab inbrev latest bid said unlikely win COMPANY...     0.127   \n",
       "1  Appointment  COMPANY shakes board two new business chiefs t...    -0.074   \n",
       "2  Appointment  press serco set appoint roy gardner ex COMPANY...     0.195   \n",
       "3  Appointment  press COMPANY set appoint roy gardner ex centr...     0.122   \n",
       "4     Strategy      barclays sells benchmark indices unit COMPANY     0.281   \n",
       "\n",
       "                                             snippet  \n",
       "0  latest bid said unlikely to win sabmiller's ap...  \n",
       "1  shakes up board with two new business chiefs, ...  \n",
       "2  set to appoint roy gardner, ex-centrica, as ch...  \n",
       "3  set to appoint roy gardner, ex-centrica, as ch...  \n",
       "4                       sells benchmark indices unit  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_data=pickle.load(open('test_head.dat',\"rb\"))\n",
    "post_data=pickle.load(open('test_post.dat',\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>snippets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Financial</td>\n",
       "      <td>COMPANY delivers small profit increase 2014</td>\n",
       "      <td>0.247</td>\n",
       "      <td>delivers small profit increase for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Financial</td>\n",
       "      <td>COMPANY reports million loss first quarter</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>reports $583 million loss in first quarter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sales</td>\n",
       "      <td>osborne extends COMPANY sell plan</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>sell-off plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>companies COMPANY shares hit 2016 revenue fore...</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>hit as 2016 revenue forecast to fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strategy</td>\n",
       "      <td>COMPANY ends 27 year sponsorship tate falling ...</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>as falling oil price takes toll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         aspect                                           sentence  sentiment  \\\n",
       "0     Financial        COMPANY delivers small profit increase 2014      0.247   \n",
       "1     Financial         COMPANY reports million loss first quarter     -0.741   \n",
       "2         Sales                  osborne extends COMPANY sell plan     -0.311   \n",
       "3  Price Action  companies COMPANY shares hit 2016 revenue fore...     -0.375   \n",
       "4      Strategy  COMPANY ends 27 year sponsorship tate falling ...     -0.293   \n",
       "\n",
       "                                     snippets  \n",
       "0          delivers small profit increase for  \n",
       "1  reports $583 million loss in first quarter  \n",
       "2                               sell-off plan  \n",
       "3        hit as 2016 revenue forecast to fall  \n",
       "4             as falling oil price takes toll  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(head_data,open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Code\\head_data_1.dat\",\"wb\"),protocol=2)\n",
    "pickle.dump(post_data,open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Code\\post_data_1.dat\",\"wb\"),protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 99)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(head_data),len(post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "all_data=[]\n",
    "for i in train_data['sentence']:\n",
    "    all_data.append(i)\n",
    "for i in head_data['sentence']:\n",
    "    all_data.append(i)\n",
    "for i in post_data['sentence']:\n",
    "    all_data.append(i)\n",
    "tokenizer = create_tokenizer(all_data)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3340"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def word2vec(fname, save_file,save_file1):\n",
    "    sentences = []\n",
    "    for line in fname:\n",
    "            sentence = line\n",
    "            sentences.append(nltk.word_tokenize(sentence))\n",
    "    #sentences\n",
    "   # print(sentences[:3])\n",
    "    model = Word2Vec(sentences, size=300, min_count=1, window=5, sg=1, iter=10)\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    weights = model.wv.syn0\n",
    "    d = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n",
    "    embeddings_index = {}\n",
    "    i=0\n",
    "    for item in d:\n",
    "        embeddings_index[item] = weights[d[item], :]\n",
    "        embedding_matrix[i]=weights[d[item], :]\n",
    "        i+=1\n",
    "        #print(embeddings_index)\n",
    "    pickle.dump(embeddings_index, open(save_file1, 'wb'))\n",
    "    pickle.dump(embedding_matrix, open(save_file, 'wb'))\n",
    "\n",
    "    return embeddings_index,embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "w2v,embedding= word2vec(train_data['sentence'], 'D:\\PythonCodes\\Sentiment-Analysis\\Code\\Other team\\embeddings_300_dim.dat','D:\\PythonCodes\\Sentiment-Analysis\\Code\\Other team\\embedding_index.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=pickle.load(open(r'D:\\PythonCodes\\Sentiment-Analysis\\Code\\Other team\\embeddings_300_dim.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3340"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval('embedding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rumors</td>\n",
       "      <td>ab inbrev latest bid said unlikely win COMPANY...</td>\n",
       "      <td>0.127</td>\n",
       "      <td>latest bid said unlikely to win sabmiller's ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>COMPANY shakes board two new business chiefs t...</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>shakes up board with two new business chiefs, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>press serco set appoint roy gardner ex COMPANY...</td>\n",
       "      <td>0.195</td>\n",
       "      <td>set to appoint roy gardner, ex-centrica, as ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>press COMPANY set appoint roy gardner ex centr...</td>\n",
       "      <td>0.122</td>\n",
       "      <td>set to appoint roy gardner, ex-centrica, as ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strategy</td>\n",
       "      <td>barclays sells benchmark indices unit COMPANY</td>\n",
       "      <td>0.281</td>\n",
       "      <td>sells benchmark indices unit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        aspect                                           sentence sentiment  \\\n",
       "0       Rumors  ab inbrev latest bid said unlikely win COMPANY...     0.127   \n",
       "1  Appointment  COMPANY shakes board two new business chiefs t...    -0.074   \n",
       "2  Appointment  press serco set appoint roy gardner ex COMPANY...     0.195   \n",
       "3  Appointment  press COMPANY set appoint roy gardner ex centr...     0.122   \n",
       "4     Strategy      barclays sells benchmark indices unit COMPANY     0.281   \n",
       "\n",
       "                                             snippet  \n",
       "0  latest bid said unlikely to win sabmiller's ap...  \n",
       "1  shakes up board with two new business chiefs, ...  \n",
       "2  set to appoint roy gardner, ex-centrica, as ch...  \n",
       "3  set to appoint roy gardner, ex-centrica, as ch...  \n",
       "4                       sells benchmark indices unit  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "def convert_lables (trainY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(trainY)\n",
    "    temp1 = le.transform(trainY)\n",
    "    return to_categorical(temp1,27),le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "def isinteger(_str):\n",
    "    return _str.strip().isdigit()\n",
    "\n",
    "\n",
    "def isfloat(_str):\n",
    "    return sum([n.isdigit() for n in _str.strip().split('.')]) == 2\n",
    "\n",
    "\n",
    "def get_index(_list, item, start_index=0):\n",
    "    for i in range(start_index, len(_list)):\n",
    "        if item == _list[i]:\n",
    "            return i\n",
    "    raise (\"can not find %s in %s\" % (item, list))\n",
    "    \n",
    "def convert_lables (trainY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(trainY)\n",
    "    temp1 = le.transform(trainY)\n",
    "    return to_categorical(temp1,27),le.classes_\n",
    "\n",
    "def read_data_for_aspect(t=train_data,pre_trained_vectors=embedding):\n",
    "\n",
    "    data_csv = t\n",
    "    embedding_size=300\n",
    "    one_aspect=False\n",
    "    print(type(t))\n",
    "    stop_words = [',', '.', ':', ';', '?', '(', ')', '[', ']', '!', '@', '#', '%', '$', '*', '-', '/', '&', '``', \"''\"]\n",
    "\n",
    "    max_context_len = 0\n",
    "    word2idx = {}    # Index 0 represents words we haven't met before\n",
    "    context = []\n",
    "    context_len = []\n",
    "    aspect_class = []\n",
    "\n",
    "    for index, row in data_csv.iterrows():\n",
    "        word_list = nltk.word_tokenize(row['sentence'].strip())\n",
    "\n",
    "        context_words = [word for word in word_list\n",
    "                         if word not in stop_words and isinteger(word) is False and isfloat(word) is False\n",
    "                         and word != 'COMPANY']\n",
    "        #print(\"Context\",context_words)\n",
    "        words_have_vector = [word for word in context_words if word in pre_trained_vectors]\n",
    "        #print(\"VECTOR\",words_have_vector)\n",
    "\n",
    "        # make sure most words can find their embedding vectors\n",
    "        if len(words_have_vector) / float(len(context_words)) < 0.8:\n",
    "            continue\n",
    "\n",
    "        max_context_len = max(max_context_len, len(words_have_vector))\n",
    "\n",
    "        idx = []\n",
    "        for word in words_have_vector:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = len(word2idx)+1    # Index 0 represents absent words, so start from 1\n",
    "            idx.append(word2idx[word])\n",
    "\n",
    "        context.append(idx)\n",
    "        context_len.append(len(words_have_vector))\n",
    "\n",
    "        aspect_class.append(row['aspect'])\n",
    "\n",
    "    # convert to numpy format\n",
    "    context_npy = np.zeros(shape=[len(context), max_context_len])\n",
    "    for i in range(len(context)):\n",
    "        context_npy[i, :len(context[i])] = context[i]\n",
    "\n",
    "    aspect_class_npy, onehot_mapping = convert_lables(aspect_class)\n",
    "\n",
    "    train_d = list()\n",
    "    train_d.append(context_npy)          # [data_size, max_context_len]\n",
    "    train_d.append(np.array(context_len))    # [data_size,]\n",
    "    train_d.append(aspect_class_npy)     # [data_size, aspect_class]\n",
    "\n",
    "    word_embeddings = np.zeros([len(word2idx)+1, embedding_size])\n",
    "    for word in word2idx.keys():\n",
    "        word_embeddings[word2idx[word]] = pre_trained_vectors[word]\n",
    "\n",
    "    return train_d, word_embeddings, word2idx, max_context_len, onehot_mapping\n",
    "\n",
    "\n",
    "def read_data_for_senti(t=train_data,pre_trained_vectors=embedding):\n",
    "    embedding_size=300\n",
    "    one_aspect=False\n",
    "\n",
    "    #data_csv = pd.read_csv(fname, sep='\\t', header=None, index_col=None,\n",
    "    #                       names=['text', 'aspect_l1', 'aspect_l2', 'score'])\n",
    "    data_csv=t\n",
    "    data_csv = data_csv.sample(frac=1).reset_index(drop=True)   # shuffle data\n",
    "\n",
    "    stop_words = [',', '.', ':', ';', '?', '(', ')', '[', ']', '!', '@', '#', '%', '$', '*', '-', '/', '&', '``', \"''\"]\n",
    "\n",
    "    max_context_len = 0\n",
    "    word2idx = {}    # Index 0 represents words we haven't met before\n",
    "    aspect2idx = {}\n",
    "    context = []\n",
    "    context_len = []\n",
    "    loc_info = []\n",
    "    aspect = []\n",
    "    score = []\n",
    "    # aspect_class = []\n",
    "\n",
    "    for index, row in data_csv.iterrows():\n",
    "        word_list = nltk.word_tokenize(row.sentence.strip())\n",
    "\n",
    "        context_words = [word for word in word_list\n",
    "                         if word not in stop_words and isinteger(word) is False and isfloat(word) is False\n",
    "                         and word != 'COMPANY']\n",
    "\n",
    "        words_have_vector = [word for word in context_words if word in pre_trained_vectors]\n",
    "\n",
    "        # make sure most words can find their embedding vectors\n",
    "        if len(words_have_vector) / float(len(context_words)) < 0.8:\n",
    "            continue\n",
    "\n",
    "        #max_context_len = max(max_context_len, len(words_have_vector))\n",
    "        max_context_len=18\n",
    "        idx, distance = [], []\n",
    "        start_index = 0\n",
    "        stock_loc = get_index(word_list, 'COMPANY')\n",
    "        \n",
    "            \n",
    "        for word in words_have_vector:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = len(word2idx)+1    # Index 0 represents absent words, so start from 1\n",
    "            idx.append(word2idx[word])\n",
    "            word_loc = get_index(word_list, word, start_index=start_index)\n",
    "            start_index = word_loc + 1\n",
    "            distance.append(1 - abs(word_loc-stock_loc) / len(word_list))\n",
    "        context.append(idx)\n",
    "        context_len.append(len(words_have_vector))\n",
    "        loc_info.append(distance)\n",
    "\n",
    "        if one_aspect is True:      # consider there is only one abstract aspect\n",
    "            aspect.append(0)\n",
    "        else:\n",
    "            if row.aspect not in aspect2idx:\n",
    "                aspect2idx[row.aspect] = len(aspect2idx)\n",
    "            aspect.append(aspect2idx[row.aspect])\n",
    "\n",
    "        score.append([row.sentiment])\n",
    "\n",
    "    if one_aspect is True:\n",
    "        aspect2idx['one_aspect'] = len(aspect2idx)\n",
    "\n",
    "    # convert to numpy format\n",
    "    context_npy = np.zeros(shape=[len(context), max_context_len])\n",
    "    loc_info_npy = np.zeros(shape=[len(loc_info), max_context_len])\n",
    "\n",
    "    for i in range(len(context)):\n",
    "        context_npy[i, :len(context[i])] = context[i]\n",
    "        loc_info_npy[i, :len(loc_info[i])] = loc_info[i]\n",
    "\n",
    "    data = list()\n",
    "    data.append(context_npy)          # [data_size, max_context_len]\n",
    "    data.append(np.array(context_len))    # [data_size,]\n",
    "    data.append(loc_info_npy)         # [data_size, max_context_len]\n",
    "    data.append(np.array(aspect))     # [data_size]\n",
    "    data.append(np.array(score))      # [data_size, 1]\n",
    "\n",
    "    word_embeddings = np.zeros([len(word2idx)+1, embedding_size])\n",
    "    for word in word2idx.keys():\n",
    "        word_embeddings[word2idx[word]] = pre_trained_vectors[word]\n",
    "\n",
    "    return data, word_embeddings, word2idx, aspect2idx, max_context_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "    \n",
    "def _clear_flags():\n",
    "    \"\"\"Resets Tensorflow's FLAG values\"\"\"\n",
    "    #pylint: disable=W0212\n",
    "    for flag_key in dir(tf.app.flags.FLAGS):\n",
    "        delattr(tf.app.flags.FLAGS, flag_key)\n",
    "    #tf.app.flags.FLAGS = tf.app.flags._FlagValues()\n",
    "    tf.app.flags._global_parser = argparse.ArgumentParser()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "#import utils\n",
    "#from read_data import read_data_for_senti\n",
    "#from model import DeepMem\n",
    "#from model import AT_LSTM\n",
    "_clear_flags()\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "# common hyper-parameter\n",
    "flags.DEFINE_integer('n_word', 0, 'number of words')\n",
    "flags.DEFINE_integer('n_aspect', 27, 'number of aspects')\n",
    "flags.DEFINE_integer('max_len', 0, 'max length of one sentence')\n",
    "flags.DEFINE_integer('embedding_dim', 300, 'word embedding dimension')\n",
    "flags.DEFINE_integer('n_epoch', 50, 'max epoch to train')\n",
    "flags.DEFINE_integer('batch_size', 64, 'batch size')\n",
    "flags.DEFINE_integer('early_stopping_step', 3, \"if loss doesn't descend in 3 epochs, stop training\")\n",
    "flags.DEFINE_integer('train_time', 0, 'train time')\n",
    "flags.DEFINE_float('stddev', 0.01, 'weight initialization stddev')\n",
    "flags.DEFINE_float('l2_reg', 0.001, 'l2 regularization')\n",
    "flags.DEFINE_boolean('show', True, 'print train progress')\n",
    "flags.DEFINE_boolean('embed_trainable', True, 'whether word embeddings are trainable')\n",
    "flags.DEFINE_boolean('one_aspect', False,\n",
    "                     'whether consider all texts are related to one aspect (in an abstract level)')\n",
    "\n",
    "# hyper-parameter for deep memory model\n",
    "flags.DEFINE_integer('n_hop', 5, 'number of hops')\n",
    "flags.DEFINE_boolean('use_loc_info', True, 'whether to add location attention')\n",
    "flags.DEFINE_string('classifier_type', 'lstm', 'type of classification model: lstm, cnn')\n",
    "# hyper-parameter for attention-based & target-dependent lstm model\n",
    "flags.DEFINE_integer('hidden_size', 300, \"lstm's hidden units\")\n",
    "flags.DEFINE_integer('n_layer', 3, 'number of lstm layers')\n",
    "flags.DEFINE_boolean('is_multi', True, 'whether to use multi lstm')\n",
    "flags.DEFINE_string('lstm_type', 'at', 'type of lstm model')\n",
    "#flags.DEFINE_string('model_path', '.', 'path to save model')\n",
    "#flags.DEFINE_string('model_name', 'm', 'model_name')\n",
    "\n",
    "#logging.basicConfig(level=logging.DEBUG,\n",
    "#                    format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n",
    "#                    datefmt='%a, %d %b %Y %H:%M:%S', filename='./log/train.log', filemode='a')\n",
    "\n",
    "def split(data, i , fold_size):\n",
    "        current = i\n",
    "        size = fold_size\n",
    "        \n",
    "        index = np.arange(len(data[0]))\n",
    "        lower_bound = index >= current*size\n",
    "        upper_bound = index < (current + 1)*size\n",
    "        v_region = lower_bound*upper_bound\n",
    "        \n",
    "        train_data=list()\n",
    "        v_data=list()\n",
    "        for j in range (len(data)):\n",
    "            v_data.append(data[j][v_region])\n",
    "            train_data.append(data[j][~v_region])\n",
    "        \n",
    "        \n",
    "        return train_data, v_data\n",
    "    \n",
    "def train_model(train,valid, word_embeddings,model_type=\"\"):\n",
    "    #n_fold = 10\n",
    "    #fold_size = int(len(data[0]) / n_fold)\n",
    "    #loss_list, acc_list = [], []\n",
    "    \n",
    "    best_acc=0\n",
    "    graph = tf.Graph()\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        model = Classifier(FLAGS, sess)\n",
    "        model.build_model()\n",
    "        _, _ = model.run(train, valid, word_embeddings) \n",
    "       \n",
    "     \n",
    "    \n",
    "    return models\n",
    "        \n",
    "    #logging.debug(model_type + ' 10fold_mse: ' + str(avg_mse) + '\\t10fold_r2 :' + str(avg_r2))\n",
    "    #logging.debug(model_type + ' 10fold_mse_std: ' + str(np.std(mse_list)) + '\\t10fold_r2_std :' + str(np.std(r2_list)))\n",
    "    \n",
    "\n",
    "def main():\n",
    "    pre_trained_vectors = w2v\n",
    "    data, word_embeddings, word2idx, max_context_len, onehot_mapping = read_data_for_aspect(train_data,pre_trained_vectors)\n",
    "    head_data,_,_,_,_ = read_data_for_aspect(train_data,pre_trained_vectors)   \n",
    "    post_data,_,_,_,_ = read_data_for_aspect(train_data,pre_trained_vectors)   \n",
    "\n",
    "    #print(data)\n",
    "    FLAGS.max_len = max_context_len\n",
    "    FLAGS.n_word = word_embeddings.shape[0]\n",
    "\n",
    "\n",
    "    print('unique words embedding: ', word_embeddings.shape)\n",
    "    print('max sentence len: ', max_context_len)\n",
    "\n",
    "    model = train_model(data,post_data, word_embeddings)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "class Classifier(object):\n",
    "    def __init__(self, config, sess):\n",
    "        self.edim = config.embedding_dim\n",
    "        self.n_epoch = config.n_epoch\n",
    "        self.batch_size = config.batch_size\n",
    "        self.n_class = 27\n",
    "        self.n_layer = config.n_layer\n",
    "        self.early_stopping_step = config.early_stopping_step\n",
    "        self.stddev = config.stddev\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.n_word = config.n_word\n",
    "        self.max_len = config.max_len\n",
    "        self.classifier_type = config.classifier_type\n",
    "        self.l2_reg = config.l2_reg\n",
    "        self.show = config.show\n",
    "        self.embed_trainable = config.embed_trainable\n",
    "        self.is_multi = config.is_multi\n",
    "    \n",
    "\n",
    "        self.sess = sess\n",
    "\n",
    "        # input\n",
    "        self.context = tf.placeholder(shape=[None, self.max_len], dtype=tf.int32, name='context')\n",
    "        self.aspect_class = tf.placeholder(shape=[None, self.n_class], dtype=tf.int32, name='aspect_class')\n",
    "        self.context_len = tf.placeholder(dtype=tf.int32, name='context_len')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "        # word embeddings will be optimized if trainable == True\n",
    "        self.word_embeddings = tf.Variable(tf.random_uniform([self.n_word, self.edim], -1.0, 1.0),\n",
    "                                           name='word_embeddings', trainable=self.embed_trainable)\n",
    "\n",
    "        # batch context embedding & aspect embedding\n",
    "        self.context_embed = tf.nn.embedding_lookup(self.word_embeddings, self.context,\n",
    "                                                    name='context_embed')  # [batch_size, max_len, edim]\n",
    "\n",
    "        # Weights & Bias\n",
    "        self.W_hidden = tf.Variable(tf.truncated_normal([self.hidden_size, 128], stddev=self.stddev), name='W_hidden')\n",
    "        self.B_hidden = tf.Variable(tf.constant(0.1, shape=[128]), name='B_hidden')\n",
    "\n",
    "        self.W_final = tf.Variable(tf.truncated_normal([128, self.n_class], stddev=self.stddev), name='W_final')\n",
    "\n",
    "        self.B_final = tf.Variable(tf.constant(0.1, shape=[self.n_class]), name='B_final')\n",
    "\n",
    "        self.predict = None\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        self.train_step = None\n",
    "        self.correct_prediction=None\n",
    "        self.best_loss = 128.0\n",
    "        self.best_epoch = 0\n",
    "        self.best_accuracy = 0.0\n",
    "        self.stopping_step = 0\n",
    "\n",
    "        \n",
    "    def multi_lstm(self):\n",
    "        batch_size = tf.shape(self.context_embed)[0]\n",
    "        if self.is_multi:\n",
    "            stacked_lstm = []\n",
    "            for i in range(self.n_layer):\n",
    "                stacked_lstm.append(tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size,\n",
    "                                                                                          state_is_tuple=True),\n",
    "                                                                  output_keep_prob=self.keep_prob))\n",
    "            multi_lstm_cell = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_lstm, state_is_tuple=True)\n",
    "        else:\n",
    "            multi_lstm_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size,\n",
    "                                                                                    state_is_tuple=True),\n",
    "                                                            output_keep_prob=self.keep_prob)\n",
    "        initial_state = multi_lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        return multi_lstm_cell, initial_state\n",
    "\n",
    "    def dynamic_rnn(self, lstm_cell, inputs, context_len, initial_state, max_len, out_type='all'):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                           inputs=inputs,\n",
    "                                           sequence_length=context_len,\n",
    "                                           initial_state=initial_state,\n",
    "                                           dtype=tf.float32)  # [batch_size, max_len, hidden_size]\n",
    "        batch_size = tf.shape(outputs)[0]\n",
    "        if out_type == 'last':\n",
    "            indexs = tf.range(0, batch_size) * max_len + (context_len - 1)\n",
    "            outputs = tf.gather(tf.reshape(outputs, [-1, self.hidden_size]), indexs)  # [batch_size, hidden_size]\n",
    "        elif outputs == 'all_avg':\n",
    "            outputs = AT_LSTM.reduce_mean(outputs, context_len)  # [batch_size, hidden_size]\n",
    "        return outputs\n",
    "\n",
    "    def lstm_model(self):\n",
    "        multi_lstm_cell, initial_state = self.multi_lstm()\n",
    "        lstm_output = self.dynamic_rnn(multi_lstm_cell, self.context_embed, self.context_len, initial_state,\n",
    "                                       self.max_len, out_type='last')  # [batch_size, hidden]\n",
    "        hidden_output = tf.matmul(lstm_output, self.W_hidden) + self.B_hidden\n",
    "        predict = tf.nn.softmax(tf.matmul(hidden_output, self.W_final) + self.B_final)\n",
    "        return predict\n",
    "\n",
    "    def cnn_model(self):\n",
    "        conv_filter_w = tf.Variable(tf.truncated_normal([3, self.edim, 64],\n",
    "                                                        stddev=self.stddev), name='W_hidden')\n",
    "        conv_filter_b = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "        conv_output = tf.nn.conv1d(self.context_embed, conv_filter_w, stride=2, padding='SAME')\n",
    "        relu_output = tf.nn.relu(conv_output + conv_filter_b)\n",
    "\n",
    "        batch_size = tf.shape(relu_output)[0]\n",
    "        flatten = tf.reshape(relu_output, [batch_size, -1])\n",
    "\n",
    "        W_hidden = tf.Variable(tf.truncated_normal([14 * 64, 128], stddev=self.stddev), name='W_hidden_cnn')\n",
    "        B_hidden = tf.Variable(tf.constant(0.1, shape=[128]), name='B_hidden_cnn')\n",
    "        hidden_output = tf.matmul(flatten, W_hidden) + B_hidden\n",
    "        predict = tf.nn.softmax(tf.matmul(hidden_output, self.W_final) + self.B_final)\n",
    "        return predict\n",
    "\n",
    "    def build_model(self):\n",
    "        if self.classifier_type == 'lstm':\n",
    "            self.predict = self.lstm_model()\n",
    "        elif self.classifier_type == 'cnn':\n",
    "            self.predict = self.cnn_model()\n",
    "        else:\n",
    "            self.predict = self.lstm_model()\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=self.aspect_class, logits=self.predict))\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.predict, 1), tf.argmax(self.aspect_class, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        self.train_step = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "\n",
    "    def train(self, train_data, valid_data, word_embeddings):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(tf.local_variables_initializer())\n",
    "        self.sess.run(self.word_embeddings.assign(word_embeddings))\n",
    "\n",
    "        context, context_len, aspect_class = train_data\n",
    "        loops = int(len(context) / 64)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            avg_loss, avg_accuracy = 0.0, 0.0\n",
    "            for i in range(loops):\n",
    "                feed_dict = {self.context: context[i * self.batch_size: (i + 1) * self.batch_size],\n",
    "                             self.context_len: context_len[i * self.batch_size: (i + 1) * self.batch_size],\n",
    "                             self.aspect_class: aspect_class[i * self.batch_size: (i + 1) * self.batch_size],\n",
    "                             self.keep_prob: 0.5}\n",
    "                #_, loss, accuracy = self.sess.run([self.train_step, self.loss, self.accuracy], feed_dict=feed_dict)\n",
    "                self.correct_prediction = tf.argmax(self.predict, 1)\n",
    "                _,y_pred,val_accuracy  = self.sess.run([self.train_step, self.accuracy, self.correct_prediction], feed_dict=feed_dict)\n",
    "                print (\"validation accuracy:\", val_accuracy)\n",
    "                y_true = np.argmax(self.aspect_class,1)\n",
    "                print (\"f1_score\", sk.metrics.f1_score(y_true, y_pred,average='macro'))\n",
    "                print (\"f1_score Weighted\", sk.metrics.f1_score(y_true, y_pred,average='weighted'))\n",
    "\n",
    "                avg_loss += loss\n",
    "                avg_accuracy += accuracy\n",
    "\n",
    "            avg_loss /= loops\n",
    "            avg_accuracy /= loops\n",
    "\n",
    "            \n",
    "            if self.show:\n",
    "                print(self.classifier_type, ' epoch:', epoch, 'train_loss:', avg_loss, 'train_accuracy:', avg_accuracy)\n",
    "\n",
    "            #saver.save(self.sess, os.path.join(self.save_model_path + self.save_model_name), global_step=epoch)\n",
    "\n",
    "            valid_loss, valid_accuracy = self.valid(valid_data)\n",
    "\n",
    "            if valid_accuracy > self.best_accuracy:\n",
    "                self.best_loss = valid_loss\n",
    "                self.best_accuracy = valid_accuracy\n",
    "                self.best_epoch = epoch\n",
    "                self.stopping_step = 0\n",
    "            else:\n",
    "                self.stopping_step += 1\n",
    "            if self.stopping_step >= self.early_stopping_step:\n",
    "                #logging.debug(self.classifier_type + ' early stopping is trigger at epoch: ' + str(epoch))\n",
    "                if self.show:\n",
    "                    print(self.classifier_type, 'early stopping is trigger at epoch: ', epoch)\n",
    "                break\n",
    "\n",
    "        if self.show:\n",
    "            print(self.classifier_type, 'best epoch:', self.best_epoch, 'best loss:', self.best_loss,\n",
    "                  'best accuracy:', self.best_accuracy)\n",
    "    \n",
    "        return self.best_loss, self.best_accuracy\n",
    "\n",
    "    def valid(self, valid_data):\n",
    "        context, context_len, aspect_class = valid_data\n",
    "        feed_dict = {self.context: context,\n",
    "                     self.context_len: context_len,\n",
    "                     self.aspect_class: aspect_class,\n",
    "                     self.keep_prob: 1.0}\n",
    "        loss, accuracy = self.sess.run([self.loss, self.accuracy], feed_dict=feed_dict)\n",
    "        if self.show:\n",
    "            print(self.classifier_type, ' valid_loss:', loss, 'valid_accuracy:', accuracy)\n",
    "        return loss, accuracy\n",
    "\n",
    "    def run(self, train_data, valid_data, word_embeddings):\n",
    "        loss, accuracy = self.train(train_data, valid_data, word_embeddings)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "unique words embedding:  (3048, 300)\n",
      "max sentence len:  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: [26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26\n",
      " 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26\n",
      " 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26]\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'argmax'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-245-be61d161457b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-234-9a8b36cae923>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'max sentence len: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_context_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpost_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-234-9a8b36cae923>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(train, valid, word_embeddings, model_type)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-244-28d44b9a9b9c>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, train_data, valid_data, word_embeddings)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-244-28d44b9a9b9c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_data, valid_data, word_embeddings)\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_accuracy\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"validation accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                 \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maspect_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"f1_score\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"f1_score Weighted\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m     \"\"\"\n\u001b[1;32m-> 1004\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m# a downstream library like 'pandas'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "model1=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention-based lstm\n",
    "class AT_LSTM(object):\n",
    "    def __init__(self, config, sess):\n",
    "        self.edim = config.embedding_dim\n",
    "        self.n_epoch = config.n_epoch\n",
    "        self.batch_size = config.batch_size\n",
    "        self.n_aspect = config.n_aspect\n",
    "        self.n_layer = config.n_layer\n",
    "        self.early_stopping_step = config.early_stopping_step\n",
    "        self.stddev = config.stddev\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.n_word = config.n_word\n",
    "        self.max_len = config.max_len\n",
    "        self.lstm_type = config.lstm_type\n",
    "        self.l2_reg = config.l2_reg\n",
    "        self.show = config.show\n",
    "        self.embed_trainable = config.embed_trainable\n",
    "        self.is_multi = config.is_multi\n",
    "        #self.save_model_path = config.model_path\n",
    "        #self.save_model_name = config.model_name\n",
    "        self.train_time = config.train_time\n",
    "\n",
    "        self.sess = sess\n",
    "\n",
    "        # input\n",
    "        self.context = tf.placeholder(shape=[None, self.max_len], dtype=tf.int32, name='context')\n",
    "        self.aspect = tf.placeholder(dtype=tf.int32, name='aspect')\n",
    "        self.score = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='score')\n",
    "        self.context_len = tf.placeholder(dtype=tf.int32, name='context_len')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "\n",
    "        # aspect embeddings will be optimized along with training process\n",
    "        # word embeddings will be optimized if trainable == True\n",
    "        self.word_embeddings = tf.Variable(tf.random_uniform([self.n_word, self.edim], -1.0, 1.0),\n",
    "                                           name='word_embeddings', trainable=self.embed_trainable)\n",
    "        self.aspect_embeddings = tf.Variable(tf.random_uniform([self.n_aspect, self.edim], -1.0, 1.0),\n",
    "                                             name='aspect_embeddings')\n",
    "\n",
    "        # batch context embedding & aspect embedding\n",
    "        self.context_embed = tf.nn.embedding_lookup(self.word_embeddings, self.context,\n",
    "                                                    name='context_embed')  # [batch_size, max_len, edim]\n",
    "        self.aspect_embed = tf.nn.embedding_lookup(self.aspect_embeddings, self.aspect,\n",
    "                                                   name='aspect_embed')  # [batch_size, edim]\n",
    "\n",
    "        # Weights\n",
    "        self.W_concat = tf.Variable(tf.truncated_normal([self.edim + self.hidden_size, self.edim + self.hidden_size],\n",
    "                                                        stddev=self.stddev), name='W_concat')\n",
    "        self.W_softmax = tf.Variable(tf.truncated_normal([self.edim + self.hidden_size, 1], stddev=self.stddev),\n",
    "                                     name='W_softmax')\n",
    "        self.Wp = tf.Variable(tf.truncated_normal([self.hidden_size, self.hidden_size], stddev=self.stddev),\n",
    "                              name='Wp')\n",
    "        self.Wx = tf.Variable(tf.truncated_normal([self.hidden_size, self.hidden_size], stddev=self.stddev),\n",
    "                              name='Wx')\n",
    "        self.W_final = tf.Variable(tf.truncated_normal([self.hidden_size, 1], stddev=self.stddev), name='W_final')\n",
    "\n",
    "        self.predict = None\n",
    "        self.mse = None\n",
    "        self.r2 = None\n",
    "        self.train_step = None\n",
    "\n",
    "        self.best_mse = 4.0\n",
    "        self.best_epoch = 0\n",
    "        self.best_r2 = 0.0\n",
    "        self.stopping_step = 0\n",
    "\n",
    "       # logging.basicConfig(level=logging.DEBUG,\n",
    "       #                     format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n",
    "       #                     datefmt='%a, %d %b %Y %H:%M:%S', filename='./data/AT_LSTM_watch.log', filemode='a')\n",
    "\n",
    "    def multi_lstm(self):\n",
    "        batch_size = tf.shape(self.context_embed)[0]\n",
    "\n",
    "        if self.is_multi:\n",
    "            stacked_lstm = []\n",
    "            for i in range(self.n_layer):\n",
    "                stacked_lstm.append(tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size,\n",
    "                                                                                          state_is_tuple=True),\n",
    "                                                                  output_keep_prob=self.keep_prob))\n",
    "            multi_lstm_cell = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_lstm, state_is_tuple=True)\n",
    "        else:\n",
    "            multi_lstm_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size,\n",
    "                                                                                    state_is_tuple=True),\n",
    "                                                            output_keep_prob=self.keep_prob)\n",
    "\n",
    "        initial_state = multi_lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "        return multi_lstm_cell, initial_state\n",
    "\n",
    "    def dynamic_rnn(self, lstm_cell, inputs, context_len, initial_state, max_len, out_type='all'):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                           inputs=inputs,\n",
    "                                           sequence_length=context_len,\n",
    "                                           initial_state=initial_state,\n",
    "                                           dtype=tf.float32)  # [batch_size, max_len, hidden_size]\n",
    "\n",
    "        batch_size = tf.shape(outputs)[0]\n",
    "        if out_type == 'last':\n",
    "            indexs = tf.range(0, batch_size) * max_len + (context_len - 1)\n",
    "            outputs = tf.gather(tf.reshape(outputs, [-1, self.hidden_size]), indexs)  # [batch_size, hidden_size]\n",
    "        elif outputs == 'all_avg':\n",
    "            outputs = AT_LSTM.reduce_mean(outputs, context_len)  # [batch_size, hidden_size]\n",
    "        return outputs\n",
    "\n",
    "    def cnn_model(self):\n",
    "        conv_filter_w = tf.Variable(tf.truncated_normal([3, self.edim, 64],\n",
    "                                                        stddev=self.stddev), name='W_hidden')\n",
    "        conv_filter_b = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "        conv_output = tf.nn.conv1d(self.context_embed, conv_filter_w, stride=1, padding='SAME')\n",
    "        relu_output = tf.nn.relu(conv_output + conv_filter_b)\n",
    "\n",
    "        relu_output = tf.expand_dims(relu_output, 1)\n",
    "        max_pool = tf.nn.max_pool(relu_output, ksize=[1, 1, 2, 1], strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "        batch_size = tf.shape(max_pool)[0]\n",
    "        flatten = tf.reshape(max_pool, [batch_size, -1])\n",
    "\n",
    "        W_hidden = tf.Variable(tf.truncated_normal([9 * 64, 128], stddev=self.stddev), name='W_hidden_cnn')\n",
    "        B_hidden = tf.Variable(tf.constant(0.1, shape=[128]), name='B_hidden_cnn')\n",
    "        hidden_output = tf.matmul(flatten, W_hidden) + B_hidden\n",
    "\n",
    "        W_final = tf.Variable(tf.truncated_normal([128, 1], stddev=self.stddev), name='W_final')\n",
    "        predict = tf.matmul(hidden_output, W_final, name='predict')\n",
    "        return predict\n",
    "\n",
    "    # orignal lstm model\n",
    "    def original_model(self):\n",
    "        multi_lstm_cell, initial_state = self.multi_lstm()\n",
    "        lstm_output = self.dynamic_rnn(multi_lstm_cell, self.context_embed, self.context_len, initial_state,\n",
    "                                       self.max_len, out_type='last')  # [batch_size, hidden]\n",
    "\n",
    "        predict = tf.matmul(lstm_output, self.W_final, name='predict')\n",
    "\n",
    "        return predict\n",
    "\n",
    "    # at_model: use attention mechanism\n",
    "    # concatenate the aspect vector into the sentence hidden representations for computing attention weights\n",
    "    def at_model(self):\n",
    "        batch_size = tf.shape(self.context_embed)[0]\n",
    "        aspect_expand = tf.tile(self.aspect_embed, [1, self.max_len])\n",
    "        aspect_expand = tf.reshape(aspect_expand, [-1, self.max_len, self.edim],\n",
    "                                   name='aspect_expand')  # [batch_size, max_len, edim]\n",
    "\n",
    "        multi_lstm_cell, initial_state = self.multi_lstm()\n",
    "        lstm_output = self.dynamic_rnn(multi_lstm_cell, self.context_embed, self.context_len, initial_state,\n",
    "                                       self.max_len, out_type='all')  # [batch_size, max_len, hidden]\n",
    "\n",
    "        w_concat_tile = tf.reshape(tf.tile(self.W_concat, [batch_size, 1]),\n",
    "                                   [batch_size, self.edim + self.hidden_size, -1],\n",
    "                                   name='w_concat_title')  # [batch_size, edim+hidden_size, edim+hidden_size]\n",
    "        M = tf.nn.tanh(tf.matmul(tf.concat([lstm_output, aspect_expand], axis=2), w_concat_tile),\n",
    "                       name='M')  # [batch_size, max_len, edim+hidden_size]\n",
    "        w_softmax_tile = tf.reshape(tf.tile(self.W_softmax, [batch_size, 1]),\n",
    "                                    [batch_size, -1, 1])  # [batch_size, edim+hidden_size, 1]\n",
    "        w_mul_M = tf.reshape(tf.matmul(M, w_softmax_tile), [batch_size, 1, -1],\n",
    "                             name='w_mul_M')  # [batch_size, 1, max_len]\n",
    "\n",
    "        alpha = AT_LSTM.softmax(w_mul_M, self.context_len, self.max_len)  # [batch_size, 1, max_len)\n",
    "\n",
    "        r = tf.reshape(tf.matmul(alpha, lstm_output), [batch_size, self.hidden_size],\n",
    "                       name='r')  # weighted representation of sentence with given aspect: [batch_size, hidden_size]\n",
    "\n",
    "        # get last output of lstm\n",
    "        index = tf.range(batch_size) * self.max_len + (self.context_len - 1)\n",
    "        lstm_last = tf.gather(tf.reshape(lstm_output, [-1, self.hidden_size]), index)  # [batch_size, hidden_size]\n",
    "\n",
    "        h = tf.tanh(tf.matmul(r, self.Wp) + tf.matmul(lstm_last, self.Wx))  # final sentence representation\n",
    "\n",
    "        predict = tf.matmul(h, self.W_final, name='predict')\n",
    "\n",
    "        return predict\n",
    "\n",
    "    # ae_model: additionally append the aspect vector into the input word vectors\n",
    "    def ae_model(self):\n",
    "        aspect_expand = tf.tile(self.aspect_embed, [1, self.max_len])\n",
    "        aspect_expand = tf.reshape(aspect_expand, [-1, self.max_len, self.edim],\n",
    "                                   name='aspect_expand')  # [batch_size, max_len, edim]\n",
    "        inputs = tf.concat([self.context_embed, aspect_expand], axis=2, name='inputs')\n",
    "\n",
    "        multi_lstm_cell, initial_state = self.multi_lstm()\n",
    "        lstm_output = self.dynamic_rnn(multi_lstm_cell, inputs, self.context_len, initial_state, self.max_len,\n",
    "                                       out_type='last')  # [batch_size, hidden]\n",
    "\n",
    "        predict = tf.matmul(lstm_output, self.W_final, name='predict')\n",
    "\n",
    "        return predict\n",
    "\n",
    "    # atae_model: attention mechanism & aspect vector as input\n",
    "    def atae_model(self):\n",
    "        batch_size = tf.shape(self.context_embed)[0]\n",
    "        aspect_expand = tf.tile(self.aspect_embed, [1, self.max_len])\n",
    "        aspect_expand = tf.reshape(aspect_expand, [-1, self.max_len, self.edim],\n",
    "                                   name='aspect_expand')  # [batch_size, max_len, edim]\n",
    "        inputs = tf.concat([self.context_embed, aspect_expand], axis=2, name='inputs')\n",
    "\n",
    "        multi_lstm_cell, initial_state = self.multi_lstm()\n",
    "        lstm_output = self.dynamic_rnn(multi_lstm_cell, inputs, self.context_len, initial_state, self.max_len,\n",
    "                                       out_type='all')  # [batch_size, max_len, hidden]\n",
    "\n",
    "        w_concat_tile = tf.reshape(tf.tile(self.W_concat, [batch_size, 1]),\n",
    "                                   [batch_size, self.edim + self.hidden_size, -1],\n",
    "                                   name='w_concat_title')  # [batch_size, edim+hidden_size, edim+hidden_size]\n",
    "        M = tf.nn.tanh(tf.matmul(tf.concat([lstm_output, aspect_expand], axis=2), w_concat_tile),\n",
    "                       name='M')  # [batch_size, max_len, edim+hidden_size]\n",
    "        w_softmax_tile = tf.reshape(tf.tile(self.W_softmax, [batch_size, 1]),\n",
    "                                    [batch_size, -1, 1])  # [batch_size, edim+hidden_size, 1]\n",
    "        w_mul_M = tf.reshape(tf.matmul(M, w_softmax_tile), [batch_size, 1, -1],\n",
    "                             name='w_mul_M')  # [batch_size, 1, max_len]\n",
    "\n",
    "        alpha = AT_LSTM.softmax(w_mul_M, self.context_len, self.max_len)  # [batch_size, 1, max_len)\n",
    "\n",
    "        r = tf.reshape(tf.matmul(alpha, lstm_output), [batch_size, self.hidden_size],\n",
    "                       name='r')  # weighted representation of sentence with given aspect: [batch_size, hidden_size]\n",
    "\n",
    "        # get last output of lstm\n",
    "        index = tf.range(batch_size) * self.max_len + (self.context_len - 1)\n",
    "        lstm_last = tf.gather(tf.reshape(lstm_output, [-1, self.hidden_size]), index)  # [batch_size, hidden_size]\n",
    "\n",
    "        h = tf.tanh(tf.matmul(r, self.Wp) + tf.matmul(lstm_last, self.Wx))  # final sentence representation\n",
    "\n",
    "        predict = tf.matmul(h, self.W_final, name='predict')\n",
    "\n",
    "        return predict\n",
    "\n",
    "    def build_model(self):\n",
    "        if self.lstm_type == 'at':\n",
    "            self.predict = self.at_model()\n",
    "        elif self.lstm_type == 'ae':\n",
    "            self.predict = self.ae_model()\n",
    "        elif self.lstm_type == 'atae':\n",
    "            self.predict = self.atae_model()\n",
    "        elif self.lstm_type == 'cnn':\n",
    "            self.predict = self.cnn_model()\n",
    "        else:\n",
    "            self.predict = self.original_model()\n",
    "\n",
    "        reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        self.mse = tf.reduce_mean(tf.square(self.score - self.predict)) + sum(reg_loss)\n",
    "\n",
    "        score_mean = tf.reduce_mean(self.score)\n",
    "        self.r2 = 1 - tf.reduce_sum(tf.square(self.score - self.predict)) / tf.reduce_sum(\n",
    "            tf.square(self.score - score_mean))\n",
    "\n",
    "        self.train_step = tf.train.AdamOptimizer().minimize(self.mse)\n",
    "\n",
    "    def train(self, train_data, valid_data,head_data,post_data,word_embeddings, ):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(tf.local_variables_initializer())\n",
    "        self.sess.run(self.word_embeddings.assign(word_embeddings))\n",
    "      #  saver = tf.train.Saver(max_to_keep=self.n_epoch)\n",
    "\n",
    "        context, context_len, loc_info, aspect, score = train_data\n",
    "        print(len(context))\n",
    "        loops = int(len(context) / self.batch_size)\n",
    "        print(\"Loops \",loops)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            avg_mse, avg_r2 = 0.0, 0.0\n",
    "            for i in range(loops):\n",
    "                feed_dict = {self.context: context[i * self.batch_size: (i + 1) * self.batch_size],\n",
    "                             self.context_len: context_len[i * self.batch_size: (i + 1) * self.batch_size],\n",
    "                             self.aspect: aspect[i * self.batch_size: (i + 1) * self.batch_size],\n",
    "                             self.score: score[i * self.batch_size: (i + 1) * self.batch_size],\n",
    "                             self.keep_prob: 0.5}\n",
    "                # print(self.sess.run(self.predict, feed_dict=feed_dict))\n",
    "                _, mse, r2 = self.sess.run([self.train_step, self.mse, self.r2], feed_dict=feed_dict)\n",
    "\n",
    "                avg_mse += mse\n",
    "                avg_r2 += r2\n",
    "\n",
    "            avg_mse /= loops\n",
    "            avg_r2 /= loops\n",
    "            #logging.debug(self.lstm_type + ' ' + str(epoch) + ' train_mse: ' +\n",
    "            #              str(avg_mse) + '\\ttrain_r2 :' + str(avg_r2))\n",
    "\n",
    "            if self.show:\n",
    "                print(self.lstm_type, ' epoch:', epoch, 'train_mse:', avg_mse, 'train_r2:', avg_r2)\n",
    "\n",
    "            #saver.save(self.sess, os.path.join(self.save_model_path, self.save_model_name), global_step=epoch)\n",
    "            \n",
    "            valid_mse, valid_r2 = self.valid(valid_data)\n",
    "\n",
    "            if valid_mse < self.best_mse:\n",
    "                self.best_mse = valid_mse\n",
    "                self.best_r2 = valid_r2\n",
    "                self.best_epoch = epoch\n",
    "                self.stopping_step = 0\n",
    "            else:\n",
    "                self.stopping_step += 1\n",
    "            if self.stopping_step >= self.early_stopping_step:\n",
    "                #logging.debug(self.lstm_type + ' early stopping is trigger at epoch: ' + str(epoch))\n",
    "                if self.show:\n",
    "                    print(self.lstm_type + ' early stopping is trigger at epoch: ', epoch)\n",
    "                break\n",
    "\n",
    "        if self.show:\n",
    "            print(self.lstm_type + ' best epoch:', self.best_epoch, 'best mse:', self.best_mse, 'best r2:', self.best_r2)\n",
    "       # logging.debug(self.lstm_type + ' best epoch: ' + str(self.best_epoch) +\n",
    "       #               '\\tbest mse: ' + str(self.best_mse) + '\\tbest r2 :' + str(self.best_r2))\n",
    "        \n",
    "        #only_save_best_epoch(self.save_model_path, self.save_model_name, self.best_epoch, self.train_time)\n",
    "        \n",
    "        head_mse, head_r2 = self.valid(head_data)\n",
    "        post_mse, post_r2 = self.valid(post_data)\n",
    "        \n",
    "        print(\"Headline\")\n",
    "        print(\"MSE : \",head_mse)\n",
    "        print(\"R2 : \",post_r2)\n",
    "        \n",
    "        print(\"Post\")\n",
    "        print(\"MSE : \",post_mse)\n",
    "        print(\"R2 : \",post_r2)\n",
    "        \n",
    "        return self.best_mse, self.best_r2\n",
    "\n",
    "    def valid(self, valid_data):\n",
    "        context, context_len, loc_info, aspect, score = valid_data\n",
    "\n",
    "        feed_dict = {self.context: context,\n",
    "                     self.context_len: context_len,\n",
    "                     self.aspect: aspect,\n",
    "                     self.score: score,\n",
    "                     self.keep_prob: 1.0}\n",
    "\n",
    "        mse, r2 = self.sess.run([self.mse, self.r2], feed_dict=feed_dict)\n",
    "\n",
    "       # logging.debug(self.lstm_type + ' valid_mse: ' + str(mse) + '\\tvalid_r2 :' + str(r2))\n",
    "\n",
    "        if self.show:\n",
    "            print(self.lstm_type, ' valid_mse:', mse, 'valid_r2:', r2)\n",
    "\n",
    "        return mse, r2\n",
    "\n",
    "    \n",
    "    def run(self, train_data, valid_data, head, post ,word_embeddings):\n",
    "        mse, r2 = self.train(train_data, valid_data,head,post, word_embeddings)\n",
    "        return mse, r2\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_mean(inputs, length):\n",
    "        \"\"\"\n",
    "        :param inputs: 3D tensor->[batch_size, sequence_len, embedding_size]\n",
    "        :param length: 1D tensor->[batch_size], represent every sample's len, not bigger than sequence_len\n",
    "        :return: 2D tensor->[batch_size, embedding_size]\n",
    "        \"\"\"\n",
    "        length = tf.reshape(length, [-1, 1])\n",
    "        outputs = tf.reduce_sum(inputs, axis=1) / length\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(inputs, length, max_length):\n",
    "        \"\"\"\n",
    "        :param inputs: 3D tensor->[batch_size, 1, n_class]\n",
    "        :param length: 1D tensor->[batch_size], represent every sample's len, not bigger than sequence_len\n",
    "        :param max_length:\n",
    "        :return: 3D tensor->[batch_size, 1, n_class]\n",
    "        \"\"\"\n",
    "        length = tf.reshape(length, [-1])\n",
    "        inputs = tf.exp(inputs)\n",
    "        mask = tf.reshape(tf.cast(tf.sequence_mask(length, max_length), tf.float32), tf.shape(inputs))\n",
    "        inputs *= mask\n",
    "        _sum = tf.reduce_sum(inputs, reduction_indices=2, keepdims=True)\n",
    "        return inputs / _sum\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def only_save_best_epoch(save_model_path,save_model_name,best_epoch,train_time):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
