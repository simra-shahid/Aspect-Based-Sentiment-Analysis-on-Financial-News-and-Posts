{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk import pos_tag\n",
    "from string import punctuation,digits\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i in list(punctuation):\n",
    "    if i=='%':\n",
    "        continue\n",
    "    else:\n",
    "        a.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    list_punctuation = a\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,'')\n",
    "    return s\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', r'', sentence)\n",
    "    sentence = re.sub(r'(?P<url>http?://[^\\s]+)', r'', sentence)\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "    sentence = re.sub(r\"$ \", \"\", sentence)\n",
    "    sentence = sentence.replace('#',' ')\n",
    "    sentence = sentence.replace(\"'s\",' ')\n",
    "    sentence = sentence.replace(\"-\",' ')\n",
    "    tokens = sentence.split()\n",
    "    tokens = [remove_punctuation(w) for w in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    #remove_digits = str.maketrans('', '', digits)\n",
    "    tokens = [w.translate(tokens) for w in tokens]\n",
    "    tokens = [w.strip() for w in tokens]\n",
    "    tokens = [w for w in tokens if w!=\"\"]\n",
    "    tokens = [i.replace(\"ocompany\",\"OCOMPANY\") for i in tokens]\n",
    "    tokens = [i.replace(\"company\",\"COMPANY\") for i in tokens]\n",
    "\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_specific_features(text):\n",
    "    features=[]\n",
    "    for i in text.split():\n",
    "        match=re.search(\"(^|[ \\t])([-+$]?(\\d+|\\.\\d+|\\d+\\.\\d*))($|[^+-.]*)\", i)\n",
    "        if  match:\n",
    "            features.append(match.group(0))\n",
    "    return features\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$123']\n"
     ]
    }
   ],
   "source": [
    "print(extract_domain_specific_features(\"Hello $123 this\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-b5ffe4864d86>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-b5ffe4864d86>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    }\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "typos={'NewsMorrisons':'News Morrissons',\n",
    "       'Taylor Wimpey':'Tailor Wimpey',\n",
    "       'AB InBev':'AB InBrev',\n",
    "       'Aberdeen Asset Management':'Aberdeen Asset Managment',\n",
    "       'International Business Machines Corp. ':'IBM',\n",
    "      }\n",
    "\n",
    "target_typos={\n",
    "       'shoeders':'schroders',\n",
    "     #  'jp morgan chase':'jpmorgan chase',\n",
    "       'gs':'companiesg4s',\n",
    "    ''\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing Finance dataset...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_data(fname):\n",
    "        \n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            foo = json.load(f)\n",
    "            sentence_l=[]\n",
    "            target_l=[]\n",
    "            aspect_l=[]\n",
    "            sentiment_l=[]\n",
    "            snippet_l=[]\n",
    "            features=[]\n",
    "            for key in foo.keys():\n",
    "                for info in foo[key]['info']:\n",
    "                    #print(info)\n",
    "                    \n",
    "                    sentence=foo[key]['sentence']\n",
    "                    z=sentence\n",
    "                    target= info['target']\n",
    "                \n",
    "                    c=[]\n",
    "                    #print(sentence)\n",
    "                    sentence = sentence.replace(\"'s\",' ')\n",
    "\n",
    "                    for typo,correct in typos.items(): \n",
    "                        if typo in sentence:\n",
    "                            \n",
    "                            #print(sentence)\n",
    "\n",
    "                            sentence=re.sub(typo,correct,sentence)\n",
    "                            \n",
    "                            \n",
    "                            #print(sentence)\n",
    "                            \n",
    "                    #sentence=re.sub(target.lower(),'COMPANY',sentence)\n",
    "                    #sentence = re.sub(r\"[$]?(\\w+)\", \"OCOMPANY\", sentence)\n",
    "                    \n",
    "\n",
    "                    features.append(extract_domain_specific_features(sentence))\n",
    "                    sentence=sentence.lower()\n",
    "                \n",
    "                    p=[]\n",
    "                    for i in sentence.partition(target.lower()):\n",
    "                        if i==target.lower():\n",
    "                            p.append('COMPANY')\n",
    "                            \n",
    "                        else: \n",
    "                            p.append(i)\n",
    "                    p=\" \".join(p)\n",
    "                    p=p.strip()\n",
    "                    \n",
    "                    l=[]\n",
    "                    for i in p.split(\" \"):\n",
    "                        if i.startswith('$'):\n",
    "                            if not i.lstrip('$').isalpha():\n",
    "                                l.append('')\n",
    "                            elif not i.lstrip('$').isnumeric():\n",
    "                                l.append(\"OCOMPANY\")\n",
    "                        else:\n",
    "                            l.append(i)\n",
    "                                \n",
    "                    l=\" \".join(l)\n",
    "                    l=l.strip()            \n",
    "                            \n",
    "                    \n",
    "                    sentence = [clean_sentence(x) for x in l.split(\" \")]\n",
    "                    #print(sentence)\n",
    "                    sentence=' '.join(sentence)\n",
    "                              \n",
    "                    if 'COMPANY' not in sentence.split():\n",
    "                        \n",
    "                            k=[]\n",
    "                            #print(\"STILL NOT SOLVED 1\")\n",
    "                            #print(sentence,\"AND\",target)\n",
    "                            \n",
    "                            #if sentence=='xli potential intermediate top uptrend charts tdf':\n",
    "                            #      sentence=re.sub('xli','COMPANY',sentence)\n",
    "                                    \n",
    "                            new=target.lower()[:3]\n",
    "                            for i in sentence.split():\n",
    "                                #print(i)\n",
    "                                if i.startswith(new):\n",
    "                                    k.append('COMPANY')\n",
    "                                else :\n",
    "                                    k.append(i)\n",
    "                                    \n",
    "                            k=' '.join(k)\n",
    "                            sentence=k\n",
    "                            \n",
    "                    #print(z,\"AND\",target)\n",
    "                    #print(sentence)\n",
    "                    #if 'COMPANY' not in sentence.split(\" \") : \n",
    "                        #print(\"NOT SOLVED :\",sentence, \"   AND  \",target.lower())\n",
    "                    #    sentence=re.sub('OCOMPANY','COMPANY',sentence)\n",
    "                       # if 'COMPANY' in sentence.split(\" \") : print(\"SOLVED :\",sentence)\n",
    "                        \n",
    "                    if 'COMPANY' not in sentence.split(\" \"):\n",
    "                    \n",
    "                           # print(\"STILL NOT SOLVED 2\")\n",
    "                           # print(sentence,\"AND\",target)\n",
    "                 \n",
    "                            if sentence.strip().startswith('gemalto shares slump third profit'):\n",
    "                                      sentence=re.sub('gemalto','COMPANY',sentence)\n",
    "                                \n",
    "                            if sentence.strip().startswith('coca cola shares rise earnings beat expectations'):\n",
    "                                      sentence=re.sub('coca cola','COMPANY',sentence)\n",
    "                                \n",
    "                            if sentence.strip().startswith('alphabet revenue 21 percent'):\n",
    "                                      sentence=re.sub('alphabet','COMPANY',sentence)                            \n",
    "\n",
    "                                \n",
    "                            if sentence.startswith('price comparison site slumps 4 y'):\n",
    "                                     #sentence=re.sub('companiescoutts','companies coutts',sentence)\n",
    "                                     sentence='COMPANY '+sentence\n",
    "                            '''           \n",
    "                            for typo,correct in target_typos.items(): \n",
    "                                    if typo ==target.lower():\n",
    "                                            sentence=re.sub(correct,'COMPANY',sentence)   \n",
    "                            '''                \n",
    "                                    \n",
    "                            #if 'COMPANY' not in sentence.split(\" \") : \n",
    "                            #    sentence='COMPANY '+sentence+ \" AND ALS0 \"+ target\n",
    "                            if 'COMPANY' not in sentence.split(\" \") : print('NOT solved',z,\"#####\", target,\"@@@@\",sentence)\n",
    "                \n",
    "                    if sentence.strip().startswith('industry newsrevenue earnings take'):\n",
    "                                sentence=re.sub('newsrevenue','news revenue',sentence)\n",
    "                   \n",
    "    \n",
    "                    snippet=info['snippets'].lstrip('[')\n",
    "                    snippet=snippet.rstrip(']')\n",
    "                    snippet=snippet.lower()\n",
    "                   \n",
    "                    #print(snippet)\n",
    "                    \n",
    "                    sentiment_score = info['sentiment_score']\n",
    "                                    #print(sentiment_score)\n",
    "                    aspect= info['aspects']\n",
    "                    \n",
    "                    #print(aspect)\n",
    "                    asp=aspect[0].lstrip(\"['\")\n",
    "                   \n",
    "                    asp=asp.rstrip(\"']\")\n",
    "                    #print(asp)\n",
    "                    l=asp.split(\"/\")\n",
    "                    #print(l)\n",
    "\n",
    "                    aspect=l[1]\n",
    "                    #print(aspect)\n",
    "                        \n",
    "                    sentence=re.sub(' +', ' ',sentence)\n",
    "                    sentence=sentence.strip()\n",
    "                    \n",
    "                    \n",
    "                    #print(sentence)\n",
    "                    \n",
    "                    sentence_l.append(sentence)\n",
    "                    target_l.append(target)\n",
    "                    sentiment_l.append(sentiment_score)\n",
    "                    aspect_l.append(aspect)\n",
    "                    s=snippet.lstrip('\\'')\n",
    "                    s=s.rstrip('\\'')\n",
    "                    #print(s)\n",
    "                    snippet_l.append(s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        return sentence_l,target_l,sentiment_l,aspect_l,features,snippet_l\n",
    "\n",
    "print(\"preparing Finance dataset...\")\n",
    "fname = {\n",
    "            'finance': {\n",
    "                'train_headline': r'D:\\Datasets\\FinanceHeadlineDataset\\train\\headline.json',\n",
    "                'train_post': r'D:\\Datasets\\FinanceHeadlineDataset\\train\\post.json',\n",
    "                'test_headline':  r'D:\\Datasets\\FinanceHeadlineDataset\\test\\headline.json',\n",
    "                'test_post':  r'D:\\Datasets\\FinanceHeadlineDataset\\test\\post.json',\n",
    "                'validation_post' : r'D:/PythonCodes/Sentiment-Analysis/Data/validation_post_1.json',\n",
    "                'validation_headline' :r'D:/PythonCodes/Sentiment-Analysis/Data/validation_headline_1.json'\n",
    "            },\n",
    "            'Data_Augmentation':{\n",
    "                'sentiment':{\n",
    "                    'train_headline':r'D:\\PythonCodes\\Sentiment-Analysis\\Data\\Headline_Train.json',\n",
    "                    'train_post':r'D:\\PythonCodes\\Sentiment-Analysis\\Data\\Microblog_Trainingdata.json',\n",
    "                    'validation_headline':'D:\\PythonCodes\\Sentiment-Analysis\\Data\\Headline_Trialdata.json',\n",
    "                    'validation_post':'D:\\PythonCodes\\Sentiment-Analysis\\Data\\Microblog_Trialdata.json'\n",
    "                },\n",
    "                'aspect':{\n",
    "                    'train_data':'train_data_augmented.dat'\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TRAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#headlines=pd.DataFrame(data={'sentence':h_sentence,'sentiment':h_sentiment,'aspect':h_aspect,'snippet':h_snippet,'features':h_features})\n",
    "#pickle.dump(headlines,open('headlines.data',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines=pickle.load(open('headlines.data',\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>features</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rumors</td>\n",
       "      <td>[]</td>\n",
       "      <td>ab inbrev latest bid said unlikely win COMPANY...</td>\n",
       "      <td>0.127</td>\n",
       "      <td>latest bid said unlikely to win sabmiller's ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPANY shakes board two new business chiefs t...</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>shakes up board with two new business chiefs, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>[]</td>\n",
       "      <td>press serco set appoint roy gardner ex COMPANY...</td>\n",
       "      <td>0.195</td>\n",
       "      <td>set to appoint roy gardner, ex-centrica, as ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>[]</td>\n",
       "      <td>press COMPANY set appoint roy gardner ex centr...</td>\n",
       "      <td>0.122</td>\n",
       "      <td>set to appoint roy gardner, ex-centrica, as ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strategy</td>\n",
       "      <td>[]</td>\n",
       "      <td>barclays sells benchmark indices unit COMPANY</td>\n",
       "      <td>0.281</td>\n",
       "      <td>sells benchmark indices unit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        aspect features                                           sentence  \\\n",
       "0       Rumors       []  ab inbrev latest bid said unlikely win COMPANY...   \n",
       "1  Appointment       []  COMPANY shakes board two new business chiefs t...   \n",
       "2  Appointment       []  press serco set appoint roy gardner ex COMPANY...   \n",
       "3  Appointment       []  press COMPANY set appoint roy gardner ex centr...   \n",
       "4     Strategy       []      barclays sells benchmark indices unit COMPANY   \n",
       "\n",
       "  sentiment                                            snippet  \n",
       "0     0.127  latest bid said unlikely to win sabmiller's ap...  \n",
       "1    -0.074  shakes up board with two new business chiefs, ...  \n",
       "2     0.195  set to appoint roy gardner, ex-centrica, as ch...  \n",
       "3     0.122  set to appoint roy gardner, ex-centrica, as ch...  \n",
       "4     0.281                       sells benchmark indices unit  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aspect                                              Sales\n",
       "features                                            [-4%]\n",
       "sentence     COMPANY gap september comparable store sales\n",
       "sentiment                                          -0.441\n",
       "snippet              september comparable store sales -4%\n",
       "Name: 650, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post=pickle.load(open('post.dat',\"rb\"))\n",
    "post.iloc[650]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h_sentence,h_target,h_sentiment,h_aspect,h_snippet,h_features=prepare_data(fname['finance']['train_headline'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_sentence,p_target,p_sentiment,p_aspect,p_snippet,p_features=prepare_data(fname['finance']['train_post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_sentence[:1],p_target[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post=pd.DataFrame(data={'sentence':p_sentence,'sentiment':p_sentiment,'aspect':p_aspect,'snippet':p_snippet,'features':p_features})\n",
    "#pickle.dump(post,open('post.dat',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in p_sentence:\n",
    "    h_sentence.append(i)\n",
    "\n",
    "for i in p_sentiment:\n",
    "    h_sentiment.append(i)\n",
    "\n",
    "for i in p_aspect:\n",
    "    h_aspect.append(i)\n",
    "    \n",
    "for i in p_snippet:\n",
    "    h_snippet.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.DataFrame(data={'sentence': h_sentence,'sentiment':h_sentiment,'aspect':h_aspect,'snippet':h_snippet})\n",
    "pickle.dump(train_data,open(\"train_data_initial.dat\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-c4dda1b59424>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sentence[:2],p_target[:2],p_sentiment[:2],p_aspect[:2],p_snippet[:2],p_features[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_h_sentence,t_h_target,t_h_sentiment,t_h_aspect,t_h_features,t_h_snippet=prepare_data(fname['finance']['validation_headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "head=pd.DataFrame(data={'sentence':t_h_sentence,'sentiment':t_h_sentiment,'aspect':t_h_aspect,'snippets':t_h_snippet})\n",
    "pickle.dump(head,open('test_head.dat',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "post=pd.DataFrame(data={'sentence':t_p_sentence,'sentiment':t_p_sentiment,'aspect':t_p_aspect,'snippets':snippet})\n",
    "pickle.dump(post,open('test_post.dat',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p_sentence,t_p_target,t_p_sentiment,t_p_aspect,t_p_features,snippet=prepare_data(fname['finance']['validation_post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COMPANY currently way undervalued rise back soon',\n",
       " 'COMPANY q217 delivered 6% organic revenue de growth 3% reported net revenue de OCOMPANY q2 organic revenue 31% reported net revenue 20%',\n",
       " 'one stock oversold rsi 20 watch possible reverseal COMPANY',\n",
       " 'COMPANY added dip yesterday rick holding night i’m running away',\n",
       " 'COMPANY revenue falls 9% lowers full year revenue guidance',\n",
       " 'COMPANY 25k shares left u might sell monday',\n",
       " 'COMPANY shares slump third profit warning … via',\n",
       " 'COMPANY revenue growth 3% past year 20th consecutive yoy decline OCOMPANY',\n",
       " 'COMPANY downgraded neutral buy goldman pt',\n",
       " 'COMPANY 87% analysts buy rating OCOMPANY 20% upside',\n",
       " 'COMPANY sell pop market extreme overbought OCOMPANY OCOMPANY OCOMPANY',\n",
       " 'COMPANY ca na bk analyst dropped rating perform target price high likelihood dividend cut',\n",
       " 'COMPANY closed 8ema sold position',\n",
       " 'rsi overbought COMPANY momentum falling',\n",
       " 'COMPANY stock oversold shorted death dont really think matters er says shorts monday',\n",
       " 'google parent COMPANY stock alphabet gets rating upgrade investor business daily',\n",
       " 'COMPANY buy rating mkm partners pt',\n",
       " 'COMPANY ett sports shares plummet 30% profit warning ‘woefully late’ e commerce launch',\n",
       " 'COMPANY seriously bought',\n",
       " 'COMPANY overvaluedsorry folks merger costs housing slowdowns less paper product demand wy shares high',\n",
       " 'COMPANY 1007 please stop selling people get hold reality let get back ish buy buy buy buy buy buy buy',\n",
       " 'COMPANY exited riskyy',\n",
       " 'COMPANY shares rise earnings beat expectations',\n",
       " 'COMPANY revenue 21 percent',\n",
       " 'COMPANY dont catch falling knife taders protect',\n",
       " 'COMPANY gs reiterated sell rating isnt helping',\n",
       " 'COMPANY q2 revenue 11% yy exp eps exp',\n",
       " '5 star analyst robin farley ubs reiterated buy COMPANY',\n",
       " 'COMPANY takes hit earnings miss today',\n",
       " 'COMPANY markets overbought feel correction close',\n",
       " 'amazoncom inc COMPANY analyst rating upgrade',\n",
       " 'COMPANY selling pressure aided put buyers trying exploit opp thing creates bullish pressure call',\n",
       " 'twitter inc COMPANY rating lowered sell bidaskclub',\n",
       " 'COMPANY quarter position 810',\n",
       " 'COMPANY bubble stock',\n",
       " 'COMPANY sold position 9% profit',\n",
       " 'COMPANY selling pressure today 25%',\n",
       " 'COMPANY OCOMPANY OCOMPANY OCOMPANY OCOMPANY OCOMPANY waay hot due another tech sell',\n",
       " 'clsa COMPANY rating upgrade underperform sell taget rs 285',\n",
       " 'COMPANY OCOMPANY OCOMPANY OCOMPANY lol amazon tesla overvalued',\n",
       " 'sold COMPANY shares today buy back lower',\n",
       " 'COMPANY tesoro achieves investment grade credit rating standard poor upgrade',\n",
       " 'COMPANY overbought need reset technicals',\n",
       " 'COMPANY erpillar big revenue sales increases asia pacific sales marginally domestically',\n",
       " 'COMPANY stochastic oversold winning 6265% past',\n",
       " 'COMPANY plenty green early sign upbeat day coming buy back open u sold soon',\n",
       " 'COMPANY look overbought yet level',\n",
       " 'brexit drives profits advertising revenue COMPANY',\n",
       " 'stocks oversold undervalued need buy support COMPANY OCOMPANY OCOMPANY OCOMPANY OCOMPANY OCOMPANY OCOMPANY OCOMPANY',\n",
       " 'barclays reiterated mack cali realty COMPANY sell maybe wanna stay away value investors',\n",
       " 'COMPANY price comparison site slumps 4 years profit warning',\n",
       " 'COMPANY 3418 buy 5 thousand shares put side calling bottom always bullish sometimes bear',\n",
       " 'COMPANY investors like sell sold 50k shares plx know plan',\n",
       " 'check point software technologies ltd COMPANY receives outperform rating oppenheimer holdings inc',\n",
       " 'COMPANY looks like oversold already unless fundamentally bearish factor good first small entry',\n",
       " 'COMPANY im sold 1384',\n",
       " 'COMPANY accumulating',\n",
       " 'COMPANY nalyst activity – barclays plc reiterates buy agilent technologies nysea OCOMPANY',\n",
       " 'sensata technologies holding nv COMPANY shares gap following earnings beat',\n",
       " 'COMPANY gets raise',\n",
       " 'COMPANY shopify revenue 86% year year long',\n",
       " 'macquarie reiterates “buy” rating inc COMPANY',\n",
       " 'COMPANY back bullish',\n",
       " 'COMPANY overbought fomo traders',\n",
       " 'marketwatch COMPANY offers positive 2017 guidance q2 earnings beat shares wobble',\n",
       " 'COMPANY good entry right',\n",
       " 'sold remaining 13 COMPANY 30% average',\n",
       " 'bought back COMPANY previously sold',\n",
       " 'COMPANY great short entry earnings miss bad guide downgrade next week downside',\n",
       " 'COMPANY overbought shooting star candle friday lots bull ppl board 3 extremely reliable indicators impending doom next week',\n",
       " 'COMPANY overvalued 5176% mediocre fundamentals strong buy',\n",
       " 'COMPANY stock significantly undervalued fastener business alone worth current stock price cash cow',\n",
       " 'COMPANY downgraded market perform outperform fbr co',\n",
       " 'COMPANY guess ill buy another 1k shares',\n",
       " 'might buy COMPANY hours get dip earnings',\n",
       " 'amazoncom inc COMPANY downgraded zacks investment research “strong sell”',\n",
       " 'COMPANY domino overvalued leveraged COMPANY high flying sector market near time highs',\n",
       " 'COMPANY 3 day trading oversold 2844 rsi stock price reversal happen',\n",
       " 'COMPANY gravestone doji indicating downward selling pressure long upward trajectoryhealthy retest 108106',\n",
       " 'COMPANY notice insiders buying last 3 months selling',\n",
       " 'COMPANY looks overbought level time frame',\n",
       " 'COMPANY huge short interest retail oversold rsi 20 today big squeeze coming likely back upper 30s earnings',\n",
       " 'COMPANY citigroup inc issued sell rating price objective stock',\n",
       " 'COMPANY drops sharply bad earnings investors liking see say hello',\n",
       " 'COMPANY 10% increase revenue yoy 245 million revenue 25 million breakeven point total cost operatingdebt included',\n",
       " 'COMPANY still looks overpriced rated 17 price 3497',\n",
       " 'armour downgraded sell hold deutsche bank COMPANY',\n",
       " 'COMPANY recent significant insider buying',\n",
       " 'COMPANY added 2300 77 going long term hit next month',\n",
       " 'COMPANY insider selling',\n",
       " 'cantor fitzgerald sticks buy rating amazon COMPANY',\n",
       " 'COMPANY like insider buying taking place',\n",
       " 'bought COMPANY belief stock oversold dividend cut bad news priced',\n",
       " 'COMPANY undervalued goldplatinum stock world target 60',\n",
       " 'COMPANY lost much hard earned money sold 240 puts yesterday mad',\n",
       " 'COMPANY back swing one 1535sh last time 50% within 3 months',\n",
       " 'rsi stochastic oversold territory COMPANY',\n",
       " 'COMPANY stock overvalued ridiculous go back 80s',\n",
       " 'COMPANY earnings tanking']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pickle.load(open('test_head.dat',\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence=[]\n",
    "test_sentiment=[]\n",
    "test_aspect=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence,test_sentiment,test_aspect=headlines['sentence'],headlines['sentiment'],headlines['aspect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in headlines['sentence']:\n",
    "    test_sentence.append(i)\n",
    "\n",
    "for i in headlines['sentiment']:\n",
    "    test_sentiment.append(i)\n",
    "\n",
    "for i in headlines['aspect']:\n",
    "    test_aspect.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in t_p_sentence:\n",
    "    test_sentence.append(i)\n",
    "\n",
    "for i in t_p_sentiment:\n",
    "    test_sentiment.append(i)\n",
    "\n",
    "for i in t_p_aspect:\n",
    "    test_aspect.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_data=pd.DataFrame(data={'sentence': test_sentence,'sentiment':test_sentiment,'aspect':test_aspect})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>COMPANY uk personnel director quits supermarket</td>\n",
       "      <td>-0.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Strategy</td>\n",
       "      <td>companies COMPANY raises â£ ahead london listing</td>\n",
       "      <td>0.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Risks</td>\n",
       "      <td>COMPANY chief warns squeeze high street retailers</td>\n",
       "      <td>-0.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Strategy</td>\n",
       "      <td>COMPANY backs view buys businesses</td>\n",
       "      <td>0.487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Financial</td>\n",
       "      <td>COMPANY trading profit beats expectations</td>\n",
       "      <td>0.565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        aspect                                           sentence  sentiment\n",
       "0  Appointment    COMPANY uk personnel director quits supermarket     -0.223\n",
       "1     Strategy   companies COMPANY raises â£ ahead london listing      0.296\n",
       "2        Risks  COMPANY chief warns squeeze high street retailers     -0.397\n",
       "3     Strategy                 COMPANY backs view buys businesses      0.487\n",
       "4    Financial          COMPANY trading profit beats expectations      0.565"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Validation_data,open(\"Final_ValidationData.dat\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cantor fitzgerald sticks buy rating amazon COMPANY'],\n",
       " ['AMZN'],\n",
       " [0.247],\n",
       " ['Coverage'],\n",
       " [[]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p_sentence[:1],t_p_target[:1],t_p_sentiment[:1],t_p_aspect[:1],t_p_features[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA AUGMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_sentence_1=[]\n",
    "h_target_1=[]\n",
    "h_sentiment_1=[]\n",
    "\n",
    "p_sentence_1=[]\n",
    "p_target_1=[]\n",
    "p_sentiment_1=[]\n",
    "\n",
    "\n",
    "def prepare_data1(fname):\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            foo = json.load(f)\n",
    "            sentence_l=[]\n",
    "            target_l=[]\n",
    "            sentiment_l=[]\n",
    "            features=[]\n",
    "            for info in foo:\n",
    "                sentence=info['title']\n",
    "    \n",
    "                target= info['company']\n",
    "                features.append(extract_domain_specific_features(sentence))\n",
    "                    \n",
    "                #print(sentence,\" +Tar+ \",target)\n",
    "\n",
    "                m=[]\n",
    "                for i in sentence.partition(target):\n",
    "                        if i==target:\n",
    "                            m.append('COMPANY')\n",
    "\n",
    "                        else : m.append(i)\n",
    "                m=' '.join(m)\n",
    "                k=[]\n",
    "                for i in m.split():\n",
    "\n",
    "                    if i.lstrip('$')==target[0]:\n",
    "                            k.append('COMPANY')\n",
    "\n",
    "                    elif i ==target[0]:\n",
    "                            k.append('COMPANY')\n",
    "\n",
    "                    else: \n",
    "                        k.append(i)\n",
    "                k=' '.join(k)\n",
    "\n",
    "            \n",
    "                sentence = [clean_sentence(x) for x in k.split(\" \")]\n",
    "                            #print(sentence)\n",
    "                sentence=' '.join(sentence)\n",
    "\n",
    "                sentiment_score = info['sentiment']\n",
    "                                #print(sentiment_score)\n",
    "           \n",
    "                sentence=re.sub(' +', ' ',sentence)\n",
    "                sentence=sentence.strip()\n",
    "\n",
    "                sentence_l.append(sentence)\n",
    "                target_l.append(target)\n",
    "                sentiment_l.append(sentiment_score)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        return sentence_l,target_l,sentiment_l,features\n",
    "\n",
    "h_sentence_1,h_target_1,h_sentiment_1,h_features_1=prepare_data1(fname['Data_Augmentation']['sentiment']['validation_headline'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['COMPANY says uk store closures put jobs risk',\n",
       "  'COMPANY concrete bid holcim lafarge assets',\n",
       "  'crh concrete bid COMPANY assets',\n",
       "  'COMPANY share price slides underwhelming full year results',\n",
       "  'COMPANY bid mr bricolage runs trouble'],\n",
       " ['Tesco', 'CRH', 'Holcim Lafarge', 'Reed Elsevier', 'Kingfisher'],\n",
       " [-0.9, 0.3, 0.3, -0.9, -0.3],\n",
       " [['2000'], [], [], [], []])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_sentence_1[:5],h_target_1[:5],h_sentiment_1[:5],h_features_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_sentence_2,h_target_2,h_sentiment_2,h_features_2=prepare_data1(fname['Data_Augmentation']['sentiment']['train_headline'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data2(fname):\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            foo = json.load(f)\n",
    "            sentence_l=[]\n",
    "            target_l=[]\n",
    "            sentiment_l=[]\n",
    "            features=[]\n",
    "            for info in foo:\n",
    "                sentence=info['spans'][0]\n",
    "                #print(sentence)\n",
    "                target= info['cashtag']\n",
    "                \n",
    "                features.append(extract_domain_specific_features(sentence))\n",
    "                \n",
    "                m=[]\n",
    "                for i in sentence.partition(target):\n",
    "                        if i==target:\n",
    "                            m.append('COMPANY')\n",
    "\n",
    "                        else : m.append(i)\n",
    "                m=' '.join(m)\n",
    "               \n",
    "                #target= info['cashtag'].lstrip(\"$\")\n",
    "                #print(target)\n",
    "\n",
    "                sentiment_score = info['sentiment score']\n",
    "                #print(sentiment_score)\n",
    "        \n",
    "                sentence_l.append(m)\n",
    "                target_l.append(target)\n",
    "                sentiment_l.append(sentiment_score)\n",
    "        return sentence_l,target_l,sentiment_l,features\n",
    "                \n",
    "p_sentence_1,p_target_1,p_sentiment_1,p_features_1=prepare_data2(fname['Data_Augmentation']['sentiment']['validation_post'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Putting on a little  COMPANY  short'], ['$F'], [-0.454], [[]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sentence_1[:1],p_target_1[:1],p_sentiment_1[:1],p_features_1[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sentence_2,p_target_2,p_sentiment_2,p_features_2=prepare_data2(fname['Data_Augmentation']['sentiment']['train_post'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498, 675)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h_sentence), len(p_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 10)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h_sentence_1), len(p_sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1142, 1700)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h_sentence_2), len(p_sentence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence\n",
    "for i in h_sentence_1:\n",
    "    h_sentence.append(i)\n",
    "for i in h_sentence_2:\n",
    "    h_sentence.append(i)\n",
    "    \n",
    "for i in p_sentence_1:\n",
    "    p_sentence.append(i)\n",
    "for i in p_sentence_2:\n",
    "    p_sentence.append(i)\n",
    "\n",
    "#Sentiment\n",
    "for i in h_sentiment_1:\n",
    "    h_sentiment.append(i)\n",
    "for i in h_sentiment_2:\n",
    "    h_sentiment.append(i)\n",
    "\n",
    "for i in p_sentiment_1:\n",
    "    p_sentiment.append(i)\n",
    "for i in p_sentiment_2:\n",
    "    p_sentiment.append(i)\n",
    "    \n",
    "#Features\n",
    "for i in h_features_1:\n",
    "    h_features.append(i)\n",
    "for i in h_features_2:\n",
    "    h_features.append(i)\n",
    "\n",
    "for i in p_features_1:\n",
    "    p_features.append(i)\n",
    "for i in p_features_2:\n",
    "    p_features.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1654, 2385)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h_sentence), len(p_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in p_sentence:\n",
    "    h_sentence.append(i)\n",
    "for i in p_sentiment:\n",
    "    h_sentiment.append(i)\n",
    "for i in p_features:\n",
    "    h_features.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4039"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'sentence':h_sentence,\n",
    "     'sentiment':h_sentiment,\n",
    "     'features':h_features}\n",
    "pickle.dump(data,open(\"Final_TrainData.dat\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pickle.load(open(\"Final_TrainData.dat\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4039"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_google_word2vec(file_name):\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)\n",
    "\n",
    "\n",
    "def build_embedding_matrix(vocab_size, embed_dim, tokenizer ):\n",
    "    \n",
    "    embedding_matrix_file_name='D:/PythonCodes/Sentiment-Analysis/Data/embedding_matrix_sentiment.dat'\n",
    "    \n",
    "    if os.path.exists(embedding_matrix_file_name):\n",
    "        print('loading embedding_matrix:', embedding_matrix_file_name)\n",
    "        embedding_matrix = pickle.load(open(embedding_matrix_file_name, 'rb'))\n",
    "   \n",
    "    else:\n",
    "        print('loading word vectors...')\n",
    "        fname = 'D:\\PythonCodes\\Jupyter notebooks\\Word Embeddings\\GoogleNews-vectors-negative300.bin' \n",
    "\n",
    "        model=load_google_word2vec(fname)\n",
    "\n",
    "        embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "\n",
    "            try:\n",
    "                embedding_vector = model[word]\n",
    "            except KeyError:\n",
    "                embedding_vector = None\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i]=embedding_vector\n",
    "\n",
    "\n",
    "        pickle.dump(embedding_matrix, open(embedding_matrix_file_name, 'wb'))\n",
    "\n",
    "\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding_matrix: D:/PythonCodes/Sentiment-Analysis/Data/embedding_matrix_sentiment.dat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5357"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_file_name='D:/PythonCodes/Sentiment-Analysis/Data/embedding_matrix_sentiment.dat'\n",
    "    \n",
    "if os.path.exists(embedding_matrix_file_name):\n",
    "    print('loading embedding_matrix:', embedding_matrix_file_name)\n",
    "    embedding_matrix = pickle.load(open(embedding_matrix_file_name, 'rb'))\n",
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT RESCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(series,old_range,new_range):\n",
    "    m = interp1d(old_range,new_range)\n",
    "    return [float(m(x)) for x in series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rescale' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-29058c4d8dd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrescale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rescale' is not defined"
     ]
    }
   ],
   "source": [
    "sentiment = rescale(data['sentiment'],[-1,1],[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_sentiment = rescale(t_h_sentiment,[-1,1],[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_sentiment = rescale(t_p_sentiment,[-1,1],[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "c = list(zip(data['sentiment'], data['sentence'],data['features']))\n",
    "random.shuffle(c)\n",
    "data['sentiment'], data['sentence'],data['features']= zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(data['sentence'])\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5357"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Bad Apple  ',\n",
       " 'intertek profit expects meet forecast',\n",
       " 'Reuters: Green Mountain revenue misses, shares plunge http://stks.co/13mW > $ COMPANY prints 43.80, market in a foul mood, bad day to disappoint',\n",
       " 'Morrisons and COMPANY surprise City with Christmas bounce back',\n",
       " 'In Play Longs  ',\n",
       " 'aldi lidl expansion plans speed ahead tesco COMPANY morrisons',\n",
       " 'Monday sold  ',\n",
       " 'helge lund moves COMPANY month early',\n",
       " 'Drugs and Biotech Getting Perky  ',\n",
       " 'long  ')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentence'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=[]\n",
    "for i in data['sentence'] :\n",
    "    if \"COMPANY\" in i :\n",
    "        target.append('COMPANY')\n",
    "    else:\n",
    "        target.append(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aldi lidl expansion plans speed ahead tesco COMPANY morrisons'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentence'][5]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1227, 1228,  400,  215,  588,  166,   47,    1,  345,    0,    0,\n",
       "          0,    0,    0,    0])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=15\n",
    "trainX= encode_text(tokenizer, data['sentence'], max_length)\n",
    "target= encode_text(tokenizer, target, 1)\n",
    "trainY= sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.tile(target,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding_matrix: D:/PythonCodes/Sentiment-Analysis/Data/embedding_matrix_sentiment.dat\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = build_embedding_matrix(vocab_size, 300,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5357"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval('embedding_matrix'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_X= encode_text(tokenizer, t_h_sentence, max_length)\n",
    "postX =encode_text(tokenizer, t_p_sentence, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented_test={'data':data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data_augmented_test,open(\"Sentiment.dat\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_sentiment={\n",
    "    'trainX': trainX,\n",
    "    'target':target,\n",
    "    'trainY':trainY,\n",
    "    'trainFeatures':h_features,\n",
    "    'HEAD_testX':head_X,\n",
    "    'HEAD_testY':head_sentiment,\n",
    "    'POST_testX':postX,\n",
    "    'POST_testY':post_sentiment,\n",
    "    'embedding_matrix':embedding_matrix,\n",
    "    'vocab_size':vocab_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data_for_sentiment,open(\"ALLdataForSentiment.dat\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
