{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Lambda,Reshape,concatenate,Input, Embedding, LSTM\n",
    "from keras.layers import Dense,Dropout, Activation ,Flatten ,RepeatVector, Bidirectional,GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.activations import softmax\n",
    "from keras import regularizers\n",
    "\n",
    "from keras import backend as K, regularizers, constraints, initializers\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from Attention import Attention\n",
    "\n",
    "from keras.layers import Concatenate,Dot\n",
    "from keras.layers import Permute, merge\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk import pos_tag\n",
    "from string import punctuation,digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    list_punctuation = list(punctuation)\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,'')\n",
    "    return s.lower()\n",
    "\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', 'URL', sentence)\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", 'MENTION', sentence)\n",
    "    sentence = sentence.replace('#(\\w+)','HASHTAG')\n",
    "    sentence = sentence.replace('$(\\w+)','TARGET')\n",
    "    sentence = sentence.replace(\"'s\",' ')\n",
    "    sentence = sentence.replace(\"-\",' ')\n",
    "    tokens = sentence.split()\n",
    "    tokens = [remove_punctuation(w) for w in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    tokens = [w.translate(remove_digits) for w in tokens]\n",
    "    tokens = [w.strip() for w in tokens]\n",
    "    tokens = [w for w in tokens if w!=\"\"]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1=pickle.load(open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Data\\train_data_augmented.dat\",\"rb\"))\n",
    "head=pickle.load(open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Data\\head.dat\",\"rb\"))\n",
    "post=pickle.load(open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Data\\post.dat\",\"rb\"))\n",
    "\n",
    "a=pickle.load(open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Data\\all_data.dat\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=dataset1['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['p_sentiment', 'aug_aspect', 'lable_encoding', 'h_aspect_encoding', 'text_raw', 'h_target', 'train_sentiment', 'h_aspect', 'input_aspect', 'validation_X', 'p_aspect', 'h_sentence', 'embedding_matrix', 'aspect_level1', 'validation_Y', 'p_target', 'vocab_size', 'aug_sentence', 'aspect_level2', 'p_sentence', 'aspect', 'test_raw', 'sentence', 'target', 'h_sentiment', 'p_aspect_encoding'])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_pred(pred):\n",
    "    return [label_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    }
   ],
   "source": [
    "input_context=a['aug_sentence']\n",
    "#input_target=a['target']\n",
    "\n",
    "label_encoding=a['lable_encoding']\n",
    "\n",
    "embedding_matrix=a['embedding_matrix']\n",
    "\n",
    "sentiment= a['train_sentiment']\n",
    "aspect_encoded=a['aug_aspect']\n",
    "\n",
    "h_sentence=a['h_sentence']\n",
    "h_aspect_encoding=a['h_aspect_encoding']\n",
    "h_sentiment= a['h_sentiment']\n",
    "\n",
    "\n",
    "p_sentence=a['p_sentence']\n",
    "p_aspect_encoding=a['p_aspect_encoding']\n",
    "p_sentiment= a['p_sentiment']\n",
    "\n",
    "\n",
    "\n",
    "y=[]\n",
    "for i in h_aspect_encoding:\n",
    "    y.append(list(i))\n",
    "for j in p_aspect_encoding:\n",
    "    y.append(list(j)) \n",
    "print(len(y))\n",
    "y=np.array(y)\n",
    "\n",
    "validation_X=a['validation_X']\n",
    "vocab_size=a['vocab_size']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_text= []\n",
    "for i in head['h_sentence']:\n",
    "    validation_text.append(clean_sentence(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3727"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect=dataset1['aspect']\n",
    "input_text=[]\n",
    "for i in dataset1['sentence']:\n",
    "    input_text.append(clean_sentence(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amzn going thru roof needs sell'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "X_train = vectorizer.fit_transform(input_text)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train,aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1470458553791887"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vectorizer.transform(head['h_sentence'])\n",
    "predictions = classifier.predict(X_test)\n",
    "f1_score(head['aspect'],predictions,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10545106931661555"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "forest = RandomForestClassifier(n_estimators = 300) \n",
    "forest = forest.fit( X_train, aspect)\n",
    "predictions = forest.predict(X_test)\n",
    "f1_score(head['aspect'],predictions,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(input_text)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10949252615038314"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=clf.predict(count_vect.transform(head['h_sentence']))\n",
    "f1_score(head['aspect'],predictions,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, aspect)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12576378963938772"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(head['aspect'],y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Attention to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_2(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag,em_dim):\n",
    "     \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "        \n",
    "    a = Bidirectional(LSTM(300, return_sequences=True,recurrent_dropout=dropout))(context)\n",
    "    \n",
    "    print(a.shape)\n",
    "\n",
    "    alpha = Attention()(a)\n",
    "    \n",
    "    x=Dense(int((2*lstm_out+27)/2),activation='relu')(alpha)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "    \n",
    "    model= Model(inputs=input_context ,outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics = ['accuracy'])\n",
    "    \n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 600)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Context (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 11, 300)           1118100   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 11, 600)           1442400   \n",
      "_________________________________________________________________\n",
      "attention_2 (Attention)      (None, 600)               611       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 313)               188113    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 27)                8478      \n",
      "=================================================================\n",
      "Total params: 2,757,702\n",
      "Trainable params: 1,639,602\n",
      "Non-trainable params: 1,118,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attention_2 = attention_2(learning_rate=0.00069,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1343/1343 [==============================] - 16s 12ms/step - loss: 2.5115 - acc: 0.3552\n",
      "Epoch 2/100\n",
      "1343/1343 [==============================] - 12s 9ms/step - loss: 1.6936 - acc: 0.5443\n",
      "Epoch 3/100\n",
      "1343/1343 [==============================] - 13s 10ms/step - loss: 1.2348 - acc: 0.6575\n",
      "Epoch 4/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.9288 - acc: 0.7215\n",
      "Epoch 5/100\n",
      "1343/1343 [==============================] - 13s 10ms/step - loss: 0.7106 - acc: 0.7870\n",
      "Epoch 6/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.5657 - acc: 0.8310\n",
      "Epoch 7/100\n",
      "1343/1343 [==============================] - 13s 10ms/step - loss: 0.4455 - acc: 0.8675\n",
      "Epoch 8/100\n",
      "1343/1343 [==============================] - 13s 10ms/step - loss: 0.3710 - acc: 0.8928\n",
      "Epoch 9/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.2964 - acc: 0.9077\n",
      "Epoch 10/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.2560 - acc: 0.9248\n",
      "Epoch 11/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.2021 - acc: 0.9427\n",
      "Epoch 12/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.1656 - acc: 0.9501\n",
      "Epoch 13/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.1538 - acc: 0.9583\n",
      "Epoch 14/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.1498 - acc: 0.9568\n",
      "Epoch 15/100\n",
      "1343/1343 [==============================] - 13s 10ms/step - loss: 0.1186 - acc: 0.9695\n",
      "Epoch 16/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.1049 - acc: 0.9687\n",
      "Epoch 17/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.0961 - acc: 0.9710\n",
      "Epoch 18/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.0885 - acc: 0.9732\n",
      "Epoch 19/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.0894 - acc: 0.9769\n",
      "Epoch 20/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.0727 - acc: 0.9762\n",
      "Epoch 21/100\n",
      "1343/1343 [==============================] - 13s 10ms/step - loss: 0.0589 - acc: 0.9844\n",
      "Epoch 22/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.0808 - acc: 0.9747\n",
      "Epoch 23/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.0841 - acc: 0.9680\n",
      "Epoch 24/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.0510 - acc: 0.9836\n",
      "Epoch 25/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.0353 - acc: 0.9903\n",
      "Epoch 26/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.0339 - acc: 0.9911\n",
      "Epoch 27/100\n",
      "1343/1343 [==============================] - 13s 10ms/step - loss: 0.0324 - acc: 0.9888\n",
      "Epoch 28/100\n",
      "1343/1343 [==============================] - 13s 9ms/step - loss: 0.0480 - acc: 0.9836\n",
      "Epoch 29/100\n",
      "1343/1343 [==============================] - 12s 9ms/step - loss: 0.0594 - acc: 0.9829\n",
      "Epoch 30/100\n",
      "1343/1343 [==============================] - 8s 6ms/step - loss: 0.0592 - acc: 0.9792\n",
      "Epoch 31/100\n",
      "1343/1343 [==============================] - 9s 7ms/step - loss: 0.0504 - acc: 0.9866\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a62181ea90>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "attention_2.fit(x=a['sentence'],y=aspect, epochs=100,batch_size=16,callbacks=[earlystopper])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1208631889552942"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=attention_2.predict(h_sentence)\n",
    "predictions=get_class_from_pred(predictions)\n",
    "f1_score(head['aspect'],predictions,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=11\n",
    "def cnn_4(dropout,learning_rate,em,em_dim,em_trainable_flag,num_filters=150):\n",
    "    \n",
    "        # channel 1\n",
    "        inputs1 = Input(shape=(length,))\n",
    "        embedding1 = Embedding(vocab_size, 300)(inputs1)\n",
    "        \n",
    "        q = Bidirectional(LSTM(327,return_sequences=True,recurrent_dropout=dropout))(embedding1)\n",
    "        q = Bidirectional(LSTM(327,return_sequences=True,recurrent_dropout=dropout))(q)\n",
    "\n",
    "\n",
    "\n",
    "        conv1 = Conv1D(filters=num_filters, kernel_size=1, activation='relu')(q)\n",
    "        drop1 = Dropout(0.2)(conv1)\n",
    "        pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "        flat1 = Flatten()(pool1)\n",
    "        #q1 = Bidirectional(LSTM(300,return_sequences=True,recurrent_dropout=dropout))(pool1)\n",
    "\n",
    "        \n",
    "        # channel 2\n",
    "        inputs2 = Input(shape=(length,))\n",
    "        embedding2 = Embedding(vocab_size, 300)(inputs2)\n",
    "        conv2 = Conv1D(filters=num_filters, kernel_size=2, activation='relu')(embedding2)\n",
    "        drop2 = Dropout(0.2)(conv2)\n",
    "        pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "        flat2 = Flatten()(pool2)\n",
    "        #q2 = Bidirectional(LSTM(300,return_sequences=True,recurrent_dropout=dropout))(pool2)\n",
    "\n",
    "        # channel 3\n",
    "        inputs3 = Input(shape=(length,))\n",
    "        embedding3 = Embedding(vocab_size, 300)(inputs3)\n",
    "        conv3 = Conv1D(filters=num_filters, kernel_size=3, activation='relu')(embedding3)\n",
    "        drop3 = Dropout(0.2)(conv3)\n",
    "        pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "        flat3 = Flatten()(pool3)\n",
    "        #q3 = Bidirectional(LSTM(300,return_sequences=False,recurrent_dropout=dropout))(pool3)\n",
    "\n",
    "        \n",
    "        # merge\n",
    "        merged = concatenate([flat1, flat2, flat3])\n",
    "        # interpretation\n",
    "        \n",
    "        dense1 = Dense(80, activation='relu')(merged)\n",
    "        outputs = Dense(27, activation='softmax')(dense1)\n",
    "        model = Model(inputs=[inputs1,inputs2,inputs3], outputs=outputs)\n",
    "        # compile\n",
    "        optimizer=Adam(lr=learning_rate)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        # summarize\n",
    "        print(model.summary())\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_32 (InputLayer)           (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_34 (Embedding)        (None, 11, 300)      1118100     input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 11, 654)      1642848     embedding_34[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_33 (InputLayer)           (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_34 (InputLayer)           (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 11, 654)      2568912     bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_35 (Embedding)        (None, 11, 300)      1118100     input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_36 (Embedding)        (None, 11, 300)      1118100     input_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 11, 150)      98250       bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 10, 150)      90150       embedding_35[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 9, 150)       135150      embedding_36[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 11, 150)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 10, 150)      0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 9, 150)       0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 3, 150)       0           dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 3, 150)       0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 3, 150)       0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 450)          0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 450)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 450)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1350)         0           flatten_29[0][0]                 \n",
      "                                                                 flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 327)          441777      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 27)           8856        dense_22[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,340,243\n",
      "Trainable params: 8,340,243\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn_model_4 = cnn_4(dropout=0.2,\n",
    "                     learning_rate=0.001,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(monitor='loss', patience=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1343 samples, validate on 93 samples\n",
      "Epoch 1/50\n",
      "1343/1343 [==============================] - 76s 57ms/step - loss: 0.1249 - acc: 0.9661 - val_loss: 0.1740 - val_acc: 0.9602\n",
      "Epoch 2/50\n",
      "1343/1343 [==============================] - 68s 51ms/step - loss: 0.0780 - acc: 0.9773 - val_loss: 0.1797 - val_acc: 0.9586\n",
      "Epoch 3/50\n",
      "1343/1343 [==============================] - 68s 51ms/step - loss: 0.0298 - acc: 0.9909 - val_loss: 0.2255 - val_acc: 0.9550\n",
      "Epoch 4/50\n",
      "1343/1343 [==============================] - 69s 51ms/step - loss: 0.0073 - acc: 0.9979 - val_loss: 0.2347 - val_acc: 0.9546\n",
      "Epoch 5/50\n",
      "1343/1343 [==============================] - 69s 51ms/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.2525 - val_acc: 0.9518\n",
      "Epoch 6/50\n",
      "1343/1343 [==============================] - 68s 51ms/step - loss: 0.0024 - acc: 0.9996 - val_loss: 0.2641 - val_acc: 0.9522\n",
      "Epoch 7/50\n",
      "1343/1343 [==============================] - 68s 51ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.2664 - val_acc: 0.9490\n",
      "Epoch 8/50\n",
      "1343/1343 [==============================] - 70s 52ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.2612 - val_acc: 0.9486\n",
      "Epoch 9/50\n",
      "1343/1343 [==============================] - 76s 57ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.2862 - val_acc: 0.9450\n",
      "Epoch 10/50\n",
      "1343/1343 [==============================] - 73s 54ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.2987 - val_acc: 0.9466\n",
      "Epoch 11/50\n",
      "  16/1343 [..............................] - ETA: 1:10 - loss: 3.1790e-05 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-4e6a1ae68f57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcnn_model_4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maspect_encoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlystopper\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh_sentence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh_sentence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh_sentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh_aspect_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "cnn_model_4.fit(x=[a['sentence'],a['sentence'],a['sentence']],y=aspect_encoded, epochs=50,batch_size=8,callbacks=[earlystopper],validation_data=([h_sentence,h_sentence,h_sentence],h_aspect_encoding))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0738205365402405"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=cnn_model_4.predict([h_sentence,h_sentence,h_sentence])\n",
    "predictions=get_class_from_pred(predictions)\n",
    "f1_score(head['aspect'],predictions,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1533531746031746"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=cnn_model_4.predict([p_sentence,p_sentence,p_sentence])\n",
    "predictions=get_class_from_pred(predictions)\n",
    "f1_score(post['aspect'],predictions,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_filters = 100\n",
    "    h_sentence :0.106\n",
    "    p_sentence :0.204\n",
    "n_filters = 150 \n",
    "    h_sentence :0.132\n",
    "    p_sentence :0.195\n",
    "n_dense_layer = 317\n",
    "    h_sentence = 0.091\n",
    "    p=0.29\n",
    "1 Bi LSTM LAYER \n",
    "h -0.111\n",
    "p-0.153"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "class Data(object):\n",
    "    \"\"\"\n",
    "    Class to handle loading and processing of raw datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source,aspect_source,\n",
    "                 alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\",\n",
    "                 input_size=1014, num_of_classes=27):\n",
    "        \"\"\"\n",
    "        Initialization of a Data object.\n",
    "        Args:\n",
    "            alphabet (str): Alphabet of characters to index\n",
    "            input_size (int): Size of input features\n",
    "            num_of_classes (int): Number of classes in data\n",
    "        \"\"\"\n",
    "        self.alphabet = alphabet\n",
    "        self.alphabet_size = len(self.alphabet)\n",
    "        self.dict = {}  # Maps each character to an integer\n",
    "        self.no_of_classes = num_of_classes\n",
    "        for idx, char in enumerate(self.alphabet):\n",
    "            self.dict[char] = idx + 1\n",
    "        self.length = input_size\n",
    "        self.data_source=data_source\n",
    "        self.aspect_source=aspect_source\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load raw data from the source file into data variable.\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for row in self.data_source:\n",
    "                txt = \"\"\n",
    "                for s in row:\n",
    "                    txt = txt + \" \" + re.sub(\"^\\s*(.-)\\s*$\", \"%1\", s).replace(\"\\\\n\", \"\\n\")\n",
    "                data.append(txt)  # format: (label, text)\n",
    "                \n",
    "        self.data = np.array(data)\n",
    "        print(\"Data loaded \")\n",
    "\n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Return all loaded data from data variable.\n",
    "        Returns:\n",
    "            (np.ndarray) Data transformed from raw to indexed form with associated one-hot label.\n",
    "        \"\"\"\n",
    "        data_size = len(self.data)\n",
    "        start_index = 0\n",
    "        end_index = data_size\n",
    "        batch_texts = self.data[start_index:end_index]\n",
    "        batch_indices = []\n",
    "        one_hot = np.eye(self.no_of_classes, dtype='int64')\n",
    "        classes = []\n",
    "        for s in batch_texts:\n",
    "            batch_indices.append(self.str_to_indexes(s))\n",
    "            \n",
    "        return np.asarray(batch_indices, dtype='int64'), self.aspect_source\n",
    "\n",
    "    def str_to_indexes(self, s):\n",
    "        \"\"\"\n",
    "        Convert a string to character indexes based on character dictionary.\n",
    "        \n",
    "        Args:\n",
    "            s (str): String to be converted to indexes\n",
    "        Returns:\n",
    "            str2idx (np.ndarray): Indexes of characters in s\n",
    "        \"\"\"\n",
    "        s = s.lower()\n",
    "        max_length = min(len(s), self.length)\n",
    "        str2idx = np.zeros(self.length, dtype='int64')\n",
    "        for i in range(1, max_length + 1):\n",
    "            c = s[-i]\n",
    "            if c in self.dict:\n",
    "                str2idx[i - 1] = self.dict[c]\n",
    "        return str2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded \n"
     ]
    }
   ],
   "source": [
    "training_data = Data(    \n",
    "                         data_source=input_text,\n",
    "                         aspect_source=aspect_encoded,\n",
    "                         alphabet=alphabet,\n",
    "                         input_size=1014,\n",
    "                         num_of_classes=27)\n",
    "training_data.load_data()\n",
    "training_inputs, training_labels = training_data.get_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded \n"
     ]
    }
   ],
   "source": [
    "validation_data = Data(    data_source=validation_text,\n",
    "                           aspect_source=h_aspect_encoding,\n",
    "                           alphabet=alphabet,\n",
    "                           input_size=1014,\n",
    "                           num_of_classes=27)\n",
    "validation_data.load_data()\n",
    "validation_inputs, validation_labels = validation_data.get_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D\n",
    "from keras.layers import AlphaDropout\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "\n",
    "class CharCNNKim(object):\n",
    "   \n",
    "    def __init__(self,optimizer='adam', loss='categorical_crossentropy'):\n",
    "        \"\"\"\n",
    "        Initialization for the Character Level CNN model.\n",
    "        Args:\n",
    "            input_size (int): Size of input features\n",
    "            alphabet_size (int): Size of alphabets to create embeddings for\n",
    "            embedding_size (int): Size of embeddings\n",
    "            conv_layers (list[list[int]]): List of Convolution layers for model\n",
    "            fully_connected_layers (list[list[int]]): List of Fully Connected layers for model\n",
    "            num_of_classes (int): Number of classes in data\n",
    "            dropout_p (float): Dropout Probability\n",
    "            optimizer (str): Training optimizer\n",
    "            loss (str): Loss function\n",
    "        \"\"\"\n",
    "        self.input_size = 1014\n",
    "        self.alphabet_size = 69\n",
    "        self.embedding_size = 128\n",
    "        self.conv_layers =[\n",
    "                            [\n",
    "                                256,\n",
    "                                10\n",
    "                              ],\n",
    "                              [\n",
    "                                256,\n",
    "                                7\n",
    "                              ],\n",
    "                              [\n",
    "                                256,\n",
    "                                5\n",
    "                              ],\n",
    "                              [\n",
    "                                256,\n",
    "                                3\n",
    "                              ]\n",
    "                            ]\n",
    "        self.fully_connected_layers = [1024,1024]\n",
    "        self.num_of_classes =27\n",
    "        self.dropout_p = 0.5\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self._build_model()  # builds self.model variable\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build and compile the Character Level CNN model\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        inputs = Input(shape=(self.input_size,), name='sent_input', dtype='int64')\n",
    "        # Embedding layers\n",
    "        x = Embedding(self.alphabet_size + 1, self.embedding_size, input_length=self.input_size)(inputs)\n",
    "        # Convolution layers\n",
    "        convolution_output = []\n",
    "        for num_filters, filter_width in self.conv_layers:\n",
    "            conv = Convolution1D(filters=num_filters,\n",
    "                                 kernel_size=filter_width,\n",
    "                                 activation='tanh',\n",
    "                                 name='Conv1D_{}_{}'.format(num_filters, filter_width))(x)\n",
    "            pool = GlobalMaxPooling1D(name='MaxPoolingOverTime_{}_{}'.format(num_filters, filter_width))(conv)\n",
    "            convolution_output.append(pool)\n",
    "        x = Concatenate()(convolution_output)\n",
    "        # Fully connected layers\n",
    "        for fl in self.fully_connected_layers:\n",
    "            x = Dense(fl, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "            x = AlphaDropout(self.dropout_p)(x)\n",
    "        # Output layer\n",
    "        predictions = Dense(self.num_of_classes, activation='softmax')(x)\n",
    "        # Build and compile model\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        model.compile(optimizer=self.optimizer, loss=self.loss)\n",
    "        self.model = model\n",
    "        print(\"CharCNNKim model built: \")\n",
    "        self.model.summary()\n",
    "\n",
    "\n",
    "    def train(self, training_inputs, training_labels,\n",
    "              validation_inputs, validation_labels,\n",
    "              epochs, batch_size, checkpoint_every=100):\n",
    "        \"\"\"\n",
    "        Training function\n",
    "        Args:\n",
    "            training_inputs (numpy.ndarray): Training set inputs\n",
    "            training_labels (numpy.ndarray): Training set labels\n",
    "            validation_inputs (numpy.ndarray): Validation set inputs\n",
    "            validation_labels (numpy.ndarray): Validation set labels\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size\n",
    "            checkpoint_every (int): Interval for logging to Tensorboard\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        tensorboard = TensorBoard(log_dir='./logs', histogram_freq=checkpoint_every, batch_size=batch_size,\n",
    "                                  write_graph=False, write_grads=True, write_images=False,\n",
    "                                  embeddings_freq=checkpoint_every,\n",
    "                                  embeddings_layer_names=None)\n",
    "        # Start training\n",
    "        print(\"Training CharCNNKim model: \")\n",
    "        self.model.fit(training_inputs, training_labels,\n",
    "                       validation_data=(validation_inputs, validation_labels),\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       verbose=2,\n",
    "                       callbacks=[tensorboard])\n",
    "\n",
    "    def test(self, testing_inputs, testing_labels, batch_size):\n",
    "        \"\"\"\n",
    "        Testing function\n",
    "        Args:\n",
    "            testing_inputs (numpy.ndarray): Testing set inputs\n",
    "            testing_labels (numpy.ndarray): Testing set labels\n",
    "            batch_size (int): Batch size\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Evaluate inputs\n",
    "        self.model.evaluate(testing_inputs, testing_labels, batch_size=batch_size, verbose=1)\n",
    "        # self.model.predict(testing_inputs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "dictionary=dict()\n",
    "for idx, char in enumerate(alphabet):\n",
    "            dictionary[char] = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharCNNKim model built: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sent_input (InputLayer)         (None, 1014)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1014, 128)    8960        sent_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D_256_10 (Conv1D)          (None, 1005, 256)    327936      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D_256_7 (Conv1D)           (None, 1008, 256)    229632      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D_256_5 (Conv1D)           (None, 1010, 256)    164096      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D_256_3 (Conv1D)           (None, 1012, 256)    98560       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "MaxPoolingOverTime_256_10 (Glob (None, 256)          0           Conv1D_256_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "MaxPoolingOverTime_256_7 (Globa (None, 256)          0           Conv1D_256_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "MaxPoolingOverTime_256_5 (Globa (None, 256)          0           Conv1D_256_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "MaxPoolingOverTime_256_3 (Globa (None, 256)          0           Conv1D_256_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1024)         0           MaxPoolingOverTime_256_10[0][0]  \n",
      "                                                                 MaxPoolingOverTime_256_7[0][0]   \n",
      "                                                                 MaxPoolingOverTime_256_5[0][0]   \n",
      "                                                                 MaxPoolingOverTime_256_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_1 (AlphaDropout)  (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         1049600     alpha_dropout_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "alpha_dropout_2 (AlphaDropout)  (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 27)           27675       alpha_dropout_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 2,956,059\n",
      "Trainable params: 2,956,059\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CharCNNKim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CharCNNKim model: \n",
      "Train on 1343 samples, validate on 93 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-e4f48b9a3116>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                 checkpoint_every=100)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-91-928a2fad96ae>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, training_inputs, training_labels, validation_inputs, validation_labels, epochs, batch_size, checkpoint_every)\u001b[0m\n\u001b[0;32m    108\u001b[0m                        \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                        \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                        callbacks=[tensorboard])\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(training_inputs=training_inputs,\n",
    "                training_labels=training_labels,\n",
    "                validation_inputs=validation_inputs,\n",
    "                validation_labels=validation_labels,\n",
    "                epochs=30,\n",
    "                batch_size=128,\n",
    "                checkpoint_every=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(testing_inputs=validation_inputs, \n",
    "           testing_labels=validation_labels, \n",
    "           batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
