{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Lambda,Reshape,concatenate,Input, Embedding, LSTM,GRU\n",
    "from keras.layers import Dense,Dropout, Activation ,Flatten ,RepeatVector, Bidirectional,GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.activations import softmax\n",
    "from keras import regularizers\n",
    "\n",
    "from keras import backend as K, regularizers, constraints, initializers\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras.layers.convolutional import Conv1D,Conv2D\n",
    "from keras.layers.convolutional import MaxPooling1D,MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from Attention import Attention\n",
    "\n",
    "from keras.layers import Concatenate,Dot\n",
    "from keras.layers import Permute, merge\n",
    "\n",
    "# FOR ATAE\n",
    "from AttentionwithContext import AttentionWithContext\n",
    "from Final import FinalSentenceRepresentation\n",
    "\n",
    "from final2 import Final2\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import keras\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import codecs\n",
    "import nltk\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk import pos_tag\n",
    "from string import punctuation,digits\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import f1_score,accuracy_score\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pickle.load(open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Code\\train_data_initial.dat\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_data=pickle.load(open('test_head.dat',\"rb\"))\n",
    "post_data=pickle.load(open('test_post.dat',\"rb\"))\n",
    "dataframe=pickle.load(open('dataframe.dat',\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_mse(y_true,y_pred):\n",
    "     return np.mean(np.square(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a=pickle.load(open(\"D:/PythonCodes/Sentiment-Analysis/Code/ALLdataForSentiment.dat\",\"rb\"))\n",
    "\n",
    "a.keys()\n",
    "\n",
    "trainX=a['trainX']\n",
    "trainY=a['trainY']\n",
    "embedding_matrix=a['embedding_matrix']\n",
    "head_X=a['HEAD_testX']\n",
    "head_Y=a['HEAD_testY']\n",
    "post_Y=a['POST_testY']\n",
    "post_X=a['POST_testX']\n",
    "target = a['target']\n",
    "def sk_mse(y_true,y_pred):\n",
    "     return np.mean(np.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "\n",
    "max_length=15\n",
    "\n",
    "vocab_size=a['vocab_size']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAKING ONLY POST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>features</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPANY big money pouring facebook directions</td>\n",
       "      <td>0.645</td>\n",
       "      <td>big money is pouring into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPANY guess want look sell orders</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>guess they want it down look at the those sell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Technical Analysis</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPANY bounced support early week indicators ...</td>\n",
       "      <td>0.351</td>\n",
       "      <td>now indicators turning up (rsi, macd, smi)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>[]</td>\n",
       "      <td>OCOMPANY OCOMPANY COMPANY long morning</td>\n",
       "      <td>0.476</td>\n",
       "      <td>long this morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>[2]</td>\n",
       "      <td>COMPANY pos saying years going lower forget go...</td>\n",
       "      <td>-0.631</td>\n",
       "      <td>going lower. forget about it or go short.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               aspect features  \\\n",
       "0        Price Action       []   \n",
       "1        Price Action       []   \n",
       "2  Technical Analysis       []   \n",
       "3        Price Action       []   \n",
       "4        Price Action      [2]   \n",
       "\n",
       "                                            sentence sentiment  \\\n",
       "0      COMPANY big money pouring facebook directions     0.645   \n",
       "1                COMPANY guess want look sell orders    -0.465   \n",
       "2  COMPANY bounced support early week indicators ...     0.351   \n",
       "3             OCOMPANY OCOMPANY COMPANY long morning     0.476   \n",
       "4  COMPANY pos saying years going lower forget go...    -0.631   \n",
       "\n",
       "                                             snippet  \n",
       "0                          big money is pouring into  \n",
       "1  guess they want it down look at the those sell...  \n",
       "2         now indicators turning up (rsi, macd, smi)  \n",
       "3                                  long this morning  \n",
       "4          going lower. forget about it or go short.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post=pickle.load(open('post.dat',\"rb\"))\n",
    "post.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(post,open('post.dat',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(post_data,open('test_post.dat',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "c = list(zip(train_data['sentiment'], train_data['sentence'],train_data['aspect']))\n",
    "random.shuffle(c)\n",
    "train_data['sentiment'], train_data['sentence'],train_data['aspect']= zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentiment']=list(train_data['sentiment'])\n",
    "train_data['sentence']=list(train_data['sentence'])\n",
    "train_data['aspect']=list(train_data['aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=[]\n",
    "val_y=[]\n",
    "for i in post_data['snippets']:\n",
    "    val_test.append(i)\n",
    "for i in head_data['snippets']:\n",
    "    val_test.append(i)\n",
    "for i in post_data['sentiment']:\n",
    "    val_y.append(i)\n",
    "for i in head_data['sentiment']:\n",
    "    val_y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = list(zip(val_test, val_y))\n",
    "random.shuffle(c)\n",
    "val_test, val_y= zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=list(val_test)\n",
    "val_y=list(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "for i in dataframe['sentence']:\n",
    "    all_data.append(i)\n",
    "for i in head_data['sentence']:\n",
    "    all_data.append(i)\n",
    "for i in post_data['sentence']:\n",
    "    all_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(all_data)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3304"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POST ONLY AS TRAIN\n",
    "data_X= encode_text(tokenizer, dataframe['snippet'], 16)\n",
    "data_X_1= encode_text(tokenizer, dataframe['sentence'], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX= encode_text(tokenizer, train_data['snippet'], 16)\n",
    "trainX_1= encode_text(tokenizer, train_data['sentence'], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_X= encode_text(tokenizer, head_data['snippets'], 16)\n",
    "head_X_1= encode_text(tokenizer, head_data['sentence'], 16)\n",
    "\n",
    "post_X =encode_text(tokenizer, post_data['snippets'], 16)\n",
    "post_X_1 =encode_text(tokenizer, post_data['sentence'], 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(head_data['aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aspect = encode_text(tokenizer, train_data['aspect'],1)\n",
    "train_aspect = np.tile(train_aspect,16)\n",
    "head_aspect = encode_text(tokenizer, head_data['aspect'],1)\n",
    "head_aspect = np.tile(head_aspect,16)\n",
    "post_aspect = encode_text(tokenizer, post_data['aspect'],1)\n",
    "post_aspect = np.tile(post_aspect,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(series,old_range,new_range):\n",
    "    m = interp1d(old_range,new_range)\n",
    "    return [float(m(x)) for x in series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = rescale(train_data['sentiment'],[-1,1],[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_sentiment= rescale (head_data['sentiment'],[-1,1],[0,1])\n",
    "p_sentiment= rescale (post_data['sentiment'],[-1,1],[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_X=[]\n",
    "validation_aspect=[]\n",
    "validation_Y=[]\n",
    "for i in post_X:\n",
    "    validation_X.append(i)\n",
    "for i in head_X:\n",
    "    validation_X.append(i)\n",
    "    \n",
    "for i in p_sentiment:\n",
    "    validation_Y.append(i)\n",
    "for i in h_sentiment:\n",
    "    validation_Y.append(i) \n",
    "    \n",
    "for i in post_aspect:\n",
    "    validation_aspect.append(i)\n",
    "for i in head_aspect:\n",
    "    validation_aspect.append(i) \n",
    "    \n",
    "validation_X=np.array(validation_X)\n",
    "validation_aspect=np.array(validation_aspect)\n",
    "validation_Y=np.array(validation_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(validation_X,validation_aspect,validation_Y))\n",
    "random.shuffle(c)\n",
    "validation_X,validation_aspect,validation_Y= zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "validation_X=np.array(validation_X)\n",
    "validation_aspect=np.array(validation_aspect)\n",
    "validation_Y=np.array(validation_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = KeyedVectors.load_word2vec_format('D:\\PythonCodes\\Jupyter notebooks\\Word Embeddings\\GoogleNews-vectors-negative300.bin',limit=500000,binary=True)\n",
    "\n",
    "pickle.dump(model,open(\"GoogleNews-vectors-negative300.dat\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def build_embedding_matrix(vocab_size, embed_dim, tokenizer ):\n",
    "    \n",
    "    embedding_matrix_file_name='D:/PythonCodes/Sentiment-Analysis/Data/embedding_matrix_sentiment_last.dat'\n",
    "    \n",
    "    if os.path.exists(embedding_matrix_file_name):\n",
    "        \n",
    "        print('loading embedding_matrix:', embedding_matrix_file_name)\n",
    "        embedding_matrix = pickle.load(open(embedding_matrix_file_name, 'rb'))\n",
    "        word2vec = pickle.load(open(\"word2vec.dat\", 'rb'))\n",
    "\n",
    "    else:\n",
    "        \n",
    "\n",
    "        embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "        word2vec={}\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "\n",
    "            try:\n",
    "                embedding_vector = model[word]\n",
    "            except KeyError:\n",
    "                embedding_vector = None\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i]=embedding_vector\n",
    "                word2vec[word]=i\n",
    "\n",
    "\n",
    "        pickle.dump(embedding_matrix, open(embedding_matrix_file_name, 'wb'),protocol=2)\n",
    "        pickle.dump(word2vec, open(\"word2vec.dat\", 'wb'),protocol=2)\n",
    "\n",
    "\n",
    "    return embedding_matrix,word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding_matrix: D:/PythonCodes/Sentiment-Analysis/Data/embedding_matrix_sentiment_last.dat\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix,w2v = build_embedding_matrix(vocab_size, 300,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3304\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix[w2v['company']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_dict = gensim.models.KeyedVectors.load_word2vec_format(dictFileName, binary=False)\n",
    "#embedding_dict.save_word2vec_format(dictFileName+\".bin\", binary=True)\n",
    "#dictFileName=\"D:\\wiki-news-300d-1M.vec\\wiki-news-300d-1M.vec.bin\"\n",
    "#embedding_dict = gensim.models.KeyedVectors.load_word2vec_format(dictFileName, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fast_text_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    w2v={}\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "            w2v[word]=i\n",
    "\n",
    "    return embedding_matrix,w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance,w2v = get_fast_text_matrix(embedding_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(fast_text_model, open('finance_model.dat', 'wb'),protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_model = pickle.load(open('fasttext_model.dat', 'rb'))\n",
    "finance_model = pickle.load(open('finance_model.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1166543  words loaded!\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',errors='ignore')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        try: \n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "        except :\n",
    "            pass\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "model = loadGloveModel(r\"D:\\glove.twitter.27B\\glove.twitter.27B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('glove_model.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =pickle.load(open('glove_model.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_matrix(model):\n",
    "    embedding_matrix = np.random.uniform(-0.001, 0.001, (vocab_size, 200))\n",
    "    w2v={}\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector= None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "            w2v[word]=i\n",
    "\n",
    "    return embedding_matrix,w2v\n",
    "\n",
    "glove,glove_w2v=get_glove_matrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3304"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(glove, open('glove_embedding.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = pickle.load(open('glove_embedding.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['concatenate', 'random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "def fun(data,all_data):\n",
    "    for row, i in data.iterrows():\n",
    "        all_data.append(i['aspect'])\n",
    "        all_data.append(i['sentence'])\n",
    "    return all_data\n",
    "    \n",
    "all_data = fun(train_data,all_data)\n",
    "all_data = fun(head_data,all_data)\n",
    "all_data = fun(post_data,all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = ' '.join(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    words = raw.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = all_data\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesco set sell COMPANY giraffe businesses sky news\n",
      "['tesco', 'set', 'sell', 'COMPANY', 'giraffe', 'businesses', 'sky', 'news']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[101])\n",
    "print(sentence_to_wordlist(raw_sentences[101]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 13,138 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 1\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "#more workers, faster we train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 5\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling =1e-5\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1\n",
    "\n",
    "finance2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 3337\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(finance2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68331, 394140)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance2vec.train(sentences,total_examples=finance2vec.corpus_count,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")\n",
    "    \n",
    "finance2vec.save(os.path.join(\"trained\", \"finance2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"finance2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_embedding = finance2vec.wv\n",
    "embedding=finance_embedding.get_keras_embedding(train_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3337"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finance2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPANY Price Action\n"
     ]
    }
   ],
   "source": [
    "print(finance2vec.wv.index2word[0], finance2vec.wv.index2word[1], finance2vec.wv.index2word[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABEL ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "def convert_lables (trainY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(trainY)\n",
    "    temp1 = le.transform(trainY)\n",
    "    return to_categorical(temp1,27),le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY,lable_encoding = convert_lables(train_data['aspect'])\n",
    "dataY,lable_encoding = convert_lables(dataframe['aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Appointment', 'Buyside', 'Central Banks', 'Company Communication',\n",
       "       'Conditions', 'Coverage', 'Currency', 'Dividend Policy',\n",
       "       'Financial', 'Fundamentals', 'IPO', 'Insider Activity', 'Legal',\n",
       "       'M&A', 'Market', 'Options', 'Price Action', 'Regulatory',\n",
       "       'Reputation', 'Risks', 'Rumors', 'Sales', 'Signal', 'Strategy',\n",
       "       'Technical Analysis', 'Trade', 'Volatility'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lable_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEXICONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) OPINION LEXICON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\lexicons\\opinion-lexicon-English\\negative-words.txt\",\"r\") as f: \n",
    "    lines= f.read()\n",
    "    negative_words=lines.split(\"\\n\")\n",
    "    for words in negative_words: \n",
    "        if words=='':\n",
    "            negative_words.remove(words)\n",
    "    negative_words=negative_words[:len(negative_words)-1]\n",
    "\n",
    "\n",
    "with open(r\"D:\\lexicons\\opinion-lexicon-English\\positive-words.txt\",\"r\") as f: \n",
    "    lines= f.read()\n",
    "    positive_words=lines.split(\"\\n\")\n",
    "    for words in positive_words: \n",
    "        if words=='':\n",
    "            positive_words.remove(words)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) MRP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    }
   ],
   "source": [
    "with open(r\"D:\\lexicons\\MSOL-June15-09.txt\\MSOL-June15-09.txt\",\"r\") as f: \n",
    "    lines= f.read()\n",
    "    mrp_words=lines.split(\"\\n\")\n",
    "    positive=[]\n",
    "    negative=[]\n",
    "    for words in mrp_words: \n",
    "        if words=='':\n",
    "            mrp_words.remove(words)\n",
    "        w=words.split(\" \")\n",
    "        try:\n",
    "            if w[1]=='positive':\n",
    "                positive.append(w[0])\n",
    "            elif w[1]=='negative':\n",
    "                negative.append(w[0])\n",
    "            else: \n",
    "                print(w)\n",
    "        except: \n",
    "            print(w)\n",
    "    #negative_words=negative_words[:len(negative_words)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abjectly\n",
      "a_sticky_moment\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(positive[100]),print(negative[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Loughran-McDonald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\lexicons\\LM_Negative.txt\",\"r\") as f: \n",
    "    lines= f.read()\n",
    "    negative_words_1=lines.split(\"\\n\")\n",
    "    n=[]\n",
    "    for words in negative_words_1: \n",
    "        if words!='':\n",
    "            n.append(words.lstrip('\\x0c'))\n",
    "    n=n[1:]\n",
    "    n=n[:len(n)-1]\n",
    "\n",
    "with open(r\"D:\\lexicons\\LM_Positive.txt\",\"r\") as f: \n",
    "    lines= f.read()\n",
    "    positive_words_1=lines.split(\"\\n\")\n",
    "    p=[]\n",
    "    for words in positive_words_1: \n",
    "        if words!='':\n",
    "            p.append(words.lstrip('\\x0c'))\n",
    "    p=p[1:]\n",
    "    p=p[:len(p)-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine All Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_w=[]\n",
    "negative_w=[]\n",
    "for i in positive: \n",
    "    positive_w.append(i.lower())\n",
    "for i in p: \n",
    "    if i.lower() not in positive_w:\n",
    "        positive_w.append(i.lower())    \n",
    "for i in positive_words:\n",
    "    if i.lower() not in positive_w:\n",
    "        positive_w.append(i.lower())\n",
    "\n",
    "for i in negative: \n",
    "    negative_w.append(i.lower())\n",
    "for i in n: \n",
    "    if i.lower() not in negative_w:\n",
    "        negative_w.append(i.lower())    \n",
    "for i in negative_words:\n",
    "    if i.lower() not in negative_w:\n",
    "        negative_w.append(i.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31250, 48649)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_w),len(negative_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=tokenizer\n",
    "freq_word={}\n",
    "for word, count in t.word_counts.items():\n",
    "    freq_word[word]=count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2vec=[]\n",
    "for i in train_data['sentence']:\n",
    "    vec=[]\n",
    "    for word in i.lower().split(\" \"):\n",
    "        try:\n",
    "            vec.append(embedding_matrix[w2v[word]])\n",
    "            \n",
    "        except:\n",
    "            v=np.zeros((300),dtype=float)\n",
    "            vec.append(v)\n",
    "            \n",
    "        \n",
    "    sentence2vec.append(np.mean(vec,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_sentence2vec=[]\n",
    "for i in head_data['sentence']:\n",
    "    vec=[]\n",
    "    for word in i.lower().split(\" \"):\n",
    "        try:\n",
    "            vec.append(embedding_matrix[w2v[word]])\n",
    "            \n",
    "        except:\n",
    "            v=np.zeros((300),dtype=float)\n",
    "            vec.append(v)\n",
    "            \n",
    "        \n",
    "    head_sentence2vec.append(np.mean(vec,axis=0))\n",
    "    \n",
    "post_sentence2vec=[]\n",
    "for i in post_data['sentence']:\n",
    "    vec=[]\n",
    "    for word in i.lower().split(\" \"):\n",
    "        try:\n",
    "            vec.append(embedding_matrix[w2v[word]])\n",
    "            \n",
    "        except:\n",
    "            v=np.zeros((300),dtype=float)\n",
    "            vec.append(v)\n",
    "            \n",
    "        \n",
    "    post_sentence2vec.append(np.mean(vec,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 93)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_sentence2vec),len(head_sentence2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2vec_glove=[]\n",
    "for i in train_data['sentence']:\n",
    "    vec=[]\n",
    "    for word in i.lower().split(\" \"):\n",
    "        try:\n",
    "            vec.append(glove[glove_w2v[word]])\n",
    "            \n",
    "        except:\n",
    "            v=np.zeros((200),dtype=float)\n",
    "            vec.append(v)\n",
    "            \n",
    "        \n",
    "    sentence2vec_glove.append(np.mean(vec,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2vec_glove_post=[]\n",
    "for i in post['sentence']:\n",
    "    vec=[]\n",
    "    for word in i.lower().split(\" \"):\n",
    "        try:\n",
    "            vec.append(glove[glove_w2v[word]])\n",
    "            \n",
    "        except:\n",
    "            v=np.zeros((200),dtype=float)\n",
    "            vec.append(v)\n",
    "            \n",
    "        \n",
    "    sentence2vec_glove_post.append(np.mean(vec,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_sentence2vec_glove=[]\n",
    "for i in head_data['sentence']:\n",
    "    vec=[]\n",
    "    for word in i.lower().split(\" \"):\n",
    "        try:\n",
    "            vec.append(glove[glove_w2v[word]])\n",
    "            \n",
    "        except:\n",
    "            v=np.zeros((200),dtype=float)\n",
    "            vec.append(v)\n",
    "            \n",
    "        \n",
    "    head_sentence2vec_glove.append(np.mean(vec,axis=0))\n",
    "    \n",
    "post_sentence2vec_glove=[]\n",
    "for i in post_data['sentence']:\n",
    "    vec=[]\n",
    "    for word in i.lower().split(\" \"):\n",
    "        try:\n",
    "            vec.append(glove[glove_w2v[word]])\n",
    "            \n",
    "        except:\n",
    "            v=np.zeros((200),dtype=float)\n",
    "            vec.append(v)\n",
    "            \n",
    "        \n",
    "    post_sentence2vec_glove.append(np.mean(vec,axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Calculating PMI SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentiment']=train_data['sentiment'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pmi_score(data=train_data):\n",
    "    freq_w_pos={}\n",
    "    freq_w_neg={}\n",
    "    positive_sentences=[]\n",
    "    negative_sentences=[]\n",
    "    N=vocab_size\n",
    "    #freq_w= Counter(data['sentence'])\n",
    "    \n",
    "    for index,row in data.iterrows():\n",
    "\n",
    "        if row['sentiment']>0:\n",
    "           \n",
    "            #row['Sentiment Class']='POSITIVE'\n",
    "            positive_sentences.append(row['sentence'])\n",
    "            for i in row['sentence'].split(\" \"):\n",
    "                if i.lower() not in freq_w_pos.keys():\n",
    "                    freq_w_pos[i.lower()]=1\n",
    "                else :\n",
    "                    freq_w_pos[i.lower()]+=1\n",
    "\n",
    "\n",
    "\n",
    "        elif row['sentiment']<0:\n",
    "           \n",
    "            #row['Sentiment Class']='NEGATIVE'\n",
    "            negative_sentences.append(row['sentence'])\n",
    "            for i in row['sentence'].split(\" \"):\n",
    "                if i.lower() not in freq_w_neg.keys():\n",
    "                    freq_w_neg[i.lower()]=1\n",
    "                else :\n",
    "                    freq_w_neg[i.lower()]+=1\n",
    "\n",
    "        else : \n",
    "            pass\n",
    "            \n",
    "        \n",
    "    t1=create_tokenizer(positive_sentences)\n",
    "\n",
    "    t2=create_tokenizer(negative_sentences)\n",
    "\n",
    "    freq_pos={}\n",
    "    freq_neg={}\n",
    "    for word, count in t1.word_counts.items():\n",
    "        freq_pos[word]=count\n",
    "    for word, count in t2.word_counts.items():\n",
    "        freq_neg[word]=count\n",
    "    tp = len(freq_pos)\n",
    "    tn=  len(freq_neg)\n",
    "\n",
    "    score_w={}\n",
    "    \n",
    "    for i in t.word_counts.keys():\n",
    "       \n",
    "        try: \n",
    "                a=freq_w_pos[i]\n",
    "        except :\n",
    "                a=0\n",
    "                \n",
    "        try:\n",
    "                c=freq_w_neg[i]\n",
    "        except:\n",
    "                c=0\n",
    "    \n",
    "            \n",
    "         \n",
    "        cal_1=(a * N)/(freq_word[i] * tp)\n",
    "        try : pmi_w_pos = math.log(cal_1,2)\n",
    "        except: pmi_w_pos=0\n",
    "        cal_2=(c * N)/(freq_word[i] * tn)\n",
    "        try : pmi_w_neg = math.log(cal_2,2)\n",
    "        except : pmi_w_neg=0\n",
    "        score_w[i]= pmi_w_pos-pmi_w_neg\n",
    "\n",
    "    return score_w\n",
    "                \n",
    "                \n",
    "            \n",
    "score = cal_pmi_score(train_data)        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('accumulating', 0.00240358184332116), ('accumulate', 0.5873660825644773), ('accrue', 0.5873660825644773), ('accident', 0), ('access', 0), ('accelerate', 0.5873660825644773), ('academics', -1.1287100262546164), ('above', 0), ('abc', 0.5873660825644773), ('abandons', -1.1287100262546164)]\n",
      "[('yib', 0.5873660825644773), ('yetwhen', -1.1287100262546164), ('yet', -1.126306444411295), ('yesterdaylooking', 0.5873660825644773), ('yesterday', 1.7805841511972234), ('yes', 0.5873660825644773), ('years', 0.4586560563098611), ('year', -0.01082922699135902), ('yeah', -1.1287100262546164), ('ybxeze', -1.1287100262546164)]\n"
     ]
    }
   ],
   "source": [
    "semantic_sorted = sorted(score.items(), \n",
    "                         reverse=True)\n",
    "top_pos = semantic_sorted[30:40]\n",
    "top_neg = semantic_sorted[-30:-20]\n",
    "print(top_neg)\n",
    "print(top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_HEAD =  cal_pmi_score(head_data)\n",
    "SCORE_POST =  cal_pmi_score(post_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3303"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count=[]\n",
    "neg_count=[]\n",
    "net_count=[]\n",
    "a_score=[]\n",
    "pmi_score=[]\n",
    "for i in train_data['sentence']:\n",
    "    p_count=0\n",
    "    n_count=0\n",
    "    p=[]\n",
    "    for j in i.lower().split(\" \"):\n",
    "        if j in positive_w:\n",
    "            p_count+=1\n",
    "        if j in negative_w:\n",
    "            n_count+=1\n",
    "        try : p.append(score[j])\n",
    "        except : p.append(0)\n",
    "    pmi_score.append((np.mean(p)))\n",
    "    a=1-np.sqrt(1-((p_count-n_count)/(p_count+n_count)))\n",
    "    pos_count.append(p_count)\n",
    "    neg_count.append(n_count)\n",
    "    net_count.append(p_count-n_count)\n",
    "    a_score.append(a)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['positive']=pos_count\n",
    "train_data['negative']=neg_count\n",
    "train_data['net']=net_count\n",
    "train_data['a_score']=a_score\n",
    "train_data['pmi']= pmi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>snippet</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>net</th>\n",
       "      <th>a_score</th>\n",
       "      <th>pmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Financial</td>\n",
       "      <td>linkedin revenue nearly f COMPANY</td>\n",
       "      <td>0.423</td>\n",
       "      <td>latest bid said unlikely to win sabmiller's ap...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.456496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Risks</td>\n",
       "      <td>aviva COMPANY suspend property funds investors...</td>\n",
       "      <td>-0.807</td>\n",
       "      <td>shakes up board with two new business chiefs, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105573</td>\n",
       "      <td>-0.399879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>britain ftse bounces back mondi COMPANY lead</td>\n",
       "      <td>0.558</td>\n",
       "      <td>set to appoint roy gardner, ex-centrica, as ch...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.292893</td>\n",
       "      <td>0.676420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Risks</td>\n",
       "      <td>new COMPANY duo get former boss support diffus...</td>\n",
       "      <td>0.143</td>\n",
       "      <td>set to appoint roy gardner, ex-centrica, as ch...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.105573</td>\n",
       "      <td>0.129499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Market</td>\n",
       "      <td>amazon attack uk grocery market COMPANY deal</td>\n",
       "      <td>0.333</td>\n",
       "      <td>sells benchmark indices unit</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>-0.154701</td>\n",
       "      <td>0.246706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         aspect                                           sentence  sentiment  \\\n",
       "0     Financial                  linkedin revenue nearly f COMPANY      0.423   \n",
       "1         Risks  aviva COMPANY suspend property funds investors...     -0.807   \n",
       "2  Price Action       britain ftse bounces back mondi COMPANY lead      0.558   \n",
       "3         Risks  new COMPANY duo get former boss support diffus...      0.143   \n",
       "4        Market       amazon attack uk grocery market COMPANY deal      0.333   \n",
       "\n",
       "                                             snippet  positive  negative  net  \\\n",
       "0  latest bid said unlikely to win sabmiller's ap...         3         0    3   \n",
       "1  shakes up board with two new business chiefs, ...         3         2    1   \n",
       "2  set to appoint roy gardner, ex-centrica, as ch...         3         1    2   \n",
       "3  set to appoint roy gardner, ex-centrica, as ch...         6         4    2   \n",
       "4                       sells benchmark indices unit         2         4   -2   \n",
       "\n",
       "    a_score       pmi  \n",
       "0  1.000000 -0.456496  \n",
       "1  0.105573 -0.399879  \n",
       "2  0.292893  0.676420  \n",
       "3  0.105573  0.129499  \n",
       "4 -0.154701  0.246706  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>snippets</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>net</th>\n",
       "      <th>a_score</th>\n",
       "      <th>pmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Financial</td>\n",
       "      <td>COMPANY delivers small profit increase 2014</td>\n",
       "      <td>0.247</td>\n",
       "      <td>delivers small profit increase for</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.292893</td>\n",
       "      <td>0.366023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Financial</td>\n",
       "      <td>COMPANY reports million loss first quarter</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>reports $583 million loss in first quarter</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.212220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sales</td>\n",
       "      <td>osborne extends COMPANY sell plan</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>sell-off plan</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.154701</td>\n",
       "      <td>-0.338741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>companies COMPANY shares hit 2016 revenue fore...</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>hit as 2016 revenue forecast to fall</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105573</td>\n",
       "      <td>-0.244071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strategy</td>\n",
       "      <td>COMPANY ends 27 year sponsorship tate falling ...</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>as falling oil price takes toll</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.054093</td>\n",
       "      <td>-0.290480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         aspect                                           sentence  sentiment  \\\n",
       "0     Financial        COMPANY delivers small profit increase 2014      0.247   \n",
       "1     Financial         COMPANY reports million loss first quarter     -0.741   \n",
       "2         Sales                  osborne extends COMPANY sell plan     -0.311   \n",
       "3  Price Action  companies COMPANY shares hit 2016 revenue fore...     -0.375   \n",
       "4      Strategy  COMPANY ends 27 year sponsorship tate falling ...     -0.293   \n",
       "\n",
       "                                     snippets  positive  negative  net  \\\n",
       "0          delivers small profit increase for         3         1    2   \n",
       "1  reports $583 million loss in first quarter         2         2    0   \n",
       "2                               sell-off plan         1         2   -1   \n",
       "3        hit as 2016 revenue forecast to fall         3         2    1   \n",
       "4             as falling oil price takes toll         4         5   -1   \n",
       "\n",
       "    a_score       pmi  \n",
       "0  0.292893  0.366023  \n",
       "1  0.000000 -0.212220  \n",
       "2 -0.154701 -0.338741  \n",
       "3  0.105573 -0.244071  \n",
       "4 -0.054093 -0.290480  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count=[]\n",
    "neg_count=[]\n",
    "net_count=[]\n",
    "a_score=[]\n",
    "h_pmi_score=[]\n",
    "for i in head_data['sentence']:\n",
    "    p_count=0\n",
    "    n_count=0\n",
    "    p=[]\n",
    "    for j in i.lower().split(\" \"):\n",
    "        if j in positive_w:\n",
    "            p_count+=1\n",
    "        if j in negative_w:\n",
    "            n_count+=1\n",
    "        try : p.append(score[j])\n",
    "        except : p.append(0)\n",
    "    h_pmi_score.append((np.mean(p)))\n",
    "    a=1-np.sqrt(1-((p_count-n_count)/(p_count+n_count)))\n",
    "    pos_count.append(p_count)\n",
    "    neg_count.append(n_count)\n",
    "    net_count.append(p_count-n_count)\n",
    "    a_score.append(a)\n",
    "\n",
    "head_data['positive']=pos_count\n",
    "head_data['negative']=neg_count\n",
    "head_data['net']=net_count\n",
    "head_data['a_score']=a_score\n",
    "head_data['pmi']=h_pmi_score\n",
    "head_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>snippets</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>net</th>\n",
       "      <th>a_score</th>\n",
       "      <th>pmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>COMPANY currently way undervalued rise back soon</td>\n",
       "      <td>0.154</td>\n",
       "      <td>is currently way undervalued and will rise bac...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.074180</td>\n",
       "      <td>-0.082579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Financial</td>\n",
       "      <td>COMPANY q delivered  organic revenue de growth...</td>\n",
       "      <td>0.408</td>\n",
       "      <td>reported net revenue up 2.0%</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.292893</td>\n",
       "      <td>-0.101138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>one stock oversold rsi  watch possible reverse...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>stocks oversold rsi 20</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.183503</td>\n",
       "      <td>0.025945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>COMPANY added dip yesterday rick holding night...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>but i’m running away from this.</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Financial</td>\n",
       "      <td>COMPANY revenue falls  lowers full year revenu...</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>revenue falls</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.244071</td>\n",
       "      <td>0.040820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         aspect                                           sentence  sentiment  \\\n",
       "0  Price Action   COMPANY currently way undervalued rise back soon      0.154   \n",
       "1     Financial  COMPANY q delivered  organic revenue de growth...      0.408   \n",
       "2  Price Action  one stock oversold rsi  watch possible reverse...      0.000   \n",
       "3  Price Action  COMPANY added dip yesterday rick holding night...      0.000   \n",
       "4     Financial  COMPANY revenue falls  lowers full year revenu...     -0.950   \n",
       "\n",
       "                                            snippets  positive  negative  net  \\\n",
       "0  is currently way undervalued and will rise bac...         4         3    1   \n",
       "1                       reported net revenue up 2.0%         9         3    6   \n",
       "2                             stocks oversold rsi 20         4         2    2   \n",
       "3                    but i’m running away from this.         4         4    0   \n",
       "4                                      revenue falls         5         2    3   \n",
       "\n",
       "    a_score       pmi  \n",
       "0  0.074180 -0.082579  \n",
       "1  0.292893 -0.101138  \n",
       "2  0.183503  0.025945  \n",
       "3  0.000000  0.125071  \n",
       "4  0.244071  0.040820  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count=[]\n",
    "neg_count=[]\n",
    "net_count=[]\n",
    "a_score=[]\n",
    "p_pmi_score=[]\n",
    "for i in post_data['sentence']:\n",
    "    p_count=0\n",
    "    n_count=0\n",
    "    for j in i.lower().split(\" \"):\n",
    "        if j in positive_w:\n",
    "            p_count+=1\n",
    "        if j in negative_w:\n",
    "            n_count+=1\n",
    "        try : p.append(score[j])\n",
    "        except : p.append(0)\n",
    "    p_pmi_score.append((np.mean(p)))\n",
    "    a=1-np.sqrt(1-((p_count-n_count)/(p_count+n_count)))\n",
    "    pos_count.append(p_count)\n",
    "    neg_count.append(n_count)\n",
    "    net_count.append(p_count-n_count)\n",
    "    a_score.append(a)\n",
    "\n",
    "post_data['positive']=pos_count\n",
    "post_data['negative']=neg_count\n",
    "post_data['net']=net_count\n",
    "post_data['a_score']=a_score\n",
    "post_data['pmi']=p_pmi_score\n",
    "\n",
    "post_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTING N GRAMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) COUNT VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word')\n",
    "count_vect.fit(dataframe['sentence'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_data['sentence'])\n",
    "xtrain_1_count =  count_vect.transform(dataframe['sentence'])\n",
    "head_count =  count_vect.transform(head_data['sentence'])\n",
    "post_count =  count_vect.transform(post_data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1173x3023 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9213 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=None)\n",
    "tfidf_vect.fit(dataframe['sentence'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_data['sentence'])\n",
    "xtrain_1_tfidf =  tfidf_vect.transform(dataframe['sentence'])\n",
    "head_tfidf =  tfidf_vect.transform(head_data['sentence'])\n",
    "post_tfidf =  tfidf_vect.transform(post_data['sentence'])\n",
    "\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', ngram_range=(2,4), max_features=None)\n",
    "tfidf_vect_ngram.fit(train_data['sentence'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_data['sentence'])\n",
    "head_tfidf_ngram =  tfidf_vect_ngram.transform(head_data['sentence'])\n",
    "post_tfidf_ngram =  tfidf_vect_ngram.transform(post_data['sentence'])\n",
    "xtrain_1_tfidf_ngram =  tfidf_vect_ngram.transform(dataframe['sentence'])\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(1,3), max_features=None)\n",
    "tfidf_vect_ngram_chars.fit(train_data['sentence'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_data['sentence']) \n",
    "head_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(head_data['sentence']) \n",
    "post_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(post_data['sentence']) \n",
    "xtrain_1_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(dataframe['sentence']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for Aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['char_count'] = train_data['sentence'].apply(len)\n",
    "train_data['word_count'] = train_data['sentence'].apply(lambda x: len(x.split()))\n",
    "train_data['word_density'] = train_data['char_count'] / (train_data['word_count']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "train_data['noun_count'] = train_data['sentence'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "train_data['verb_count'] = train_data['sentence'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "train_data['adj_count'] = train_data['sentence'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "train_data['adv_count'] = train_data['sentence'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "train_data['pron_count'] = train_data['sentence'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post['noun_count'] = post['sentence'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "post['verb_count'] = post['sentence'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "post['adj_count'] = post['sentence'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "post['adv_count'] = post['sentence'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "post['pron_count'] = post['sentence'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_data['noun_count'] = head_data['sentence'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "head_data['verb_count'] = head_data['sentence'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "head_data['adj_count'] = head_data['sentence'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "head_data['adv_count'] = head_data['sentence'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "head_data['pron_count'] = head_data['sentence'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data['noun_count'] = post_data['sentence'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "post_data['verb_count'] = post_data['sentence'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "post_data['adj_count'] = post_data['sentence'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "post_data['adv_count'] = post_data['sentence'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "post_data['pron_count'] = post_data['sentence'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>snippets</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>net</th>\n",
       "      <th>a_score</th>\n",
       "      <th>pmi</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>pron_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Financial</td>\n",
       "      <td>COMPANY delivers small profit increase 2014</td>\n",
       "      <td>0.247</td>\n",
       "      <td>delivers small profit increase for</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.292893</td>\n",
       "      <td>0.366023</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Financial</td>\n",
       "      <td>COMPANY reports million loss first quarter</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>reports $583 million loss in first quarter</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.212220</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sales</td>\n",
       "      <td>osborne extends COMPANY sell plan</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>sell-off plan</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.154701</td>\n",
       "      <td>-0.338741</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>companies COMPANY shares hit 2016 revenue fore...</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>hit as 2016 revenue forecast to fall</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105573</td>\n",
       "      <td>-0.244071</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strategy</td>\n",
       "      <td>COMPANY ends 27 year sponsorship tate falling ...</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>as falling oil price takes toll</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.054093</td>\n",
       "      <td>-0.290480</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         aspect                                           sentence  sentiment  \\\n",
       "0     Financial        COMPANY delivers small profit increase 2014      0.247   \n",
       "1     Financial         COMPANY reports million loss first quarter     -0.741   \n",
       "2         Sales                  osborne extends COMPANY sell plan     -0.311   \n",
       "3  Price Action  companies COMPANY shares hit 2016 revenue fore...     -0.375   \n",
       "4      Strategy  COMPANY ends 27 year sponsorship tate falling ...     -0.293   \n",
       "\n",
       "                                     snippets  positive  negative  net  \\\n",
       "0          delivers small profit increase for         3         1    2   \n",
       "1  reports $583 million loss in first quarter         2         2    0   \n",
       "2                               sell-off plan         1         2   -1   \n",
       "3        hit as 2016 revenue forecast to fall         3         2    1   \n",
       "4             as falling oil price takes toll         4         5   -1   \n",
       "\n",
       "    a_score       pmi  noun_count  verb_count  adj_count  adv_count  \\\n",
       "0  0.292893  0.366023           3           1          1          0   \n",
       "1  0.000000 -0.212220           3           1          1          0   \n",
       "2 -0.154701 -0.338741           3           1          1          0   \n",
       "3  0.105573 -0.244071           5           2          0          0   \n",
       "4 -0.054093 -0.290480           7           3          0          0   \n",
       "\n",
       "   pron_count  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sparse.hstack([xtrain_tfidf, xtrain_tfidf_ngram,xtrain_tfidf_ngram_chars,sparse.csr_matrix(train_data['a_score']).T,sparse.csr_matrix(train_data['net']).T,sparse.csr_matrix(train_data['negative']).T,sparse.csr_matrix(train_data['positive']).T,sparse.csr_matrix(train_data['pmi']).T],'csr')\n",
    "x_test_1 = sparse.hstack([head_tfidf, head_tfidf_ngram,head_tfidf_ngram_chars,sparse.csr_matrix(head_data['a_score']).T,sparse.csr_matrix(head_data['net']).T,sparse.csr_matrix(head_data['negative']).T,sparse.csr_matrix(head_data['positive']).T,sparse.csr_matrix(head_data['pmi']).T],'csr')\n",
    "x_test_2 = sparse.hstack([post_tfidf, post_tfidf_ngram,post_tfidf_ngram_chars,sparse.csr_matrix(post_data['a_score']).T,sparse.csr_matrix(post_data['net']).T,sparse.csr_matrix(post_data['negative']).T,sparse.csr_matrix(post_data['positive']).T,sparse.csr_matrix(post_data['pmi']).T],'csr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_aspect = sparse.hstack([xtrain_tfidf, xtrain_tfidf_ngram,xtrain_tfidf_ngram_chars,\n",
    "                   sparse.csr_matrix(train_data['noun_count']).T,sparse.csr_matrix(train_data['verb_count']).T,\n",
    "                   sparse.csr_matrix(train_data['adj_count']).T,sparse.csr_matrix(train_data['adv_count']).T,\n",
    "                   sparse.csr_matrix(train_data['pron_count']).T,X_topics,\n",
    "                   sparse.csr_matrix(train_data['a_score']).T,sparse.csr_matrix(train_data['net']).T,\n",
    "                   sparse.csr_matrix(train_data['negative']).T,sparse.csr_matrix(train_data['positive']).T,\n",
    "                   sparse.csr_matrix(train_data['pmi']).T],'csr')\n",
    "\n",
    "'''\n",
    "X_aspect = sparse.hstack([xtrain_count,xtrain_tfidf, xtrain_tfidf_ngram,xtrain_tfidf_ngram_chars],'csr')\n",
    "X_aspect_1 = sparse.hstack([xtrain_1_count,xtrain_1_tfidf, xtrain_1_tfidf_ngram,xtrain_1_tfidf_ngram_chars],'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x_test_1_aspect = sparse.hstack([head_tfidf, head_tfidf_ngram,head_tfidf_ngram_chars,\n",
    "                  sparse.csr_matrix(head_data['noun_count']).T,sparse.csr_matrix(head_data['verb_count']).T,\n",
    "                   sparse.csr_matrix(head_data['adj_count']).T,sparse.csr_matrix(head_data['adv_count']).T,\n",
    "                   sparse.csr_matrix(head_data['pron_count']).T,Head_topics,sparse.csr_matrix(head_data['a_score']).T,\n",
    "                   sparse.csr_matrix(head_data['net']).T,sparse.csr_matrix(head_data['negative']).T,\n",
    "                sparse.csr_matrix(head_data['positive']).T,sparse.csr_matrix(head_data['pmi']).T],'csr')\n",
    "'''\n",
    "x_test_1_aspect = sparse.hstack([head_count,head_tfidf, head_tfidf_ngram,head_tfidf_ngram_chars],'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x_test_2_aspect = sparse.hstack([post_tfidf, post_tfidf_ngram,post_tfidf_ngram_chars,\n",
    "                   sparse.csr_matrix(post_data['noun_count']).T,sparse.csr_matrix(post_data['verb_count']).T,\n",
    "                   sparse.csr_matrix(post_data['adj_count']).T,sparse.csr_matrix(post_data['adv_count']).T,\n",
    "                   sparse.csr_matrix(post_data['pron_count']).T,Post_topics,\n",
    "                 sparse.csr_matrix(post_data['a_score']).T,sparse.csr_matrix(post_data['net']).T,sparse.csr_matrix(post_data['negative']).T,\n",
    "                sparse.csr_matrix(post_data['positive']).T,sparse.csr_matrix(post_data['pmi']).T],'csr')\n",
    "'''\n",
    "\n",
    "x_test_2_aspect = sparse.hstack([post_count,post_tfidf, post_tfidf_ngram,post_tfidf_ngram_chars],'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s2v=sparse.csc_matrix(sentence2vec)\n",
    "X_h2v=sparse.csc_matrix(head_sentence2vec)\n",
    "X_p2v=sparse.csc_matrix(post_sentence2vec)\n",
    "\n",
    "X_s2v_g=sparse.csc_matrix(sentence2vec_glove)\n",
    "X_h2v_g=sparse.csc_matrix(head_sentence2vec_glove)\n",
    "X_p2v_g=sparse.csc_matrix(post_sentence2vec_glove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "activator = Activation('relu', name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(h, avg):\n",
    "    print(\"H\",h.shape)\n",
    "    print(\"avg before: \",avg.shape)\n",
    "    avg = RepeatVector(max_length)(avg)\n",
    "    print(\"avg\",avg.shape)\n",
    "    \n",
    "    concat = concatenate([h, avg])\n",
    "    print(\"concat\",concat.shape)\n",
    "    \n",
    "    e = Dense(1,input_dim=concat.shape, activation = \"relu\")(concat)\n",
    "    print(\"e\",e.shape)\n",
    "\n",
    "    alphas = activator(e)\n",
    "    print(\"alphas\",alphas.shape)\n",
    "\n",
    "    context = dotor([alphas, h])\n",
    "    print(\"context\",context.shape)\n",
    "    \n",
    "    return K.sum(context, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import*\n",
    " \n",
    "def square_rooted(x):\n",
    " \n",
    "    return round(sqrt(sum([a*a for a in x])),3)\n",
    " \n",
    "def cosine_similarity(x,y):\n",
    " \n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return numerator/float(denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1173, 16)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Model_IAN(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag, em_dim,input_shape):\n",
    "    \n",
    "    input_context = Input(shape=(max_length,),name='Context')\n",
    "    input_features = Input(shape=(input_shape,),name='Features')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "\n",
    "    context=embedding(input_context)\n",
    " \n",
    "    H_c,_,_,_,_= Bidirectional(LSTM( lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"LSTM_C\"))(context)\n",
    "\n",
    "    c_avg = GlobalAveragePooling1D(name='POOL_C')(H_c)  \n",
    "\n",
    "    c_r = Lambda(lambda x: one_step_attention(x[0],x[1]))([H_c,input_features])\n",
    "   \n",
    "    out= Dense(327, activation='relu')(c_r)\n",
    "\n",
    "    out= Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(out)\n",
    "\n",
    "    IAN_model= Model(inputs=[input_context,input_features],outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    IAN_model.compile(loss='mse', optimizer=optimizer, metrics=['cosine'])\n",
    "    \n",
    "    #print( IAN_model.summary())\n",
    "    \n",
    "    return  IAN_model\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H (?, ?, 600)\n",
      "avg before:  (?, 27380)\n",
      "avg (?, 16, 27380)\n",
      "concat (?, 16, 27980)\n",
      "e (?, 16, 1)\n",
      "alphas (?, 16, 1)\n",
      "context (?, 1, 600)\n",
      "H (?, 16, 600)\n",
      "avg before:  (?, 27380)\n",
      "avg (?, 16, 27380)\n",
      "concat (?, 16, 27980)\n",
      "e (?, 16, 1)\n",
      "alphas (?, 16, 1)\n",
      "context (?, 1, 600)\n"
     ]
    }
   ],
   "source": [
    "IAN_model=Model_IAN(learning_rate=0.01,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='glove',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300,\n",
    "                   input_shape=X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 93 samples\n",
      "Epoch 1/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.3549 - cosine_proximity: -8.5251e-04 - val_loss: 0.3113 - val_cosine_proximity: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.3544 - cosine_proximity: -8.5252e-04 - val_loss: 0.3110 - val_cosine_proximity: -5.8674e-21\n",
      "Epoch 3/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.3546 - cosine_proximity: -2.0747e-09 - val_loss: 0.3109 - val_cosine_proximity: -1.0802e-14\n",
      "Epoch 4/100\n",
      "1173/1173 [==============================] - 14s 12ms/step - loss: 0.3545 - cosine_proximity: -6.8815e-06 - val_loss: 0.3108 - val_cosine_proximity: -1.1199e-07\n",
      "Epoch 5/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.3088 - cosine_proximity: -0.6064 - val_loss: 0.3804 - val_cosine_proximity: -1.0000\n",
      "Epoch 6/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.2937 - cosine_proximity: -1.0000 - val_loss: 0.3685 - val_cosine_proximity: -1.0000\n",
      "Epoch 7/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.1749 - cosine_proximity: -1.0000 - val_loss: 0.1153 - val_cosine_proximity: -1.0000\n",
      "Epoch 8/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0830 - cosine_proximity: -1.0000 - val_loss: 0.1025 - val_cosine_proximity: -1.0000\n",
      "Epoch 9/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0757 - cosine_proximity: -1.0000 - val_loss: 0.0983 - val_cosine_proximity: -1.0000\n",
      "Epoch 10/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0709 - cosine_proximity: -1.0000 - val_loss: 0.0941 - val_cosine_proximity: -1.0000\n",
      "Epoch 11/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0661 - cosine_proximity: -1.0000 - val_loss: 0.0878 - val_cosine_proximity: -1.0000\n",
      "Epoch 12/100\n",
      "1173/1173 [==============================] - 14s 12ms/step - loss: 0.0620 - cosine_proximity: -1.0000 - val_loss: 0.0874 - val_cosine_proximity: -1.0000\n",
      "Epoch 13/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0592 - cosine_proximity: -1.0000 - val_loss: 0.0873 - val_cosine_proximity: -1.0000\n",
      "Epoch 14/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0561 - cosine_proximity: -1.0000 - val_loss: 0.0812 - val_cosine_proximity: -1.0000\n",
      "Epoch 15/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0535 - cosine_proximity: -1.0000 - val_loss: 0.0783 - val_cosine_proximity: -1.0000\n",
      "Epoch 16/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0515 - cosine_proximity: -1.0000 - val_loss: 0.0745 - val_cosine_proximity: -1.0000\n",
      "Epoch 17/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0499 - cosine_proximity: -1.0000 - val_loss: 0.0750 - val_cosine_proximity: -1.0000\n",
      "Epoch 18/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0482 - cosine_proximity: -1.0000 - val_loss: 0.0743 - val_cosine_proximity: -1.0000\n",
      "Epoch 19/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0468 - cosine_proximity: -1.0000 - val_loss: 0.0734 - val_cosine_proximity: -1.0000e_proximity: \n",
      "Epoch 20/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0457 - cosine_proximity: -1.0000 - val_loss: 0.0714 - val_cosine_proximity: -1.0000\n",
      "Epoch 21/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0448 - cosine_proximity: -1.0000 - val_loss: 0.0699 - val_cosine_proximity: -1.0000\n",
      "Epoch 22/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0438 - cosine_proximity: -1.0000 - val_loss: 0.0716 - val_cosine_proximity: -1.0000\n",
      "Epoch 23/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0432 - cosine_proximity: -1.0000 - val_loss: 0.0679 - val_cosine_proximity: -1.0000\n",
      "Epoch 24/100\n",
      "1173/1173 [==============================] - 14s 12ms/step - loss: 0.0428 - cosine_proximity: -1.0000 - val_loss: 0.0693 - val_cosine_proximity: -1.0000\n",
      "Epoch 25/100\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 0.0421 - cosine_proximity: -1.0000 - val_loss: 0.0694 - val_cosine_proximity: -1.0000\n",
      "Epoch 26/100\n",
      "1173/1173 [==============================] - 13s 12ms/step - loss: 0.0417 - cosine_proximity: -1.0000 - val_loss: 0.0684 - val_cosine_proximity: -1.0000\n",
      "Epoch 27/100\n",
      " 496/1173 [===========>..................] - ETA: 14s - loss: 0.0412 - cosine_proximity: -1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-271-19593046f8dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m IAN_model.fit(x=[trainX,X],y=sentiment, epochs=100,batch_size=16,\n\u001b[1;32m----> 5\u001b[1;33m               callbacks=[EarlyStop],validation_data=([head_X,x_test_1],h_sentiment))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n",
    "\n",
    "\n",
    "IAN_model.fit(x=[trainX,X],y=sentiment, epochs=100,batch_size=16,\n",
    "              callbacks=[EarlyStop],validation_data=([head_X,x_test_1],h_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine :  [0.8945746]\n",
      "Headline MSE:  0.06613861480577815\n",
      "Headline R2: -0.06623033857550586\n",
      "Cosine :  [0.95096034]\n",
      "Post MSE: 0.030428054894036563\n",
      "Post R2: -0.06134750833404512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_sentiment =IAN_model.predict([head_X,x_test_1])\n",
    "\n",
    "outputs= sk_mse(h_sentiment,pred_sentiment)\n",
    "\n",
    "output1= r2_score(h_sentiment,pred_sentiment)\n",
    "\n",
    "print(\"Cosine : \",cosine_similarity(h_sentiment,list(pred_sentiment)))\n",
    "\n",
    "print(\"Headline MSE: \", np.mean(outputs))\n",
    "print(\"Headline R2:\", np.mean(output1))\n",
    "\n",
    "pred_sentiment =IAN_model.predict([post_X,x_test_2])\n",
    "\n",
    "#pred_sentiment = rescale(pred_sentiment,[0,1],[-1,1])\n",
    "\n",
    "outputs= sk_mse(p_sentiment,pred_sentiment)\n",
    "\n",
    "output1= r2_score(p_sentiment,pred_sentiment)\n",
    "print(\"Cosine : \",cosine_similarity(p_sentiment,pred_sentiment))\n",
    "\n",
    "print(\"Post MSE:\", np.mean(outputs))\n",
    "print(\"Post R2:\", np.mean(output1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Trying Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, feature_vector_train, label, feature_vector_valid,feature_vector_valid1, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    if is_neural_net: \n",
    "        EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n",
    "        model.fit(feature_vector_train, label,batch_size=16, epochs=50,callbacks=[EarlyStop],validation_data=(feature_vector_valid,h_sentiment))\n",
    "    else:\n",
    "        model.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = model.predict(feature_vector_valid)\n",
    "    predictions1 = model.predict(feature_vector_valid1)\n",
    "    predictions2 = model.predict(feature_vector_train)\n",
    "   \n",
    "\n",
    "    print(\"Cosine : \",cosine_similarity(h_sentiment,list(predictions)))\n",
    "\n",
    "    print(\"Headline MSE: \", np.mean(sk_mse(h_sentiment,predictions)))\n",
    "    print(\"Headline R2:\", np.mean(r2_score(h_sentiment,predictions)))\n",
    "    print()\n",
    "    print(\"Cosine : \",cosine_similarity(p_sentiment,predictions1))\n",
    "\n",
    "    print(\"Headline MSE: \", np.mean(sk_mse(p_sentiment,predictions1)))\n",
    "    print(\"Headline R2:\", np.mean(r2_score(p_sentiment,predictions1)))\n",
    "    \n",
    "    return model,predictions,predictions1,predictions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, metrics, svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors: \n",
      "Cosine :  0.8847039394020311\n",
      "Headline MSE:  0.08638859655412545\n",
      "Headline R2: -0.39268327320518437\n",
      "\n",
      "Cosine :  0.923343053803146\n",
      "Headline MSE:  0.07436840787624092\n",
      "Headline R2: -1.5940115026441437\n",
      "\n",
      "LR, WordLevel TF-IDF: \n",
      "Cosine :  0.8977764918508906\n",
      "Headline MSE:  0.0696291217340215\n",
      "Headline R2: -0.12250131423518829\n",
      "\n",
      "Cosine :  0.9331148702849251\n",
      "Headline MSE:  0.0476506109473248\n",
      "Headline R2: -0.6620798593816737\n",
      "\n",
      "LR, N-Gram Vectors: \n",
      "Cosine :  0.8985828709608396\n",
      "Headline MSE:  0.06710067706940412\n",
      "Headline R2: -0.08173988585116221\n",
      "\n",
      "Cosine :  0.9523212162259722\n",
      "Headline MSE:  0.032520733338035165\n",
      "Headline R2: -0.13434129844047638\n",
      "\n",
      "LR, CharLevel Vectors: \n",
      "Cosine :  0.8959732968556327\n",
      "Headline MSE:  0.07951853164029304\n",
      "Headline R2: -0.28192994611144395\n",
      "\n",
      "Cosine :  0.9273612455048713\n",
      "Headline MSE:  0.054432656293780066\n",
      "Headline R2: -0.8986413798250827\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Linear Classifier on Count Vectors\n",
    "print (\"LR, Count Vectors: \")\n",
    "m,m_A,m_B,_=train_model( LinearRegression() , xtrain_count, np.array(sentiment), head_count,post_count)\n",
    "\n",
    "print()\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "print (\"LR, WordLevel TF-IDF: \")\n",
    "m1,m_1_A,m_1_B,m_1_C=train_model( LinearRegression(), xtrain_tfidf, np.array(sentiment), head_tfidf,post_tfidf)\n",
    "print()\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "print (\"LR, N-Gram Vectors: \")\n",
    "m3,m_3_A,m_3_B,m_3_C=train_model( LinearRegression(), xtrain_tfidf_ngram, np.array(sentiment), head_tfidf_ngram, post_tfidf_ngram)\n",
    "print()\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "print (\"LR, CharLevel Vectors: \")\n",
    "m4,m_4_A,m_4_B,_=train_model(LinearRegression(), xtrain_tfidf_ngram_chars,np.array(sentiment) ,head_tfidf_ngram_chars, post_tfidf_ngram_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "clf = SVR(C=1.0, epsilon=0.2)\n",
    "a=clf.fit(X, sentiment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036675339644856725\n",
      "0.06030041562885621\n",
      "0.028242021213239592\n"
     ]
    }
   ],
   "source": [
    "pred2=clf.predict(X)\n",
    "pred=clf.predict( x_test_1)\n",
    "pred1=clf.predict( x_test_2)\n",
    "print(sk_mse(sentiment,pred2))\n",
    "print(sk_mse(h_sentiment,pred))\n",
    "print(sk_mse(p_sentiment,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  0.014902564446745203\n",
      "C =  0.02788812918134953\n"
     ]
    }
   ],
   "source": [
    "print(\"C = \", r2_score(np.array(p_sentiment).reshape(-1,1),np.array(pred1).reshape(-1,1)))\n",
    "print(\"C = \", r2_score(np.array(h_sentiment).reshape(-1,1),np.array(pred).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  [0.95217958]\n",
      "C =  [0.89999031]\n"
     ]
    }
   ],
   "source": [
    "print(\"C = \", cosine_similarity(np.array(p_sentiment).reshape(-1,1),np.array(pred1).reshape(-1,1)))\n",
    "print(\"C = \", cosine_similarity(np.array(h_sentiment).reshape(-1,1),np.array(pred).reshape(-1,1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GHOSAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) FEATURE MODEL - Only with tfid_n_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = Dense(500, activation=\"relu\")(input_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "    \n",
    "    hidden_layer = Dense(50, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "    hidden_layer = Dense(20, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "\n",
    "    classifier = Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=Adam(lr=0.001), loss='mse')\n",
    "    return classifier\n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 93 samples\n",
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 0.0399 - val_loss: 0.0676\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 0.0177 - val_loss: 0.0658\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 0.0139 - val_loss: 0.0664\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 0.0073 - val_loss: 0.0682\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 16s 13ms/step - loss: 0.0048 - val_loss: 0.0637\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 17s 14ms/step - loss: 0.0038 - val_loss: 0.0633\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 0.0031 - val_loss: 0.0641\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 16s 13ms/step - loss: 0.0031 - val_loss: 0.0631\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 16s 13ms/step - loss: 0.0029 - val_loss: 0.0662\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 0.0028 - val_loss: 0.0677\n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 16s 13ms/step - loss: 0.0025 - val_loss: 0.0636\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 16s 13ms/step - loss: 0.0025 - val_loss: 0.0678\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 0.0022 - val_loss: 0.0646\n",
      "Epoch 00013: early stopping\n",
      "Cosine :  [0.9037894]\n",
      "Headline MSE:  0.07775695409791118\n",
      "Headline R2: -0.041180398079301295\n",
      "\n",
      "Cosine :  [0.9519554]\n",
      "Headline MSE:  0.03927729760157574\n",
      "Headline R2: -0.17168223977506014\n"
     ]
    }
   ],
   "source": [
    "_,m1,m2,m3=train_model(classifier, xtrain_tfidf_ngram, sentiment, head_tfidf_ngram, post_tfidf_ngram, is_neural_net=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) FEATURE MODEL - Pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 93 samples\n",
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 28s 24ms/step - loss: 0.0289 - val_loss: 0.0526\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 23s 20ms/step - loss: 0.0132 - val_loss: 0.0562\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 21s 18ms/step - loss: 0.0075 - val_loss: 0.0529\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0053 - val_loss: 0.0551\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 19s 16ms/step - loss: 0.0047 - val_loss: 0.0587\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 18s 16ms/step - loss: 0.0037 - val_loss: 0.0558\n",
      "Epoch 00006: early stopping\n",
      "Cosine :  [0.91153574]\n",
      "Headline MSE:  0.07998227890170187\n",
      "Headline R2: 0.10009056558596485\n",
      "\n",
      "Cosine :  [0.9582718]\n",
      "Headline MSE:  0.039825587368043376\n",
      "Headline R2: 0.016856415050312323\n"
     ]
    }
   ],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = Dense(500, activation=\"relu\")(input_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "    \n",
    "    hidden_layer = Dense(50, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "    hidden_layer = Dense(20, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "\n",
    "    classifier = Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=Adam(lr=0.001), loss='mse')\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "\n",
    "classifier = create_model_architecture(X.shape[1])\n",
    "model1,m6_A,m6_B,m6_C=train_model(classifier, X, sentiment, x_test_1, x_test_2, is_neural_net=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Vector Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 93 samples\n",
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0384 - val_loss: 0.0580\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 1s 823us/step - loss: 0.0309 - val_loss: 0.0553\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 1s 809us/step - loss: 0.0258 - val_loss: 0.0469\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 1s 784us/step - loss: 0.0230 - val_loss: 0.0442\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 1s 783us/step - loss: 0.0207 - val_loss: 0.0428\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 1s 752us/step - loss: 0.0176 - val_loss: 0.0451\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 1s 748us/step - loss: 0.0158 - val_loss: 0.0421\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 1s 758us/step - loss: 0.0133 - val_loss: 0.0476\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 1s 820us/step - loss: 0.0117 - val_loss: 0.0408\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 1s 835us/step - loss: 0.0096 - val_loss: 0.0496\n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 1s 795us/step - loss: 0.0080 - val_loss: 0.0535\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 1s 758us/step - loss: 0.0070 - val_loss: 0.0425\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 1s 811us/step - loss: 0.0076 - val_loss: 0.0409\n",
      "Epoch 14/50\n",
      "1173/1173 [==============================] - 1s 771us/step - loss: 0.0066 - val_loss: 0.0532\n",
      "Epoch 00014: early stopping\n",
      "Cosine :  [0.92119557]\n",
      "Headline MSE:  0.0940858723498774\n",
      "Headline R2: 0.14298436199518294\n",
      "\n",
      "Cosine :  [0.9507268]\n",
      "Headline MSE:  0.06350053237104383\n",
      "Headline R2: -0.19198225685896708\n"
     ]
    }
   ],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = Dense(500, activation=\"relu\")(input_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "    \n",
    "    hidden_layer = Dense(50, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "    hidden_layer = Dense(20, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "\n",
    "    classifier = Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=Adam(lr=0.001), loss='mse')\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "\n",
    "classifier = create_model_architecture(X_s2v.shape[1])\n",
    "model2,m7_a,m7_b,m7_c=train_model(classifier, X_s2v, sentiment,X_h2v, X_p2v, is_neural_net=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model2(dropout,learning_rate,em,em_dim,lstm_out, n_hidden_layer,em_trainable_flag,n_filters=100):\n",
    "   \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "\n",
    "    embedding1=Embedding(vocab_size, em_dim, weights = [eval(em)],input_length=max_length,trainable = False)\n",
    "    \n",
    "    context1= embedding1(input_context)\n",
    "    \n",
    "    concat1=Dropout(0.3)(context1)\n",
    "    \n",
    "    c3=Conv1D(n_filters,kernel_size=3,activation='relu')(concat1)\n",
    "    drop3 = Dropout(0.2)(c3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat = Flatten()(pool3)\n",
    "    \n",
    "    a=Dense(327,activation='relu')(flat)\n",
    "        \n",
    "    out=Dense(1,activation='sigmoid')(a)\n",
    "    \n",
    "    model= Model(inputs=input_context ,outputs=out)\n",
    "    \n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['cosine'])\n",
    "    \n",
    "    \n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=define_model2(learning_rate=0.00069,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='glove',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=200)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 192 samples\n",
      "Epoch 1/100\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0462 - cosine_proximity: -1.0000 - val_loss: 0.0513 - val_cosine_proximity: -1.0000\n",
      "Epoch 2/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0383 - cosine_proximity: -1.0000 - val_loss: 0.0467 - val_cosine_proximity: -1.0000\n",
      "Epoch 3/100\n",
      "1173/1173 [==============================] - 1s 956us/step - loss: 0.0346 - cosine_proximity: -1.0000 - val_loss: 0.0469 - val_cosine_proximity: -1.0000\n",
      "Epoch 4/100\n",
      "1173/1173 [==============================] - 1s 993us/step - loss: 0.0313 - cosine_proximity: -1.0000 - val_loss: 0.0472 - val_cosine_proximity: -1.0000\n",
      "Epoch 5/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0292 - cosine_proximity: -1.0000 - val_loss: 0.0523 - val_cosine_proximity: -1.0000A: 0s - loss: 0.0292 - cosine_proximity: -\n",
      "Epoch 6/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0276 - cosine_proximity: -1.0000 - val_loss: 0.0506 - val_cosine_proximity: -1.0000\n",
      "Epoch 7/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0245 - cosine_proximity: -1.0000 - val_loss: 0.0519 - val_cosine_proximity: -1.0000\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x213aca59828>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n",
    "\n",
    "model.fit(x=trainX,y=sentiment, epochs=100,batch_size=16,callbacks=[EarlyStop],validation_data=(validation_X,validation_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  [0.89370571]\n",
      "Headline MSE:  0.07238007411975393\n",
      "Headline R2: -0.10588111146989698\n",
      "C =  [0.94452278]\n",
      "Post MSE: 0.03522617546012155\n",
      "Post R2: -0.26055776983025947\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_sentiment =model.predict(head_X)\n",
    "\n",
    "outputs= sk_mse(h_sentiment,pred_sentiment)\n",
    "\n",
    "output1= r2_score(h_sentiment,pred_sentiment)\n",
    "print(\"C = \", cosine_similarity(np.array(h_sentiment).reshape(-1,1),np.array(pred_sentiment).reshape(-1,1)))\n",
    "\n",
    "print(\"Headline MSE: \", np.mean(outputs))\n",
    "print(\"Headline R2:\", np.mean(output1))\n",
    "\n",
    "pred_sentiment1 =model.predict(post_X)\n",
    "\n",
    "print(\"C = \", cosine_similarity(np.array(p_sentiment).reshape(-1,1),np.array(pred_sentiment1).reshape(-1,1)))\n",
    "\n",
    "\n",
    "outputs= sk_mse(p_sentiment,pred_sentiment1)\n",
    "\n",
    "output1= r2_score(p_sentiment,pred_sentiment1)\n",
    "\n",
    "print(\"Post MSE:\", np.mean(outputs))\n",
    "print(\"Post R2:\", np.mean(output1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghosal_head = sparse.csc_matrix([p,a1,b1,c1])\n",
    "ghosal_post = sparse.csc_matrix([p1,a2,b2,c2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASPECT IN SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_sentiment(dropout,learning_rate,em,em_dim,input_shape,lstm_out, n_hidden_layer,em_trainable_flag,n_filters=100):\n",
    "   \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    input_aspect =  Input(shape=(max_length,),name='Aspect')\n",
    "    \n",
    "    embedding1=Embedding(vocab_size, em_dim, weights = [eval(em)],input_length=max_length,trainable = False)\n",
    "    aspect_embedding=Embedding(vocab_size ,em_dim , embeddings_initializer='uniform',trainable = True , input_length = max_length)(input_aspect)\n",
    "\n",
    "    context= embedding1(input_context)    \n",
    "    context=Dropout(0.2)(context)\n",
    "    \n",
    "    concat= concatenate([aspect_embedding,context])\n",
    "    context=Dropout(0.2)(concat)\n",
    "\n",
    "    c3=Conv1D(100,4,activation='relu')(context)    \n",
    "    c3= LSTM(300,return_sequences = True)(c3)\n",
    "    c3=LSTM(100)(c3)\n",
    "    \n",
    "    out=Dense(1,activation='sigmoid')(c3)\n",
    "\n",
    "    model= Model(inputs=[input_context,input_aspect] ,outputs=out)\n",
    "    \n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['cosine'])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=define_model_sentiment(learning_rate=0.00069,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300,\n",
    "                     input_shape= X_aspect.shape[1]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 93 samples\n",
      "Epoch 1/100\n",
      "1173/1173 [==============================] - 24s 20ms/step - loss: 0.1724 - cosine_proximity: -0.3078 - val_loss: 0.2752 - val_cosine_proximity: -0.0538\n",
      "Epoch 2/100\n",
      "1173/1173 [==============================] - 17s 14ms/step - loss: 0.1599 - cosine_proximity: -0.3078 - val_loss: 0.2601 - val_cosine_proximity: -0.0538\n",
      "Epoch 3/100\n",
      "1173/1173 [==============================] - 15s 12ms/step - loss: 0.1586 - cosine_proximity: -0.3078 - val_loss: 0.2666 - val_cosine_proximity: -0.0538\n",
      "Epoch 4/100\n",
      "1173/1173 [==============================] - 17s 14ms/step - loss: 0.1530 - cosine_proximity: -0.3078 - val_loss: 0.2512 - val_cosine_proximity: -0.0538\n",
      "Epoch 5/100\n",
      "1173/1173 [==============================] - 18s 15ms/step - loss: 0.1508 - cosine_proximity: -0.3078 - val_loss: 0.2472 - val_cosine_proximity: -0.0538\n",
      "Epoch 6/100\n",
      "1173/1173 [==============================] - 17s 14ms/step - loss: 0.1503 - cosine_proximity: -0.3078 - val_loss: 0.2472 - val_cosine_proximity: -0.0538\n",
      "Epoch 7/100\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 0.1500 - cosine_proximity: -0.3078 - val_loss: 0.2490 - val_cosine_proximity: -0.0538\n",
      "Epoch 8/100\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 0.1511 - cosine_proximity: -0.3078 - val_loss: 0.2513 - val_cosine_proximity: -0.0538\n",
      "Epoch 9/100\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 0.1480 - cosine_proximity: -0.3078 - val_loss: 0.2559 - val_cosine_proximity: -0.0538\n",
      "Epoch 10/100\n",
      "1173/1173 [==============================] - 17s 15ms/step - loss: 0.1483 - cosine_proximity: -0.3078 - val_loss: 0.2484 - val_cosine_proximity: -0.0538\n",
      "Epoch 11/100\n",
      "1173/1173 [==============================] - 17s 14ms/step - loss: 0.1465 - cosine_proximity: -0.3078 - val_loss: 0.2495 - val_cosine_proximity: -0.0538\n",
      "Epoch 12/100\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 0.1443 - cosine_proximity: -0.3078 - val_loss: 0.2558 - val_cosine_proximity: -0.0538\n",
      "Epoch 13/100\n",
      "1173/1173 [==============================] - 15s 13ms/step - loss: 0.1413 - cosine_proximity: -0.3078 - val_loss: 0.2509 - val_cosine_proximity: -0.0538\n",
      "Epoch 14/100\n",
      "1173/1173 [==============================] - 15s 13ms/step - loss: 0.1346 - cosine_proximity: -0.3078 - val_loss: 0.2605 - val_cosine_proximity: -0.0538\n",
      "Epoch 15/100\n",
      "1173/1173 [==============================] - 14s 12ms/step - loss: 0.1373 - cosine_proximity: -0.3078 - val_loss: 0.2557 - val_cosine_proximity: -0.0538\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x213a74a6b00>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_loss',patience=10,verbose=1)\n",
    "\n",
    "model.fit(x=[trainX,train_aspect],y=train_data['sentiment'], epochs=100,batch_size=16,\n",
    "             callbacks=[EarlyStop],validation_data=([head_X,head_aspect],head_data['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  [0.68950976]\n",
      "Headline MSE:  0.22594306609084852\n",
      "Headline R2: -2.519308900087049\n",
      "C =  [0.76232906]\n",
      "Post MSE: 0.1772782492869399\n",
      "Post R2: -5.221213927521182\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_sentiment =model.predict([head_X,head_aspect])\n",
    "\n",
    "outputs= sk_mse(h_sentiment,pred_sentiment)\n",
    "output1= r2_score(h_sentiment,pred_sentiment)\n",
    "print(\"C = \", cosine_similarity(np.array(h_sentiment).reshape(-1,1),np.array(pred_sentiment).reshape(-1,1)))\n",
    "print(\"Headline MSE: \", np.mean(outputs))\n",
    "print(\"Headline R2:\", np.mean(output1))\n",
    "\n",
    "pred_sentiment1 =model.predict([post_X,post_aspect])\n",
    "\n",
    "#pred_sentiment = rescale(pred_sentiment,[0,1],[-1,1])\n",
    "\n",
    "outputs= sk_mse(p_sentiment,pred_sentiment1)\n",
    "output1= r2_score(p_sentiment,pred_sentiment1)\n",
    "print(\"C = \", cosine_similarity(np.array(p_sentiment).reshape(-1,1),np.array(pred_sentiment1).reshape(-1,1)))\n",
    "print(\"Post MSE:\", np.mean(outputs))\n",
    "print(\"Post R2:\", np.mean(output1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  [0.78494993]\n",
      "Headline MSE:  0.1599008671054071\n",
      "Headline R2: -1.275366972923393\n",
      "C =  [0.88827377]\n",
      "Post MSE: 0.08723961300822569\n",
      "Post R2: -1.8681261816065158\n"
     ]
    }
   ],
   "source": [
    "# 1 STACKED LSTM - GLOVE\n",
    "pred_sentiment =model.predict([head_X,head_aspect])\n",
    "\n",
    "outputs= sk_mse(h_sentiment,pred_sentiment)\n",
    "output1= r2_score(h_sentiment,pred_sentiment)\n",
    "print(\"C = \", cosine_similarity(np.array(h_sentiment).reshape(-1,1),np.array(pred_sentiment).reshape(-1,1)))\n",
    "print(\"Headline MSE: \", np.mean(outputs))\n",
    "print(\"Headline R2:\", np.mean(output1))\n",
    "\n",
    "pred_sentiment1 =model.predict([post_X,post_aspect])\n",
    "\n",
    "#pred_sentiment = rescale(pred_sentiment,[0,1],[-1,1])\n",
    "\n",
    "outputs= sk_mse(p_sentiment,pred_sentiment1)\n",
    "output1= cp_sentiment,pred_sentiment1)\n",
    "print(\"C = \", cosine_similarity(np.array(p_sentiment).reshape(-1,1),np.array(pred_sentiment1).reshape(-1,1)))\n",
    "print(\"Post MSE:\", np.mean(outputs))\n",
    "print(\"Post R2:\", np.mean(output1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels (trainY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(trainY)\n",
    "    temp1 = le.transform(trainY)\n",
    "    return to_categorical(temp1,27), le.classes_, trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1hot,lable_encoding,train_y=convert_labels(train_data['aspect'])\n",
    "data_1hot,lable_encoding,data_y=convert_labels(dataframe['aspect'])\n",
    "\n",
    "#p_1hot,lable_encoding1,p_y=convert_labels(post['aspect'])\n",
    "\n",
    "#new_train_1hot,_,new_train_y=convert_labels(new_train['aspect'])\n",
    "head_1hot,_,head_y=convert_labels(head_data['aspect'])\n",
    "post_1hot,_,post_y=convert_labels(post_data['aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Financial'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_pred(pred):\n",
    "    return [lable_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUR MODEL AND ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def define_model_1(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag,em_dim,max_length=16):\n",
    "     \n",
    "    input_context= Input(shape=(max_length,),name='Context')    \n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    " \n",
    "    context= embedding(input_context)\n",
    "    \n",
    "    context=Dropout(0.2)(context)\n",
    "        \n",
    "    a = Bidirectional(LSTM(300, return_sequences=True,recurrent_dropout=dropout))(context)\n",
    "    \n",
    "    a = Attention()(a)\n",
    "    \n",
    "    x=Dense(300,activation='relu')(a)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "\n",
    "    model= Model(inputs=input_context ,outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "   # print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "Our_model = define_model_1(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='glove',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our_model+attention\n",
      "Train on 1173 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "1173/1173 [==============================] - 15s 13ms/step - loss: 2.5731 - acc: 0.3487 - val_loss: 5.7051 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1173/1173 [==============================] - 10s 8ms/step - loss: 2.0103 - acc: 0.4544 - val_loss: 6.0559 - val_acc: 0.0101\n",
      "Epoch 3/100\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 1.6340 - acc: 0.5337 - val_loss: 6.3559 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1173/1173 [==============================] - 9s 8ms/step - loss: 1.3125 - acc: 0.6232 - val_loss: 6.8383 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1173/1173 [==============================] - 10s 8ms/step - loss: 1.0842 - acc: 0.6692 - val_loss: 7.6672 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1173/1173 [==============================] - 10s 8ms/step - loss: 0.8960 - acc: 0.7425 - val_loss: 8.9355 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1173/1173 [==============================] - 9s 8ms/step - loss: 0.7515 - acc: 0.7707 - val_loss: 9.9347 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1173/1173 [==============================] - 10s 8ms/step - loss: 0.6302 - acc: 0.7988 - val_loss: 9.5423 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1173/1173 [==============================] - 9s 8ms/step - loss: 0.5448 - acc: 0.8406 - val_loss: 10.2213 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 0.4356 - acc: 0.8662 - val_loss: 11.9139 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 0.4079 - acc: 0.8713 - val_loss: 11.4384 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1173/1173 [==============================] - 9s 8ms/step - loss: 0.4088 - acc: 0.8645 - val_loss: 11.3652 - val_acc: 0.0000e+00\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x213cd0b87b8>"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Our_model+attention\")\n",
    "\n",
    "EarlyStop= EarlyStopping(monitor='val_acc',patience=10,verbose=1)\n",
    "\n",
    "Our_model.fit(x=trainX_1,y=train_1hot, epochs=100,batch_size=64,callbacks=[EarlyStop],validation_data=(post_X_1,post_1hot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headlines\n",
      "Accuracy:  0.3010752688172043\n",
      "Macro:  0.15251317856939414\n",
      "Weighted:  0.2835542472315357\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post\n",
      "Accuracy:  0.6666666666666666\n",
      "Macro:  0.1852310988674625\n",
      "Weighted:  0.7065316464306364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Headlines\")\n",
    "pred8 =Our_model.predict(head_X_1)\n",
    "pred= get_class_from_pred(pred8)\n",
    "print(\"Accuracy: \" ,accuracy_score(head_data['aspect'],pred))\n",
    "print(\"Macro: \",f1_score(head_data['aspect'],pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(head_data['aspect'],pred,average='weighted'))\n",
    "\n",
    "\n",
    "print()\n",
    "pred1 =Our_model.predict(post_X_1)\n",
    "print(\"Post\")\n",
    "pred= get_class_from_pred(pred1)\n",
    "print(\"Accuracy: \" ,accuracy_score(post_data['aspect'],pred))\n",
    "print(\"Macro: \",f1_score(post_data['aspect'],pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(post_data['aspect'],pred,average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, feature_vector_train, label, feature_vector_valid,feature_vector_valid1):\n",
    "    # fit the training dataset on the classifier\n",
    " \n",
    "    model.fit(feature_vector_train, label)\n",
    "        \n",
    "    predictions = model.predict(feature_vector_valid)\n",
    "    predictions1 = model.predict(feature_vector_valid1)\n",
    "    predictions2 = model.predict(feature_vector_train)\n",
    "\n",
    "    #pred=get_class_from_pred(predictions)\n",
    "    #pred1=get_class_from_pred(predictions1)\n",
    "    print(\"Train-> \")\n",
    "    print(\"Accuracy: \",accuracy_score(predictions2,train_y))\n",
    "    print(\"F1 score: \",f1_score(train_data['aspect'],predictions2,average='macro'))\n",
    "    print(\"Weighted F1 score: \",f1_score(train_data['aspect'],predictions2,average='weighted'))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    print(\"Headlines-> \")\n",
    "    print(\"Accuracy: \",accuracy_score(predictions,head_y))\n",
    "    print(\"F1 score: \",f1_score(head_data['aspect'],predictions,average='macro'))\n",
    "    print(\"Weighted F1 score: \",f1_score(head_data['aspect'],predictions,average='weighted'))\n",
    "    print()\n",
    "    \n",
    "    print(\"Post-> \")\n",
    "    print(\"Accuracy: \",accuracy_score(predictions1, post_y)) \n",
    "    print(\"F1 score: \",f1_score(post_data['aspect'],predictions1,average='macro'))\n",
    "    print(\"Weighted F1 score: \",f1_score(post_data['aspect'],predictions1,average='weighted'))\n",
    "     \n",
    "    return predictions,predictions1,predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1173, 3023)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "LogisticRegression\n",
      "******Results******\n",
      "Headlines-> \n",
      "Accuracy : 11.8280%\n",
      "F1 score:  0.010582010582010583\n",
      "Weighted F1 score:  0.028673835125448032\n",
      "\n",
      "Post-> \n",
      "Accuracy : 48.4848%\n",
      "F1 score:  0.0909090909090909\n",
      "Weighted F1 score:  0.404040404040404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "LogisticRegression\n",
      "******Results******\n",
      "Headlines-> \n",
      "Accuracy : 11.8280%\n",
      "F1 score:  0.01198257080610022\n",
      "Weighted F1 score:  0.027830487033523088\n",
      "\n",
      "Post-> \n",
      "Accuracy : 53.5354%\n",
      "F1 score:  0.10028382213812678\n",
      "Weighted F1 score:  0.38999264164827085\n",
      "==============================\n",
      "LogisticRegression\n",
      "******Results******\n",
      "Headlines-> \n",
      "Accuracy : 11.8280%\n",
      "F1 score:  0.011111111111111112\n",
      "Weighted F1 score:  0.028673835125448032\n",
      "\n",
      "Post-> \n",
      "Accuracy : 54.5455%\n",
      "F1 score:  0.1411764705882353\n",
      "Weighted F1 score:  0.39215686274509803\n",
      "==============================\n",
      "KNeighborsClassifier\n",
      "******Results******\n",
      "Headlines-> \n",
      "Accuracy : 11.8280%\n",
      "F1 score:  0.04412974122651542\n",
      "Weighted F1 score:  0.09578403906187465\n",
      "\n",
      "Post-> \n",
      "Accuracy : 23.2323%\n",
      "F1 score:  0.031991153816823\n",
      "Weighted F1 score:  0.27559742933799214\n",
      "==============================\n",
      "SVC\n",
      "******Results******\n",
      "Headlines-> \n",
      "Accuracy : 12.9032%\n",
      "F1 score:  0.0126984126984127\n",
      "Weighted F1 score:  0.029493087557603687\n",
      "\n",
      "Post-> \n",
      "Accuracy : 55.5556%\n",
      "F1 score:  0.17857142857142858\n",
      "Weighted F1 score:  0.3968253968253968\n",
      "==============================\n",
      "DecisionTreeClassifier\n",
      "******Results******\n",
      "Headlines-> \n",
      "Accuracy : 7.5269%\n",
      "F1 score:  0.03624307537351016\n",
      "Weighted F1 score:  0.07422676132353553\n",
      "\n",
      "Post-> \n",
      "Accuracy : 12.1212%\n",
      "F1 score:  0.019047619047619046\n",
      "Weighted F1 score:  0.16469456469456464\n",
      "==============================\n",
      "RandomForestClassifier\n",
      "******Results******\n",
      "Headlines-> \n",
      "Accuracy : 10.7527%\n",
      "F1 score:  0.01933621933621934\n",
      "Weighted F1 score:  0.05526230042359076\n",
      "\n",
      "Post-> \n",
      "Accuracy : 25.2525%\n",
      "F1 score:  0.028011204481792715\n",
      "Weighted F1 score:  0.26455026455026454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "AdaBoostClassifier\n",
      "******Results******\n",
      "Headlines-> \n",
      "Accuracy : 10.7527%\n",
      "F1 score:  0.010964912280701754\n",
      "Weighted F1 score:  0.026881720430107527\n",
      "\n",
      "Post-> \n",
      "Accuracy : 52.5253%\n",
      "F1 score:  0.13959731543624163\n",
      "Weighted F1 score:  0.38777032065622674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "GradientBoostingClassifier\n",
      "******Results******\n",
      "Headlines-> \n",
      "Accuracy : 12.9032%\n",
      "F1 score:  0.02643658810325477\n",
      "Weighted F1 score:  0.060598819738604696\n",
      "\n",
      "Post-> \n",
      "Accuracy : 37.3737%\n",
      "F1 score:  0.050935940699720225\n",
      "Weighted F1 score:  0.33323864819927806\n"
     ]
    }
   ],
   "source": [
    "classifiers = [ \n",
    "                LogisticRegression(C = 100, random_state = 8),\n",
    "                LogisticRegression(C = 1, random_state = 8),\n",
    "                LogisticRegression(C = 0.01, random_state = 8),\n",
    "                KNeighborsClassifier(3) ,\n",
    "                SVC( kernel = 'rbf' , C = 0.025),\n",
    "                DecisionTreeClassifier() ,\n",
    "                RandomForestClassifier() , \n",
    "                AdaBoostClassifier(),\n",
    "                GradientBoostingClassifier() \n",
    "              ]\n",
    "\n",
    "TRAIN_PREDICTIONS=[]\n",
    "HEAD_PREDICTIONS=[]\n",
    "POST_PREDICTIONS=[]\n",
    "for clf in classifiers:\n",
    "    \n",
    "    clf.fit( trainX , train_y)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print( \"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('******Results******')\n",
    "\n",
    "    \n",
    "    print(\"Headlines-> \")\n",
    "    train_predictions = clf.predict(head_X)\n",
    "    acc = accuracy_score( head_data['aspect'],train_predictions )\n",
    "    print(\"Accuracy : {:.4%}\".format(acc))\n",
    "    print(\"F1 score: \",f1_score(head_data['aspect'],train_predictions,average='macro'))\n",
    "    print(\"Weighted F1 score: \",f1_score(head_data['aspect'],train_predictions,average='weighted'))\n",
    "    HEAD_PREDICTIONS.append(train_predictions)\n",
    "    print()\n",
    "    train_predictions = clf.predict(post_X)\n",
    "    print(\"Post-> \")\n",
    "    acc=accuracy_score(post_data['aspect'], train_predictions)\n",
    "    print(\"Accuracy : {:.4%}\".format(acc))\n",
    "    print(\"F1 score: \",f1_score(post_data['aspect'],train_predictions,average='macro'))\n",
    "    print(\"Weighted F1 score: \",f1_score(post_data['aspect'],train_predictions,average='weighted'))\n",
    "    POST_PREDICTIONS.append(train_predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors: \n",
      "Train-> \n",
      "Accuracy:  0.83461210571185\n",
      "F1 score:  0.5398318964173976\n",
      "Weighted F1 score:  0.8093349375443583\n",
      "\n",
      "Headlines-> \n",
      "Accuracy:  0.20430107526881722\n",
      "F1 score:  0.09240748931151407\n",
      "Weighted F1 score:  0.11251468329267189\n",
      "\n",
      "Post-> \n",
      "Accuracy:  0.7171717171717171\n",
      "F1 score:  0.28548644338118023\n",
      "Weighted F1 score:  0.6443381180223285\n",
      "\n",
      "NB, WordLevel TF-IDF: \n",
      "Train-> \n",
      "Accuracy:  0.44501278772378516\n",
      "F1 score:  0.08579416348426996\n",
      "Weighted F1 score:  0.3225299686325696\n",
      "\n",
      "Headlines-> \n",
      "Accuracy:  0.13978494623655913\n",
      "F1 score:  0.0405982905982906\n",
      "Weighted F1 score:  0.04590570719602977\n",
      "\n",
      "Post-> \n",
      "Accuracy:  0.5555555555555556\n",
      "F1 score:  0.17857142857142858\n",
      "Weighted F1 score:  0.3968253968253968\n",
      "\n",
      "NB, N-Gram Vectors: \n",
      "Train-> \n",
      "Accuracy:  0.38107416879795397\n",
      "F1 score:  0.03004565865759332\n",
      "Weighted F1 score:  0.21987978631758118\n",
      "\n",
      "Headlines-> \n",
      "Accuracy:  0.12903225806451613\n",
      "F1 score:  0.0126984126984127\n",
      "Weighted F1 score:  0.029493087557603687\n",
      "\n",
      "Post-> \n",
      "Accuracy:  0.5555555555555556\n",
      "F1 score:  0.17857142857142858\n",
      "Weighted F1 score:  0.3968253968253968\n",
      "\n",
      "NB, CharLevel Vectors: \n",
      "Train-> \n",
      "Accuracy:  0.37254901960784315\n",
      "F1 score:  0.020105820105820103\n",
      "Weighted F1 score:  0.2022408963585434\n",
      "\n",
      "Headlines-> \n",
      "Accuracy:  0.12903225806451613\n",
      "F1 score:  0.0126984126984127\n",
      "Weighted F1 score:  0.029493087557603687\n",
      "\n",
      "Post-> \n",
      "Accuracy:  0.5555555555555556\n",
      "F1 score:  0.17857142857142858\n",
      "Weighted F1 score:  0.3968253968253968\n",
      "\n",
      "NB, Sending all the features \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-> \n",
      "Accuracy:  0.6086956521739131\n",
      "F1 score:  0.2861502360747593\n",
      "Weighted F1 score:  0.5532417288131621\n",
      "\n",
      "Headlines-> \n",
      "Accuracy:  0.13978494623655913\n",
      "F1 score:  0.0405982905982906\n",
      "Weighted F1 score:  0.04590570719602977\n",
      "\n",
      "Post-> \n",
      "Accuracy:  0.5858585858585859\n",
      "F1 score:  0.24461920529801326\n",
      "Weighted F1 score:  0.4577396481369992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes on Count Vectors\n",
    "HEAD_PREDICTIONS_2=[]\n",
    "POST_PREDICTIONS_2=[]\n",
    "\n",
    "\n",
    "print(\"NB, Count Vectors: \")\n",
    "h_predictions,p_predictions,TRAIN=train_model(naive_bayes.MultinomialNB(), xtrain_count,train_y, head_count,post_count)\n",
    "HEAD_PREDICTIONS_2.append(h_predictions)\n",
    "POST_PREDICTIONS_2.append(p_predictions)\n",
    "\n",
    "print()\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "print (\"NB, WordLevel TF-IDF: \")\n",
    "h_predictions,p_predictions,TRAIN1=train_model(naive_bayes.MultinomialNB(), xtrain_tfidf,train_y,head_tfidf,post_tfidf)\n",
    "print()\n",
    "HEAD_PREDICTIONS_2.append(h_predictions)\n",
    "POST_PREDICTIONS_2.append(p_predictions)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "print( \"NB, N-Gram Vectors: \")\n",
    "h_predictions,p_predictions,TRAIN2=train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram,train_y, head_tfidf_ngram,post_tfidf_ngram)\n",
    "print()\n",
    "HEAD_PREDICTIONS_2.append(h_predictions)\n",
    "POST_PREDICTIONS_2.append(p_predictions)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "print (\"NB, CharLevel Vectors: \")\n",
    "h_predictions,p_predictions,TRAIN3=train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, head_tfidf_ngram_chars,post_tfidf_ngram_chars)\n",
    "print()\n",
    "HEAD_PREDICTIONS_2.append(h_predictions)\n",
    "POST_PREDICTIONS_2.append(p_predictions)\n",
    "\n",
    "\n",
    "print (\"NB, Sending all the features \")\n",
    "h_predictions,p_predictions,TRAIN4=train_model(naive_bayes.MultinomialNB(), X_aspect, train_y, x_test_1_aspect,x_test_2_aspect)\n",
    "print()\n",
    "HEAD_PREDICTIONS_2.append(h_predictions)\n",
    "POST_PREDICTIONS_2.append(p_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1173,)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_architecture(input_size,input_size1):\n",
    "    # create input layer \n",
    "    input_layer = Input((input_size, ), sparse=True)\n",
    "    input_layer1 = Input((input_size1, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = Dense(500, activation=\"relu\")(input_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "    \n",
    "    hidden_layer = Dense(50, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "    hidden_layer = Dense(20, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer1 = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "     # create hidden layer\n",
    "    hidden_layer = Dense(500, activation=\"relu\")(input_layer1)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "    \n",
    "    hidden_layer = Dense(50, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "    hidden_layer = Dense(20, activation=\"relu\")(hidden_layer)\n",
    "    hidden_layer2 = Dropout(0.2)(hidden_layer)\n",
    "\n",
    "    concat=concatenate([hidden_layer1,hidden_layer2])\n",
    "    output_layer = Dense(27, activation=\"softmax\")(concat)\n",
    "    output_layer = Dropout(0.2)(output_layer)\n",
    "    \n",
    "\n",
    "\n",
    "    classifier = Model(inputs = [input_layer,input_layer1], outputs = output_layer)\n",
    "    classifier.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Conv1d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-493-377b769844fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model_architecture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_s2v_g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-492-a222d483cedd>\u001b[0m in \u001b[0;36mcreate_model_architecture\u001b[1;34m(input_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mConv1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mMaxPooling1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mhidden_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Conv1d' is not defined"
     ]
    }
   ],
   "source": [
    "model = create_model_architecture(X_s2v_g.shape[1],X_aspect.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1173,)"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [<1173x200 sparse matrix of type '<class 'numpy.float64'>'\n\twith 234599 stored elements in Compressed Sparse Column format>]...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-465-a93f3107095d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_s2v_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_1hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 955\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    956\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [<1173x200 sparse matrix of type '<class 'numpy.float64'>'\n\twith 234599 stored elements in Compressed Sparse Column format>]..."
     ]
    }
   ],
   "source": [
    "model.fit(X_s2v_g, data_1hot,batch_size=64, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headlines\n",
      "Accuracy:  0.11827956989247312\n",
      "Macro:  0.011866235167206042\n",
      "Weighted:  0.027560288130285\n",
      "\n",
      "Post\n",
      "Accuracy:  0.40404040404040403\n",
      "Macro:  0.07299270072992702\n",
      "Weighted:  0.32441200324412006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Headlines\")\n",
    "pred8 =model.predict([X_h2v_g,x_test_1_aspect])\n",
    "pred= get_class_from_pred(pred8)\n",
    "print(\"Accuracy: \" ,accuracy_score(head_data['aspect'],pred))\n",
    "print(\"Macro: \",f1_score(head_data['aspect'],pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(head_data['aspect'],pred,average='weighted'))\n",
    "\n",
    "\n",
    "print()\n",
    "pred1 =model.predict([X_p2v_g,x_test_2_aspect])\n",
    "print(\"Post\")\n",
    "pred= get_class_from_pred(pred1)\n",
    "print(\"Accuracy: \" ,accuracy_score(post_data['aspect'],pred))\n",
    "print(\"Macro: \",f1_score(post_data['aspect'],pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(post_data['aspect'],pred,average='weighted'))\n",
    "\n",
    "#print(\"Headline accuracy: \", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "\n",
    "dim_learning_rate = Categorical(categories=[0.01,0.1,0.001,0.0001],name='learning_rate')\n",
    "\n",
    "\n",
    "dropout = Real(low=0.2, high=0.9,name='dropout')\n",
    "\n",
    "\n",
    "para_batch_size = Categorical(categories=[16,32,64],name='batch_size')\n",
    "\n",
    "dense_nodes = Integer(low=256, high=512, name='dense_nodes')\n",
    "\n",
    "\n",
    "lstm_out = Integer(low=200, high=400, name='lstm_out')\n",
    "\n",
    "\n",
    "\n",
    "para_filter_size = Categorical(categories=[3,4,5,6],name='filter_size')\n",
    "para_n_filters = Categorical(categories=[100,200,300,400],name='n_filters')\n",
    "\n",
    "#In[17]:\n",
    "\n",
    "\n",
    "parameters = [dim_learning_rate,\n",
    "              dropout,\n",
    "              para_batch_size,\n",
    "              lstm_out,\n",
    "             para_filter_size,\n",
    "              para_n_filters]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "default_parameters = [0.001,0.2,16,200,4,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@use_named_args(dimensions=parameters)\n",
    "def fitness(learning_rate,dropout,batch_size,lstm_out,filter_size,n_filters):\n",
    "        \n",
    "\n",
    "#        Print the hyper-parameters.\n",
    "        print('----------------------------- combination ------------------')\n",
    "        print('learning rate===> ',learning_rate)\n",
    "        print('lstm_out:', lstm_out)\n",
    "        print('dropout===>',dropout)\n",
    "        print('batch_size===> ',batch_size)\n",
    "        print('filter_size===> ',filter_size)\n",
    "        print('num_filters===> ',n_filters)\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "        \n",
    "        model = define_model(learning_rate=learning_rate,\n",
    "                                 lstm_out=lstm_out,\n",
    "                                 dropout=dropout,\n",
    "                                 em='glove',\n",
    "                                 em_trainable_flag=False,\n",
    "                                 em_dim=200,\n",
    "                                 n_filters=n_filters,\n",
    "                                 filter_size=filter_size)\n",
    "\n",
    "  \n",
    "   \n",
    "        earlystop = EarlyStopping(monitor='val_loss',patience=10,verbose=1, mode='auto')\n",
    "\n",
    "        history = model.fit(x=trainX,\n",
    "                            y=train_1hot,\n",
    "                            epochs=30,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[earlystop],\n",
    "                            validation_data=(head_X,head_1hot)\n",
    "                            )\n",
    "        accuracy = history.history['acc'][-1]\n",
    "       \n",
    "        print()\n",
    "        print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        _,accuracy_1= model.evaluate(x=head_X , y=head_1hot)\n",
    "\n",
    "        pred1 = model.predict(head_X)\n",
    "        pred_class = get_class_from_pred(pred1)\n",
    "\n",
    "        \n",
    "\n",
    "        print(\"Test Headline\")\n",
    "        print(\"F1 Score Macro: \",f1_score(head_data['aspect'],pred_class,average='macro'))\n",
    "        print(\"F1 Score Weighted: \",f1_score(head_data['aspect'],pred_class,average='weighted'))\n",
    "        print(\"Accuracy: \",accuracy_1)\n",
    "        print()\n",
    "\n",
    "        _,accuracy_2 = model.evaluate(x=post_X , y=post_1hot)\n",
    "\n",
    "\n",
    "        pred2= model.predict(post_X)\n",
    "        pred_class = get_class_from_pred(pred2)\n",
    "\n",
    "        print(\"Test Post\")\n",
    "        print(\"F1 Score Macro: \",f1_score(post_data['aspect'],pred_class,average='macro'))\n",
    "        print(\"F1 Score Weighted: \",f1_score(post_data['aspect'],pred_class,average='weighted'))\n",
    "        print(\"Accuracy: \",accuracy_2)\n",
    "        print()\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        K.clear_session()\n",
    "        \n",
    "        print(\"Session cleared \")\n",
    "        \n",
    "        return -accuracy\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- combination ------------------\n",
      "learning rate===>  0.001\n",
      "lstm_out: 200\n",
      "dropout===> 0.2\n",
      "batch_size===>  16\n",
      "filter_size===>  4\n",
      "num_filters===>  100\n",
      "\n",
      "Train on 1173 samples, validate on 93 samples\n",
      "Epoch 1/30\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 2.5756 - acc: 0.3674 - val_loss: 4.1007 - val_acc: 0.0108\n",
      "Epoch 2/30\n",
      "1173/1173 [==============================] - 3s 2ms/step - loss: 2.4820 - acc: 0.3725 - val_loss: 3.9696 - val_acc: 0.0108\n",
      "Epoch 3/30\n",
      "1173/1173 [==============================] - 3s 2ms/step - loss: 2.4735 - acc: 0.3725 - val_loss: 3.9040 - val_acc: 0.0108\n",
      "Epoch 4/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4766 - acc: 0.3717 - val_loss: 4.1336 - val_acc: 0.0108\n",
      "Epoch 5/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4724 - acc: 0.3725 - val_loss: 3.9995 - val_acc: 0.0108\n",
      "Epoch 6/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4668 - acc: 0.3725 - val_loss: 4.0109 - val_acc: 0.0108\n",
      "Epoch 7/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4569 - acc: 0.3725 - val_loss: 4.0515 - val_acc: 0.0108\n",
      "Epoch 8/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4260 - acc: 0.3725 - val_loss: 4.0836 - val_acc: 0.0108\n",
      "Epoch 9/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.3817 - acc: 0.3700 - val_loss: 3.9887 - val_acc: 0.0108\n",
      "Epoch 10/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.3281 - acc: 0.3828 - val_loss: 4.2744 - val_acc: 0.0108\n",
      "Epoch 11/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.2903 - acc: 0.3896 - val_loss: 4.3077 - val_acc: 0.0108\n",
      "Epoch 12/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.2130 - acc: 0.4007 - val_loss: 4.2473 - val_acc: 0.0108\n",
      "Epoch 13/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.1699 - acc: 0.3998 - val_loss: 4.2503 - val_acc: 0.0108\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "Accuracy: 39.98%\n",
      "\n",
      "93/93 [==============================] - 0s 525us/step\n",
      "Test Headline\n",
      "F1 Score Macro:  0.015531475748194011\n",
      "F1 Score Weighted:  0.03986928104575163\n",
      "Accuracy:  0.010752688172043012\n",
      "\n",
      "99/99 [==============================] - 0s 538us/step\n",
      "Test Post\n",
      "F1 Score Macro:  0.11032863849765258\n",
      "F1 Score Weighted:  0.3677621283255086\n",
      "Accuracy:  0.0\n",
      "\n",
      "Session cleared \n",
      "----------------------------- combination ------------------\n",
      "learning rate===>  0.001\n",
      "lstm_out: 256\n",
      "dropout===> 0.7775130357834736\n",
      "batch_size===>  32\n",
      "filter_size===>  6\n",
      "num_filters===>  100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 93 samples\n",
      "Epoch 1/30\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 2.6237 - acc: 0.3640 - val_loss: 3.9844 - val_acc: 0.0108\n",
      "Epoch 2/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.4675 - acc: 0.3725 - val_loss: 3.8952 - val_acc: 0.0108\n",
      "Epoch 3/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.4713 - acc: 0.3725 - val_loss: 4.0136 - val_acc: 0.0108\n",
      "Epoch 4/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4690 - acc: 0.3725 - val_loss: 4.0290 - val_acc: 0.0108\n",
      "Epoch 5/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4643 - acc: 0.3725 - val_loss: 3.8601 - val_acc: 0.0108\n",
      "Epoch 6/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4589 - acc: 0.3725 - val_loss: 4.0494 - val_acc: 0.0108 loss: 2.4\n",
      "Epoch 7/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4523 - acc: 0.3725 - val_loss: 4.0285 - val_acc: 0.0108\n",
      "Epoch 8/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4463 - acc: 0.3725 - val_loss: 4.0760 - val_acc: 0.0108\n",
      "Epoch 9/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.4437 - acc: 0.3725 - val_loss: 4.0533 - val_acc: 0.0108\n",
      "Epoch 10/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4340 - acc: 0.3725 - val_loss: 3.8950 - val_acc: 0.0108\n",
      "Epoch 11/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4358 - acc: 0.3725 - val_loss: 4.1403 - val_acc: 0.0108\n",
      "Epoch 12/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.4246 - acc: 0.3725 - val_loss: 3.9591 - val_acc: 0.0108\n",
      "Epoch 13/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.4150 - acc: 0.3725 - val_loss: 3.7974 - val_acc: 0.0108\n",
      "Epoch 14/30\n",
      "1173/1173 [==============================] - ETA: 0s - loss: 2.4156 - acc: 0.372 - 4s 4ms/step - loss: 2.4133 - acc: 0.3708 - val_loss: 4.0311 - val_acc: 0.0108\n",
      "Epoch 15/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.3916 - acc: 0.3725 - val_loss: 4.0916 - val_acc: 0.0108\n",
      "Epoch 16/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.3764 - acc: 0.3725 - val_loss: 3.9916 - val_acc: 0.0108\n",
      "Epoch 17/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.3717 - acc: 0.3725 - val_loss: 3.9777 - val_acc: 0.0108\n",
      "Epoch 18/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.3647 - acc: 0.3725 - val_loss: 3.9306 - val_acc: 0.0108\n",
      "Epoch 19/30\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.3475 - acc: 0.3734 - val_loss: 4.0173 - val_acc: 0.0215\n",
      "Epoch 20/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.3367 - acc: 0.3751 - val_loss: 4.0775 - val_acc: 0.0108\n",
      "Epoch 21/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.3203 - acc: 0.3768 - val_loss: 4.0527 - val_acc: 0.0108\n",
      "Epoch 22/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.3097 - acc: 0.3725 - val_loss: 4.1655 - val_acc: 0.0108\n",
      "Epoch 23/30\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.3240 - acc: 0.3768 - val_loss: 4.1118 - val_acc: 0.0108\n",
      "Epoch 00023: early stopping\n",
      "\n",
      "Accuracy: 37.68%\n",
      "\n",
      "93/93 [==============================] - 0s 826us/step\n",
      "Test Headline\n",
      "F1 Score Macro:  0.012600229095074456\n",
      "F1 Score Weighted:  0.02926504822081809\n",
      "Accuracy:  0.010752688172043012\n",
      "\n",
      "99/99 [==============================] - 0s 856us/step\n",
      "Test Post\n",
      "F1 Score Macro:  0.12\n",
      "F1 Score Weighted:  0.4\n",
      "Accuracy:  0.0\n",
      "\n",
      "Session cleared \n",
      "----------------------------- combination ------------------\n",
      "learning rate===>  0.0001\n",
      "lstm_out: 202\n",
      "dropout===> 0.4330959593643058\n",
      "batch_size===>  32\n",
      "filter_size===>  4\n",
      "num_filters===>  300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed=7\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "#fitness(default_parameters)\n",
    "\n",
    "\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=parameters,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(dropout,learning_rate,em,em_dim,lstm_out,em_trainable_flag,n_filters=100,filter_size=4):\n",
    "   \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "   \n",
    "    embedding1=Embedding(vocab_size, em_dim, weights = [eval(em)],input_length=max_length,trainable = False)\n",
    "    \n",
    "    context= embedding1(input_context)    \n",
    "    context=Dropout(dropout)(context)\n",
    "    \n",
    "    c3=Conv1D(n_filters,filter_size,activation='relu')(context)    \n",
    "    c3= LSTM(lstm_out,return_sequences = True)(c3)\n",
    "    c3=LSTM(lstm_out)(c3)\n",
    "    \n",
    "    out=Dense(27,activation='softmax')(c3)\n",
    "\n",
    "    model= Model(inputs=input_context ,outputs=out)\n",
    "    \n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Our_model = define_model(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='glove',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "1173/1173 [==============================] - 5s 5ms/step - loss: 2.7465 - acc: 0.3495 - val_loss: 4.3676 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1173/1173 [==============================] - 1s 922us/step - loss: 2.3933 - acc: 0.3725 - val_loss: 4.6635 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1173/1173 [==============================] - 1s 917us/step - loss: 2.1443 - acc: 0.3990 - val_loss: 5.1352 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1173/1173 [==============================] - 1s 915us/step - loss: 2.0209 - acc: 0.4169 - val_loss: 5.3956 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1173/1173 [==============================] - 1s 908us/step - loss: 1.9269 - acc: 0.4510 - val_loss: 5.9812 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1173/1173 [==============================] - 1s 928us/step - loss: 1.8416 - acc: 0.4689 - val_loss: 6.6469 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1173/1173 [==============================] - 1s 911us/step - loss: 1.6892 - acc: 0.5047 - val_loss: 6.6936 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1173/1173 [==============================] - 1s 926us/step - loss: 1.5224 - acc: 0.5746 - val_loss: 7.0746 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1173/1173 [==============================] - 1s 940us/step - loss: 1.3822 - acc: 0.6104 - val_loss: 7.6553 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1173/1173 [==============================] - 1s 996us/step - loss: 1.3234 - acc: 0.6172 - val_loss: 8.2820 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1173/1173 [==============================] - 1s 948us/step - loss: 1.2038 - acc: 0.6530 - val_loss: 7.5159 - val_acc: 0.0000e+00\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x213c43d7048>"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_acc',patience=10,verbose=1)\n",
    "\n",
    "Our_model.fit(x=trainX_1,y=train_1hot, epochs=100,batch_size=64,callbacks=[EarlyStop],validation_data=(post_X_1,post_1hot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headlines\n",
      "Accuracy:  0.1935483870967742\n",
      "Macro:  0.11756945908546576\n",
      "Weighted:  0.16769974130098622\n",
      "\n",
      "Post\n",
      "Accuracy:  0.6161616161616161\n",
      "Macro:  0.1373684567859325\n",
      "Weighted:  0.6435288453823317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Headlines\")\n",
    "pred8 =Our_model.predict(head_X_1)\n",
    "pred= get_class_from_pred(pred8)\n",
    "print(\"Accuracy: \" ,accuracy_score(head_data['aspect'],pred))\n",
    "print(\"Macro: \",f1_score(head_data['aspect'],pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(head_data['aspect'],pred,average='weighted'))\n",
    "\n",
    "\n",
    "print()\n",
    "pred1 =Our_model.predict(post_X_1)\n",
    "print(\"Post\")\n",
    "pred= get_class_from_pred(pred1)\n",
    "print(\"Accuracy: \" ,accuracy_score(post_data['aspect'],pred))\n",
    "print(\"Macro: \",f1_score(post_data['aspect'],pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(post_data['aspect'],pred,average='weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headlines\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-859f5c64ac08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Headlines\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpred2\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_1_aspect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mget_class_from_pred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: \"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'aspect'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Headlines\")\n",
    "pred2 =model.predict(x_test_1_aspect)\n",
    "pred= get_class_from_pred(pred2)\n",
    "print(\"Accuracy: \" ,accuracy_score(head_data['aspect'],pred))\n",
    "print(\"Macro: \",f1_score(head_data['aspect'],pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(head_data['aspect'],pred,average='weighted'))\n",
    "\n",
    "\n",
    "print()\n",
    "pred3 =model.predict(x_test_2_aspect)\n",
    "print(\"Post\")\n",
    "pred= get_class_from_pred(pred3)\n",
    "print(\"Accuracy: \" ,accuracy_score(post_data['aspect'],pred))\n",
    "print(\"Macro: \",f1_score(post_data['aspect'],pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(post_data['aspect'],pred,average='weighted'))\n",
    "\n",
    "#print(\"Headline accuracy: \", outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
