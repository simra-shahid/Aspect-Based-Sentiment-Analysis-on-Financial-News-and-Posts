{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Lambda,Reshape,concatenate,Input, Embedding, LSTM\n",
    "from keras.layers import Dense,Dropout, Activation ,Flatten ,RepeatVector, Bidirectional,GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.activations import softmax\n",
    "from keras import regularizers\n",
    "\n",
    "from keras import backend as K, regularizers, constraints, initializers\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras.layers.convolutional import Conv1D,Conv2D\n",
    "from keras.layers.convolutional import MaxPooling1D,MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from Attention import Attention\n",
    "\n",
    "from keras.layers import Concatenate,Dot\n",
    "from keras.layers import Permute, merge\n",
    "\n",
    "# FOR ATAE\n",
    "from AttentionwithContext import AttentionWithContext\n",
    "from Final import FinalSentenceRepresentation\n",
    "\n",
    "from final2 import Final2\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping by F1\n",
    "import keras\n",
    "class EarlyStopByF1(keras.callbacks.Callback):\n",
    "    def __init__(self, value = 0, verbose = 0):\n",
    "        super(keras.callbacks.Callback, self).__init__()\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "         predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "         target = self.validation_data[1]\n",
    "         score = f1_score(target, prediction)\n",
    "         if score > self.value:\n",
    "            if self.verbose >0:\n",
    "                print(\"Epoch %05d: early stopping Threshold\" % epoch)\n",
    "            self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pickle.load(open(\"D:/PythonCodes/Sentiment-Analysis/Data/all_data.dat\",\"rb\"))\n",
    "b=pickle.load(open(\"D:/PythonCodes/Sentiment-Analysis/Data/all_data23.dat\",\"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test_raw', 'embedding_matrix_sentiment', 'input_aspect', 'sentiment_Y', 'validation_Y', 'text_raw', 'h_sentence', 'train_sentiment', 'aspect_level2', 'h_aspect_encoding', 'h_target', 'sentiment_X', 'validation_X', 'p_aspect', 'h_sentiment', 'p_target', 'h_aspect', 'p_aspect_encoding', 'p_sentiment', 'vocab_size', 'sentence', 'p_sentence', 'lable_encoding', 'target', 'aspect', 'sentiment_Tar'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=b['embedding_matrix_sentiment']\n",
    "TARGET=b['sentiment_Tar']\n",
    "SENTENCE=b['sentiment_X']\n",
    "SENTIMENT=b['sentiment_Y']\n",
    "vocab_size=b['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET=np.tile(TARGET,11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_context=a['aug_sentence']\n",
    "#input_target=a['target']\n",
    "\n",
    "label_encoding=a['lable_encoding']\n",
    "\n",
    "embedding_matrix=a['embedding_matrix']\n",
    "\n",
    "sentiment= a['train_sentiment']\n",
    "aspect=a['aug_aspect']\n",
    "\n",
    "text_raw= a['text_raw']\n",
    "test_raw=a['test_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5655"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_sentence=a['h_sentence']\n",
    "h_aspect_encoding=a['h_aspect_encoding']\n",
    "h_sentiment= a['h_sentiment']\n",
    "\n",
    "\n",
    "p_sentence=a['p_sentence']\n",
    "p_aspect_encoding=a['p_aspect_encoding']\n",
    "p_sentiment= a['p_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_aspect_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y=[]\n",
    "for i in h_aspect_encoding:\n",
    "    y.append(list(i))\n",
    "for j in p_aspect_encoding:\n",
    "    y.append(list(j)) \n",
    "print(len(y))\n",
    "y=np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_X=a['validation_X']\n",
    "validation_Y=np.array(a['validation_Y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_aspect=a['input_aspect']\n",
    "vocab_size=a['vocab_size']\n",
    "#print(len(input_aspect[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_aspect=np.tile(input_aspect,11)\n",
    "print(len(input_aspect[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_mse_1(y_true,y_pred):\n",
    "     return K.mean(K.square(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_mse(y_true,y_pred):\n",
    "     return np.mean(np.square(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs for  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fb64eb5e759b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'input_target' is not defined"
     ]
    }
   ],
   "source": [
    "input_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ee016aa33f9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainX\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minput_context\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_target\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_target' is not defined"
     ]
    }
   ],
   "source": [
    "trainX= [input_context, input_target]\n",
    "trainY= sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=a['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_error(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_AT(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag, em_dim):\n",
    "    \n",
    "    \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    input_target = Input(shape=(max_length,),name='Target')\n",
    "  \n",
    "   \n",
    "    #print(input_target.shape)\n",
    "    #print(input_context.shape)\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "    v_target = embedding(input_target)\n",
    "    \n",
    "    \n",
    "    #print(context.shape)\n",
    "    print(\"Target\" ,v_target.shape)\n",
    "\n",
    "    \n",
    "    #concat = concatenate(inputs =[target,context])\n",
    "    #print(concat.shape)\n",
    "    #inputs = Dropout(dropout)(concat)\n",
    "    #print(inputs.shape)   \n",
    "    \n",
    "    \n",
    "    H = LSTM (lstm_out, recurrent_dropout=dropout,return_sequences=True)(context)\n",
    "    H = LSTM (lstm_out, recurrent_dropout=dropout,return_sequences=True)(H)\n",
    "    H_all, H_last , _ = LSTM (lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"AT\")(H)\n",
    "\n",
    "    #print(H_all.shape , \"H_all before reduction\")\n",
    "    H_all_1 =GlobalAveragePooling1D()(H_all)\n",
    "    #print(H_all.shape , \"H_all after reduction\")\n",
    "\n",
    "    #Target = GlobalAveragePooling1D()(v_target)\n",
    "    #print(Target.shape , \"Target after reduction\")\n",
    "    \n",
    "    concat = concatenate(inputs =[v_target,H_all])\n",
    "    print(\"Concat\",concat.shape)\n",
    "    \n",
    "    r=AttentionWithContext(name='Attention')([H_all,concat])\n",
    "    \n",
    "    #print(\"h_ n \", H_last.shape)\n",
    "    \n",
    "\n",
    "    out=FinalSentenceRepresentation(name='Final')([r , H_all_1])\n",
    "    \n",
    "    #out=Dense(int((2*lstm_out+1)/2),activation='relu')(out)\n",
    "            \n",
    "    out= Dense(1, activation='sigmoid',  kernel_regularizer=regularizers.l2(0.01))(out)\n",
    "\n",
    "    #print(out.shape)\n",
    "    \n",
    "    AT_model= Model(inputs=[input_context,input_target],outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    AT_model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = [sk_mse_1])\n",
    "    \n",
    "    \n",
    "    print(AT_model.summary())\n",
    "    \n",
    "    return AT_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target (?, 11, 300)\n",
      "Concat (?, 11, 600)\n",
      "M1 (?, 11, 600)\n",
      "M (?, 11, 1)\n",
      "alpha (?, 11, 1)\n",
      "r (?, 300)\n",
      "H_last (?, 300)\n",
      "m1  (?, 300)\n",
      "m2.shape (?, 300)\n",
      "(?, 300) h_final\n",
      "OUT (?, 1)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Context (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 11, 300)      997200      Context[0][0]                    \n",
      "                                                                 Target[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 11, 300)      721200      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 11, 300)      721200      lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Target (InputLayer)             (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "AT (LSTM)                       [(None, 11, 300), (N 721200      lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 11, 600)      0           embedding_4[1][0]                \n",
      "                                                                 AT[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "Attention (AttentionWithContext (300, 1)             360600      AT[0][0]                         \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           AT[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "Final (FinalSentenceRepresentat (None, 1)            180301      Attention[0][0]                  \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            2           Final[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 3,701,703\n",
      "Trainable params: 2,704,503\n",
      "Non-trainable params: 997,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "AT_model = Model_AT(learning_rate=0.00069,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 8s 7ms/step - loss: 0.6996 - sk_mse: 0.0409 - val_loss: 0.7112 - val_sk_mse: 0.0483\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6912 - sk_mse: 0.0369 - val_loss: 0.7094 - val_sk_mse: 0.0474\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 5s 5ms/step - loss: 0.6824 - sk_mse: 0.0326 - val_loss: 0.7064 - val_sk_mse: 0.0461\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 5s 5ms/step - loss: 0.6774 - sk_mse: 0.0303 - val_loss: 0.7036 - val_sk_mse: 0.0447\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6728 - sk_mse: 0.0282 - val_loss: 0.7023 - val_sk_mse: 0.0443\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6683 - sk_mse: 0.0262 - val_loss: 0.7059 - val_sk_mse: 0.0460\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6678 - sk_mse: 0.0260 - val_loss: 0.7162 - val_sk_mse: 0.0508\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6640 - sk_mse: 0.0243 - val_loss: 0.7296 - val_sk_mse: 0.0571\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6632 - sk_mse: 0.0241 - val_loss: 0.7093 - val_sk_mse: 0.0478\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6587 - sk_mse: 0.0220 - val_loss: 0.7165 - val_sk_mse: 0.0513\n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6548 - sk_mse: 0.0202 - val_loss: 0.7084 - val_sk_mse: 0.0476\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6561 - sk_mse: 0.0210 - val_loss: 0.7152 - val_sk_mse: 0.0509\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6519 - sk_mse: 0.0191 - val_loss: 0.7489 - val_sk_mse: 0.0662\n",
      "Epoch 14/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6548 - sk_mse: 0.0206 - val_loss: 0.7360 - val_sk_mse: 0.0602\n",
      "Epoch 15/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6485 - sk_mse: 0.0176 - val_loss: 0.7202 - val_sk_mse: 0.0534\n",
      "Epoch 16/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6461 - sk_mse: 0.0165 - val_loss: 0.7276 - val_sk_mse: 0.0569\n",
      "Epoch 17/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6430 - sk_mse: 0.0151 - val_loss: 0.7125 - val_sk_mse: 0.0498\n",
      "Epoch 18/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6430 - sk_mse: 0.0152 - val_loss: 0.7244 - val_sk_mse: 0.0554\n",
      "Epoch 19/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6424 - sk_mse: 0.0150 - val_loss: 0.7253 - val_sk_mse: 0.0557\n",
      "Epoch 20/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6395 - sk_mse: 0.0137 - val_loss: 0.7184 - val_sk_mse: 0.0527\n",
      "Epoch 21/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6418 - sk_mse: 0.0148 - val_loss: 0.7333 - val_sk_mse: 0.0596\n",
      "Epoch 22/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6421 - sk_mse: 0.0150 - val_loss: 0.7227 - val_sk_mse: 0.0546\n",
      "Epoch 23/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6410 - sk_mse: 0.0145 - val_loss: 0.7318 - val_sk_mse: 0.0593\n",
      "Epoch 24/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6362 - sk_mse: 0.0123 - val_loss: 0.7199 - val_sk_mse: 0.0536\n",
      "Epoch 25/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6340 - sk_mse: 0.0113 - val_loss: 0.7249 - val_sk_mse: 0.0560\n",
      "Epoch 26/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6319 - sk_mse: 0.0103 - val_loss: 0.7262 - val_sk_mse: 0.0565\n",
      "Epoch 27/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6321 - sk_mse: 0.0104 - val_loss: 0.7338 - val_sk_mse: 0.0602\n",
      "Epoch 28/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6338 - sk_mse: 0.0112 - val_loss: 0.7234 - val_sk_mse: 0.0553\n",
      "Epoch 29/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6332 - sk_mse: 0.0110 - val_loss: 0.7324 - val_sk_mse: 0.0595\n",
      "Epoch 30/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6340 - sk_mse: 0.0114 - val_loss: 0.7229 - val_sk_mse: 0.0552\n",
      "Epoch 31/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6306 - sk_mse: 0.0097 - val_loss: 0.7161 - val_sk_mse: 0.0520\n",
      "Epoch 32/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6293 - sk_mse: 0.0091 - val_loss: 0.7417 - val_sk_mse: 0.0640\n",
      "Epoch 33/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6294 - sk_mse: 0.0092 - val_loss: 0.7175 - val_sk_mse: 0.0526\n",
      "Epoch 34/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6294 - sk_mse: 0.0092 - val_loss: 0.7096 - val_sk_mse: 0.0491\n",
      "Epoch 35/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6297 - sk_mse: 0.0094 - val_loss: 0.7211 - val_sk_mse: 0.0545\n",
      "Epoch 36/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6282 - sk_mse: 0.0087 - val_loss: 0.7252 - val_sk_mse: 0.0562\n",
      "Epoch 37/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6288 - sk_mse: 0.0090 - val_loss: 0.7207 - val_sk_mse: 0.0541\n",
      "Epoch 38/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6289 - sk_mse: 0.0090 - val_loss: 0.7127 - val_sk_mse: 0.0503\n",
      "Epoch 39/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6278 - sk_mse: 0.0085 - val_loss: 0.7136 - val_sk_mse: 0.0507\n",
      "Epoch 40/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6254 - sk_mse: 0.0073 - val_loss: 0.7137 - val_sk_mse: 0.0509\n",
      "Epoch 41/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6251 - sk_mse: 0.0072 - val_loss: 0.7128 - val_sk_mse: 0.0503\n",
      "Epoch 42/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6253 - sk_mse: 0.0073 - val_loss: 0.7098 - val_sk_mse: 0.0490\n",
      "Epoch 43/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6264 - sk_mse: 0.0078 - val_loss: 0.7185 - val_sk_mse: 0.0532\n",
      "Epoch 44/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6267 - sk_mse: 0.0079 - val_loss: 0.7173 - val_sk_mse: 0.0524\n",
      "Epoch 45/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6262 - sk_mse: 0.0077 - val_loss: 0.7344 - val_sk_mse: 0.0606\n",
      "Epoch 46/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6257 - sk_mse: 0.0075 - val_loss: 0.7176 - val_sk_mse: 0.0525\n",
      "Epoch 47/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6249 - sk_mse: 0.0071 - val_loss: 0.7157 - val_sk_mse: 0.0518\n",
      "Epoch 48/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6240 - sk_mse: 0.0067 - val_loss: 0.7149 - val_sk_mse: 0.0514\n",
      "Epoch 49/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6245 - sk_mse: 0.0069 - val_loss: 0.7103 - val_sk_mse: 0.0493\n",
      "Epoch 50/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6273 - sk_mse: 0.0083 - val_loss: 0.7219 - val_sk_mse: 0.0549\n"
     ]
    }
   ],
   "source": [
    "AT_model = AT_model.fit(x=trainX,y=trainY, epochs=50,batch_size=64,validation_data=(testX,testY))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Attention to our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_1(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag,em_dim):\n",
    "     \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "        \n",
    "    a = Bidirectional(LSTM(300, return_sequences=True,recurrent_dropout=dropout))(context)\n",
    "    \n",
    "    print(a.shape)\n",
    "\n",
    "    alpha = Attention()(a)\n",
    "    \n",
    "    x=Dense(300,activation='relu')(alpha)\n",
    "        \n",
    "    out=Dense(1,activation='linear')(x)\n",
    "    \n",
    "    model= Model(inputs=input_context ,outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'mse', optimizer=optimizer,metrics = ['mae','mse', r2_error, 'cosine'])\n",
    "    \n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 600)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Context (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 11, 300)           997200    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 11, 600)           1442400   \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 600)               611       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 2,620,812\n",
      "Trainable params: 1,623,612\n",
      "Non-trainable params: 997,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "Our_model = define_model_1(learning_rate=0.0001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class EarlyStopByMSE(keras.callbacks.Callback):\n",
    "    def __init__(self, verbose = 0,mode='min',patience=5,monitor='val_loss',baseline=None):\n",
    "        super(keras.callbacks.Callback, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.patience = 5\n",
    "        self.baseline = baseline\n",
    "        self.monitor=monitor\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Allow instances to be re-used\n",
    "        self.wait = 0\n",
    "        if self.baseline is not None:\n",
    "            self.best = self.baseline\n",
    "        else:\n",
    "            self.best = -np.Inf if self.monitor_op == np.less else np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch,logs=None):\n",
    "        \n",
    "        predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        target =np.asarray(self.validation_data[1])\n",
    "        #print(target)\n",
    "        current = sk_mse(target, predict)\n",
    "        #print(current)\n",
    "        \n",
    "        if np.mean(current) < np.mean(self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "        \n",
    "        \n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0 and self.verbose > 0:\n",
    "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1173/1173 [==============================] - 24s 20ms/step - loss: 0.0859 - mean_absolute_error: 0.2363 - mean_squared_error: 0.0859 - r2_error: -2.4101 - cosine_proximity: -0.9761\n",
      "Epoch 2/100\n",
      "1173/1173 [==============================] - 27s 23ms/step - loss: 0.0450 - mean_absolute_error: 0.1781 - mean_squared_error: 0.0450 - r2_error: -0.4952 - cosine_proximity: -1.0000\n",
      "Epoch 3/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0364 - mean_absolute_error: 0.1591 - mean_squared_error: 0.0364 - r2_error: -0.2755 - cosine_proximity: -1.0000\n",
      "Epoch 4/100\n",
      "1173/1173 [==============================] - 21s 18ms/step - loss: 0.0318 - mean_absolute_error: 0.1475 - mean_squared_error: 0.0318 - r2_error: -0.0351 - cosine_proximity: -1.0000\n",
      "Epoch 5/100\n",
      "1173/1173 [==============================] - 21s 18ms/step - loss: 0.0293 - mean_absolute_error: 0.1393 - mean_squared_error: 0.0293 - r2_error: 0.0785 - cosine_proximity: -1.0000 6s - loss: 0.0295 - mean_absolute_error: 0.1396 - mean_squared_error: 0.0\n",
      "Epoch 6/100\n",
      "1173/1173 [==============================] - 21s 18ms/step - loss: 0.0269 - mean_absolute_error: 0.1331 - mean_squared_error: 0.0269 - r2_error: 0.0903 - cosine_proximity: -1.0000\n",
      "Epoch 7/100\n",
      "1173/1173 [==============================] - 21s 18ms/step - loss: 0.0244 - mean_absolute_error: 0.1273 - mean_squared_error: 0.0244 - r2_error: 0.2469 - cosine_proximity: -1.0000\n",
      "Epoch 8/100\n",
      "1173/1173 [==============================] - 22s 18ms/step - loss: 0.0226 - mean_absolute_error: 0.1213 - mean_squared_error: 0.0226 - r2_error: 0.2311 - cosine_proximity: -1.0000\n",
      "Epoch 9/100\n",
      "1173/1173 [==============================] - 21s 18ms/step - loss: 0.0210 - mean_absolute_error: 0.1172 - mean_squared_error: 0.0210 - r2_error: 0.3421 - cosine_proximity: -1.0000\n",
      "Epoch 10/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0194 - mean_absolute_error: 0.1118 - mean_squared_error: 0.0194 - r2_error: 0.3854 - cosine_proximity: -1.0000\n",
      "Epoch 11/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0182 - mean_absolute_error: 0.1081 - mean_squared_error: 0.0182 - r2_error: 0.3559 - cosine_proximity: -1.0000\n",
      "Epoch 12/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0155 - mean_absolute_error: 0.0999 - mean_squared_error: 0.0155 - r2_error: 0.4954 - cosine_proximity: -1.0000\n",
      "Epoch 13/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0153 - mean_absolute_error: 0.0981 - mean_squared_error: 0.0153 - r2_error: 0.4864 - cosine_proximity: -1.0000\n",
      "Epoch 14/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0144 - mean_absolute_error: 0.0958 - mean_squared_error: 0.0144 - r2_error: 0.4870 - cosine_proximity: -1.0000\n",
      "Epoch 15/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0131 - mean_absolute_error: 0.0916 - mean_squared_error: 0.0131 - r2_error: 0.5134 - cosine_proximity: -1.0000\n",
      "Epoch 16/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0115 - mean_absolute_error: 0.0838 - mean_squared_error: 0.0115 - r2_error: 0.5727 - cosine_proximity: -1.0000\n",
      "Epoch 17/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0109 - mean_absolute_error: 0.0820 - mean_squared_error: 0.0109 - r2_error: 0.6263 - cosine_proximity: -1.0000\n",
      "Epoch 18/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0097 - mean_absolute_error: 0.0773 - mean_squared_error: 0.0097 - r2_error: 0.6886 - cosine_proximity: -1.0000\n",
      "Epoch 19/100\n",
      "1173/1173 [==============================] - 19s 17ms/step - loss: 0.0086 - mean_absolute_error: 0.0710 - mean_squared_error: 0.0086 - r2_error: 0.7214 - cosine_proximity: -1.0000\n",
      "Epoch 20/100\n",
      "1173/1173 [==============================] - 19s 16ms/step - loss: 0.0083 - mean_absolute_error: 0.0705 - mean_squared_error: 0.0083 - r2_error: 0.6912 - cosine_proximity: -1.0000\n",
      "Epoch 21/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0073 - mean_absolute_error: 0.0660 - mean_squared_error: 0.0073 - r2_error: 0.7675 - cosine_proximity: -1.0000\n",
      "Epoch 22/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0063 - mean_absolute_error: 0.0612 - mean_squared_error: 0.0063 - r2_error: 0.7925 - cosine_proximity: -1.0000\n",
      "Epoch 23/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0060 - mean_absolute_error: 0.0599 - mean_squared_error: 0.0060 - r2_error: 0.7969 - cosine_proximity: -0.9983\n",
      "Epoch 24/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0053 - mean_absolute_error: 0.0547 - mean_squared_error: 0.0053 - r2_error: 0.8043 - cosine_proximity: -1.0000\n",
      "Epoch 25/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0049 - mean_absolute_error: 0.0528 - mean_squared_error: 0.0049 - r2_error: 0.8327 - cosine_proximity: -1.0000\n",
      "Epoch 26/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0044 - mean_absolute_error: 0.0506 - mean_squared_error: 0.0044 - r2_error: 0.8233 - cosine_proximity: -1.0000\n",
      "Epoch 27/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0042 - mean_absolute_error: 0.0493 - mean_squared_error: 0.0042 - r2_error: 0.8644 - cosine_proximity: -1.0000\n",
      "Epoch 28/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0040 - mean_absolute_error: 0.0482 - mean_squared_error: 0.0040 - r2_error: 0.8575 - cosine_proximity: -1.0000\n",
      "Epoch 29/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0035 - mean_absolute_error: 0.0441 - mean_squared_error: 0.0035 - r2_error: 0.8826 - cosine_proximity: -1.0000\n",
      "Epoch 30/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0031 - mean_absolute_error: 0.0405 - mean_squared_error: 0.0031 - r2_error: 0.8960 - cosine_proximity: -1.0000\n",
      "Epoch 31/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0030 - mean_absolute_error: 0.0403 - mean_squared_error: 0.0030 - r2_error: 0.8872 - cosine_proximity: -1.0000\n",
      "Epoch 32/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0029 - mean_absolute_error: 0.0404 - mean_squared_error: 0.0029 - r2_error: 0.8862 - cosine_proximity: -1.0000\n",
      "Epoch 33/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0026 - mean_absolute_error: 0.0370 - mean_squared_error: 0.0026 - r2_error: 0.9188 - cosine_proximity: -1.0000\n",
      "Epoch 34/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0025 - mean_absolute_error: 0.0370 - mean_squared_error: 0.0025 - r2_error: 0.9121 - cosine_proximity: -1.0000\n",
      "Epoch 35/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0025 - mean_absolute_error: 0.0361 - mean_squared_error: 0.0025 - r2_error: 0.9187 - cosine_proximity: -1.0000\n",
      "Epoch 36/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0023 - mean_absolute_error: 0.0346 - mean_squared_error: 0.0023 - r2_error: 0.9147 - cosine_proximity: -1.0000\n",
      "Epoch 37/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0022 - mean_absolute_error: 0.0344 - mean_squared_error: 0.0022 - r2_error: 0.9222 - cosine_proximity: -1.0000\n",
      "Epoch 38/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0021 - mean_absolute_error: 0.0330 - mean_squared_error: 0.0021 - r2_error: 0.9297 - cosine_proximity: -0.9983\n",
      "Epoch 39/100\n",
      "1173/1173 [==============================] - 20s 17ms/step - loss: 0.0023 - mean_absolute_error: 0.0344 - mean_squared_error: 0.0023 - r2_error: 0.9146 - cosine_proximity: -1.0000\n",
      "Epoch 40/100\n",
      "1173/1173 [==============================] - 21s 18ms/step - loss: 0.0021 - mean_absolute_error: 0.0327 - mean_squared_error: 0.0021 - r2_error: 0.9207 - cosine_proximity: -1.0000\n",
      "Epoch 41/100\n",
      " 336/1173 [=======>......................] - ETA: 15s - loss: 0.0021 - mean_absolute_error: 0.0330 - mean_squared_error: 0.0021 - r2_error: 0.9342 - cosine_proximity: -1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-51679ae8565d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#earlystop = EarlyStopping(monitor='val_loss', mode='min',patience=5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mOur_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#earlystop = EarlyStopping(monitor='val_loss', mode='min',patience=5)\n",
    "Our_model.fit(x=input_context,y=trainY, epochs=100,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Our_model.to_json()\n",
    "with open(\"Our_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model)\n",
    "# serialize weights to HDF5\n",
    "Our_model.save_weights(\"Our_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def rescale(series,old_range,new_range):\n",
    "    m = interp1d(old_range,new_range)\n",
    "    return [float(m(x)) for x in series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post MSE: 0.14276643428105218\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_sentiment =Our_model.predict(p_sentence)\n",
    "\n",
    "pred_sentiment = rescale(pred_sentiment,[0,1],[-1,1])\n",
    "\n",
    "outputs= sk_mse(p_sentiment,pred_sentiment)\n",
    "\n",
    "print(\"Post MSE:\", outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2213655f710>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import pylab \n",
    "\n",
    "pyplot.plot(Our_model.history['mean_squared_error'],label='mse')\n",
    "#pyplot.plot(Our_model.history['mean_absolute_error'],label='mae')\n",
    "pyplot.plot(Our_model.history['r2_error'],label='r2')\n",
    "#pyplot.plot(Our_model.history['cosine_proximity'],label='cosine')\n",
    "pylab.legend(loc='lower right')\n",
    "\n",
    "\n",
    "#legend((line1, line2, line3,line4), ('mse', 'mae', 'r2','cosine'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AT-AE MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_ATAE(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag, em_dim):\n",
    "    \n",
    "    \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    input_target = Input(shape=(max_length,),name='Target')\n",
    "  \n",
    "   \n",
    "    #print(input_target.shape)\n",
    "    #print(input_context.shape)\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "    v_target = embedding(input_target)\n",
    "    \n",
    "    #print(context.shape)\n",
    "    print(\"Target\" ,v_target.shape)\n",
    "\n",
    "    \n",
    "    concat = concatenate(inputs =[v_target,context])\n",
    "    print(concat.shape)\n",
    "    inputs = Dropout(dropout)(concat)\n",
    "    print(inputs.shape)   \n",
    "    \n",
    "    H = LSTM (lstm_out, recurrent_dropout=dropout,return_sequences=True)(inputs)\n",
    "    H = LSTM (lstm_out, recurrent_dropout=dropout,return_sequences=True)(H)\n",
    "    H_all, H_last , _ = LSTM (lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"ATAE\")(H)\n",
    "\n",
    "\n",
    "    H_all_1 = GlobalAveragePooling1D()(H_all)\n",
    "    print(H_all.shape , \"H_all after reduction\")\n",
    "    \n",
    "    concat = concatenate(inputs =[v_target,H_all])\n",
    "    print(\"Concat\",concat.shape)\n",
    "    \n",
    "    r=AttentionWithContext(name='Attention')([H_all,concat])\n",
    "    \n",
    "    #print(\"h_ n \", H_last.shape)\n",
    "    \n",
    "    \n",
    "    out=FinalSentenceRepresentation(name='Final')([r , H_all_1])\n",
    "\n",
    "    out= Dense(1, activation='sigmoid',  kernel_regularizer=regularizers.l2(0.01))(out)\n",
    "\n",
    "    #print(out.shape)\n",
    "    \n",
    "    ATAE_model= Model(inputs=[input_context,input_target],outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    ATAE_model.compile(optimizer = optimizer, loss = 'mse', metrics = ['mae'])\n",
    "    \n",
    "    \n",
    "    print(ATAE_model.summary())\n",
    "    \n",
    "    return ATAE_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATAE_model = Model_ATAE(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 10s 8ms/step - loss: 0.0628 - mean_absolute_error: 0.1714 - val_loss: 0.0688 - val_mean_absolute_error: 0.1748\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0585 - mean_absolute_error: 0.1605 - val_loss: 0.0702 - val_mean_absolute_error: 0.1753\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0553 - mean_absolute_error: 0.1500 - val_loss: 0.0670 - val_mean_absolute_error: 0.1739\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0535 - mean_absolute_error: 0.1465 - val_loss: 0.0655 - val_mean_absolute_error: 0.1740\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0510 - mean_absolute_error: 0.1422 - val_loss: 0.0656 - val_mean_absolute_error: 0.1717\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0500 - mean_absolute_error: 0.1389 - val_loss: 0.0625 - val_mean_absolute_error: 0.1716\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0499 - mean_absolute_error: 0.1426 - val_loss: 0.0621 - val_mean_absolute_error: 0.1695\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0462 - mean_absolute_error: 0.1329 - val_loss: 0.0641 - val_mean_absolute_error: 0.1748\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0457 - mean_absolute_error: 0.1319 - val_loss: 0.0668 - val_mean_absolute_error: 0.1783\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0456 - mean_absolute_error: 0.1334 - val_loss: 0.0658 - val_mean_absolute_error: 0.1784\n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0441 - mean_absolute_error: 0.1314 - val_loss: 0.0645 - val_mean_absolute_error: 0.1767\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0443 - mean_absolute_error: 0.1332 - val_loss: 0.0641 - val_mean_absolute_error: 0.1756\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0421 - mean_absolute_error: 0.1258 - val_loss: 0.0607 - val_mean_absolute_error: 0.1707\n",
      "Epoch 14/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0420 - mean_absolute_error: 0.1264 - val_loss: 0.0603 - val_mean_absolute_error: 0.1706\n",
      "Epoch 15/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0413 - mean_absolute_error: 0.1260 - val_loss: 0.0619 - val_mean_absolute_error: 0.1745\n",
      "Epoch 16/50\n",
      "1173/1173 [==============================] - 6s 6ms/step - loss: 0.0397 - mean_absolute_error: 0.1233 - val_loss: 0.0604 - val_mean_absolute_error: 0.1703\n",
      "Epoch 17/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0379 - mean_absolute_error: 0.1174 - val_loss: 0.0634 - val_mean_absolute_error: 0.1768\n",
      "Epoch 18/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0376 - mean_absolute_error: 0.1177 - val_loss: 0.0604 - val_mean_absolute_error: 0.1754\n",
      "Epoch 19/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0364 - mean_absolute_error: 0.1177 - val_loss: 0.0662 - val_mean_absolute_error: 0.1845\n",
      "Epoch 20/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0362 - mean_absolute_error: 0.1145 - val_loss: 0.0598 - val_mean_absolute_error: 0.1751\n",
      "Epoch 21/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0334 - mean_absolute_error: 0.1101 - val_loss: 0.0607 - val_mean_absolute_error: 0.1773\n",
      "Epoch 22/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0350 - mean_absolute_error: 0.1153 - val_loss: 0.0604 - val_mean_absolute_error: 0.1775\n",
      "Epoch 23/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0345 - mean_absolute_error: 0.1139 - val_loss: 0.0576 - val_mean_absolute_error: 0.1731\n",
      "Epoch 24/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0336 - mean_absolute_error: 0.1109 - val_loss: 0.0611 - val_mean_absolute_error: 0.1790\n",
      "Epoch 25/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0320 - mean_absolute_error: 0.1094 - val_loss: 0.0596 - val_mean_absolute_error: 0.1774\n",
      "Epoch 26/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0315 - mean_absolute_error: 0.1060 - val_loss: 0.0593 - val_mean_absolute_error: 0.1763\n",
      "Epoch 27/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0310 - mean_absolute_error: 0.1058 - val_loss: 0.0590 - val_mean_absolute_error: 0.1787\n",
      "Epoch 28/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0300 - mean_absolute_error: 0.1064 - val_loss: 0.0635 - val_mean_absolute_error: 0.1843\n",
      "Epoch 29/50\n",
      "1173/1173 [==============================] - 6s 6ms/step - loss: 0.0306 - mean_absolute_error: 0.1085 - val_loss: 0.0627 - val_mean_absolute_error: 0.1831\n",
      "Epoch 30/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0282 - mean_absolute_error: 0.1020 - val_loss: 0.0587 - val_mean_absolute_error: 0.1801\n",
      "Epoch 31/50\n",
      "1173/1173 [==============================] - 6s 6ms/step - loss: 0.0272 - mean_absolute_error: 0.0998 - val_loss: 0.0612 - val_mean_absolute_error: 0.1820\n",
      "Epoch 32/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0278 - mean_absolute_error: 0.1000 - val_loss: 0.0642 - val_mean_absolute_error: 0.1866\n",
      "Epoch 33/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0276 - mean_absolute_error: 0.1018 - val_loss: 0.0588 - val_mean_absolute_error: 0.1781\n",
      "Epoch 34/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0264 - mean_absolute_error: 0.0998 - val_loss: 0.0575 - val_mean_absolute_error: 0.1778\n",
      "Epoch 35/50\n",
      "1173/1173 [==============================] - 6s 6ms/step - loss: 0.0261 - mean_absolute_error: 0.0987 - val_loss: 0.0597 - val_mean_absolute_error: 0.1829\n",
      "Epoch 36/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0250 - mean_absolute_error: 0.0962 - val_loss: 0.0626 - val_mean_absolute_error: 0.1875\n",
      "Epoch 37/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0249 - mean_absolute_error: 0.0964 - val_loss: 0.0611 - val_mean_absolute_error: 0.1826\n",
      "Epoch 38/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0257 - mean_absolute_error: 0.0994 - val_loss: 0.0609 - val_mean_absolute_error: 0.1867\n",
      "Epoch 39/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0229 - mean_absolute_error: 0.0924 - val_loss: 0.0603 - val_mean_absolute_error: 0.1851\n",
      "Epoch 40/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0228 - mean_absolute_error: 0.0903 - val_loss: 0.0602 - val_mean_absolute_error: 0.1838\n",
      "Epoch 41/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0214 - mean_absolute_error: 0.0876 - val_loss: 0.0623 - val_mean_absolute_error: 0.1883\n",
      "Epoch 42/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0208 - mean_absolute_error: 0.0870 - val_loss: 0.0578 - val_mean_absolute_error: 0.1827\n",
      "Epoch 43/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0208 - mean_absolute_error: 0.0885 - val_loss: 0.0575 - val_mean_absolute_error: 0.1820\n",
      "Epoch 44/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0209 - mean_absolute_error: 0.0882 - val_loss: 0.0592 - val_mean_absolute_error: 0.1848\n",
      "Epoch 45/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0187 - mean_absolute_error: 0.0834 - val_loss: 0.0564 - val_mean_absolute_error: 0.1809\n",
      "Epoch 46/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0196 - mean_absolute_error: 0.0854 - val_loss: 0.0575 - val_mean_absolute_error: 0.1787\n",
      "Epoch 47/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0199 - mean_absolute_error: 0.0874 - val_loss: 0.0570 - val_mean_absolute_error: 0.1816\n",
      "Epoch 48/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0193 - mean_absolute_error: 0.0856 - val_loss: 0.0572 - val_mean_absolute_error: 0.1831\n",
      "Epoch 49/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0191 - mean_absolute_error: 0.0861 - val_loss: 0.0616 - val_mean_absolute_error: 0.1911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0192 - mean_absolute_error: 0.0865 - val_loss: 0.0578 - val_mean_absolute_error: 0.1840\n"
     ]
    }
   ],
   "source": [
    "ATAE_model = ATAE_model.fit(x=trainX,y=trainY, epochs=50,batch_size=64,validation_data=(testX,testY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'>\n",
    "\n",
    "\n",
    "loss: 0.0192 - mean_absolute_error: 0.0865\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATAE_model.save('ATAE_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Model_AE(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag, em_dim):\n",
    "     \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    input_target = Input(shape=(max_length,),name='Target')\n",
    "  \n",
    "   \n",
    "    #print(input_target.shape)\n",
    "    #print(input_context.shape)\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "    v_target = embedding(input_target)\n",
    "    \n",
    "    #print(context.shape)\n",
    "    print(\"Target\" ,v_target.shape)\n",
    "\n",
    "    \n",
    "    concat = concatenate(inputs =[v_target,context])\n",
    "    print(concat.shape)\n",
    "    inputs = Dropout(dropout)(concat)\n",
    "    print(inputs.shape)   \n",
    "    \n",
    "    H_last = Bidirectional(LSTM (lstm_out, recurrent_dropout=dropout,name=\"ATAE\"))(inputs)\n",
    "    \n",
    "    x=Dense(300,activation='relu')(H_last)\n",
    "        \n",
    "    out=Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    #print(out.shape)\n",
    "    \n",
    "    AE_model= Model(inputs=[input_context,input_target],outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    AE_model.compile(optimizer = optimizer, loss = 'mse', metrics = ['mae'])\n",
    "    \n",
    "    \n",
    "    print(AE_model.summary())\n",
    "    \n",
    "    return AE_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target (?, 11, 300)\n",
      "(?, 11, 600)\n",
      "(?, 11, 600)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Target (InputLayer)             (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Context (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 11, 300)      997200      Context[0][0]                    \n",
      "                                                                 Target[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 11, 600)      0           embedding_6[1][0]                \n",
      "                                                                 embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 11, 600)      0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 600)          2162400     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 300)          180300      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            301         dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,340,201\n",
      "Trainable params: 2,343,001\n",
      "Non-trainable params: 997,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AE_model = Model_AE(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.0418 - mean_absolute_error: 0.1735 - val_loss: 0.0438 - val_mean_absolute_error: 0.1739\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0346 - mean_absolute_error: 0.1582 - val_loss: 0.0459 - val_mean_absolute_error: 0.1762\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0302 - mean_absolute_error: 0.1425 - val_loss: 0.0450 - val_mean_absolute_error: 0.1754\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0276 - mean_absolute_error: 0.1354 - val_loss: 0.0459 - val_mean_absolute_error: 0.1768\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0260 - mean_absolute_error: 0.1301 - val_loss: 0.0465 - val_mean_absolute_error: 0.1752\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0240 - mean_absolute_error: 0.1233 - val_loss: 0.0469 - val_mean_absolute_error: 0.1759\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0221 - mean_absolute_error: 0.1176 - val_loss: 0.0480 - val_mean_absolute_error: 0.1797\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0209 - mean_absolute_error: 0.1126 - val_loss: 0.0494 - val_mean_absolute_error: 0.1793\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0191 - mean_absolute_error: 0.1087 - val_loss: 0.0469 - val_mean_absolute_error: 0.1777\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0193 - mean_absolute_error: 0.1076 - val_loss: 0.0497 - val_mean_absolute_error: 0.1787\n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0177 - mean_absolute_error: 0.1021 - val_loss: 0.0508 - val_mean_absolute_error: 0.1831\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0166 - mean_absolute_error: 0.0977 - val_loss: 0.0472 - val_mean_absolute_error: 0.1780\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0156 - mean_absolute_error: 0.0955 - val_loss: 0.0482 - val_mean_absolute_error: 0.1784\n",
      "Epoch 14/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0150 - mean_absolute_error: 0.0931 - val_loss: 0.0488 - val_mean_absolute_error: 0.1795\n",
      "Epoch 15/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0143 - mean_absolute_error: 0.0910 - val_loss: 0.0524 - val_mean_absolute_error: 0.1878\n",
      "Epoch 16/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0133 - mean_absolute_error: 0.0872 - val_loss: 0.0478 - val_mean_absolute_error: 0.1762\n",
      "Epoch 17/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0130 - mean_absolute_error: 0.0839 - val_loss: 0.0500 - val_mean_absolute_error: 0.1805\n",
      "Epoch 18/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0128 - mean_absolute_error: 0.0840 - val_loss: 0.0486 - val_mean_absolute_error: 0.1804\n",
      "Epoch 19/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0103 - mean_absolute_error: 0.0769 - val_loss: 0.0516 - val_mean_absolute_error: 0.1855\n",
      "Epoch 20/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0109 - mean_absolute_error: 0.0788 - val_loss: 0.0510 - val_mean_absolute_error: 0.1848\n",
      "Epoch 21/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0101 - mean_absolute_error: 0.0756 - val_loss: 0.0528 - val_mean_absolute_error: 0.1844\n",
      "Epoch 22/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0109 - mean_absolute_error: 0.0774 - val_loss: 0.0482 - val_mean_absolute_error: 0.1791\n",
      "Epoch 23/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0734 - val_loss: 0.0479 - val_mean_absolute_error: 0.1775\n",
      "Epoch 24/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0087 - mean_absolute_error: 0.0705 - val_loss: 0.0504 - val_mean_absolute_error: 0.1846\n",
      "Epoch 25/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0086 - mean_absolute_error: 0.0696 - val_loss: 0.0519 - val_mean_absolute_error: 0.1855\n",
      "Epoch 26/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0077 - mean_absolute_error: 0.0663 - val_loss: 0.0526 - val_mean_absolute_error: 0.1846\n",
      "Epoch 27/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0079 - mean_absolute_error: 0.0660 - val_loss: 0.0532 - val_mean_absolute_error: 0.1856\n",
      "Epoch 28/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0078 - mean_absolute_error: 0.0667 - val_loss: 0.0526 - val_mean_absolute_error: 0.1854\n",
      "Epoch 29/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0076 - mean_absolute_error: 0.0649 - val_loss: 0.0500 - val_mean_absolute_error: 0.1821\n",
      "Epoch 30/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0059 - mean_absolute_error: 0.0587 - val_loss: 0.0550 - val_mean_absolute_error: 0.1904\n",
      "Epoch 31/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0064 - mean_absolute_error: 0.0595 - val_loss: 0.0545 - val_mean_absolute_error: 0.1877\n",
      "Epoch 32/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0068 - mean_absolute_error: 0.0606 - val_loss: 0.0543 - val_mean_absolute_error: 0.1878\n",
      "Epoch 33/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0065 - mean_absolute_error: 0.0604 - val_loss: 0.0551 - val_mean_absolute_error: 0.1905\n",
      "Epoch 34/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0056 - mean_absolute_error: 0.0549 - val_loss: 0.0536 - val_mean_absolute_error: 0.1864\n",
      "Epoch 35/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0053 - mean_absolute_error: 0.0551 - val_loss: 0.0540 - val_mean_absolute_error: 0.1867\n",
      "Epoch 36/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0063 - mean_absolute_error: 0.0581 - val_loss: 0.0529 - val_mean_absolute_error: 0.1858\n",
      "Epoch 37/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0057 - mean_absolute_error: 0.0563 - val_loss: 0.0567 - val_mean_absolute_error: 0.1912\n",
      "Epoch 38/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0054 - mean_absolute_error: 0.0550 - val_loss: 0.0535 - val_mean_absolute_error: 0.1863\n",
      "Epoch 39/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0046 - mean_absolute_error: 0.0511 - val_loss: 0.0561 - val_mean_absolute_error: 0.1900\n",
      "Epoch 40/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0047 - mean_absolute_error: 0.0517 - val_loss: 0.0562 - val_mean_absolute_error: 0.1901\n",
      "Epoch 41/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0046 - mean_absolute_error: 0.0524 - val_loss: 0.0562 - val_mean_absolute_error: 0.1925\n",
      "Epoch 42/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0044 - mean_absolute_error: 0.0506 - val_loss: 0.0543 - val_mean_absolute_error: 0.1918\n",
      "Epoch 43/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0044 - mean_absolute_error: 0.0493 - val_loss: 0.0545 - val_mean_absolute_error: 0.1898\n",
      "Epoch 44/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0044 - mean_absolute_error: 0.0491 - val_loss: 0.0564 - val_mean_absolute_error: 0.1928\n",
      "Epoch 45/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0039 - mean_absolute_error: 0.0474 - val_loss: 0.0539 - val_mean_absolute_error: 0.1882\n",
      "Epoch 46/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0035 - mean_absolute_error: 0.0454 - val_loss: 0.0561 - val_mean_absolute_error: 0.1919\n",
      "Epoch 47/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0040 - mean_absolute_error: 0.0472 - val_loss: 0.0557 - val_mean_absolute_error: 0.1905\n",
      "Epoch 48/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0038 - mean_absolute_error: 0.0462 - val_loss: 0.0532 - val_mean_absolute_error: 0.1856\n",
      "Epoch 49/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0039 - mean_absolute_error: 0.0464 - val_loss: 0.0530 - val_mean_absolute_error: 0.1849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.0042 - mean_absolute_error: 0.0473 - val_loss: 0.0564 - val_mean_absolute_error: 0.1911\n"
     ]
    }
   ],
   "source": [
    "AE_model = AE_model.fit(x=trainX,y=trainY, epochs=50,batch_size=64,validation_data=(testX,testY))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_model.save('AE_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# IAN with multi target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"tanh\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(h, avg,shape):\n",
    "    \n",
    "    avg = RepeatVector(shape)(avg)\n",
    "    #print(\"avg\",avg.shape)\n",
    "    \n",
    "    concat = concatenator([h, avg])\n",
    "    #print(\"concat\",concat.shape)\n",
    "    \n",
    "    e = densor(concat)\n",
    "    #print(\"e\",e.shape)\n",
    "\n",
    "    alphas = activator(e)\n",
    "    #print(\"alphas\",alphas.shape)\n",
    "\n",
    "    context = dotor([alphas, h])\n",
    "    #print(\"context\",context.shape)\n",
    "    \n",
    "    return K.sum(context, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Model_IAN(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag, em_dim):\n",
    "    \n",
    "    input_context = Input(shape=(max_length,),name='Context')\n",
    "    input_target = Input(shape=(5,),name='Target')\n",
    "    \n",
    "    print(\"Target \",input_target.shape)\n",
    "    print(\"Context\" ,input_context.shape)\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    embedding_T=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=5,trainable = em_trainable_flag)\n",
    "\n",
    "    context=embedding(input_context)\n",
    "    target= embedding_T(input_target)\n",
    "    print(\"Target_embedding\",target.shape)\n",
    "    print(\"Context_embedding\", context.shape)\n",
    " \n",
    "    H_c , _ , _ = LSTM( lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"LSTM_C\")(context)\n",
    "    H_t , _ , _ = LSTM( lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"LSTM_T\")(target)\n",
    "\n",
    "    print(\"hc\", H_c.shape)\n",
    "    print(\"ht\",H_t.shape)\n",
    "    \n",
    "    c_avg = GlobalAveragePooling1D(name='POOL_C')(H_c)  \n",
    "    t_avg = GlobalAveragePooling1D(name='POOL_T')(H_t)\n",
    "    \n",
    "    print(\"C_AVG\", c_avg.shape)\n",
    "    print(\"t_avg\",t_avg.shape)\n",
    "    \n",
    "\n",
    "    c_r = Lambda(lambda x: one_step_attention(x[0],x[1],11))([H_c,t_avg])\n",
    "    t_r = Lambda(lambda x: one_step_attention(x[0],x[1],5))([H_t,c_avg])\n",
    "\n",
    "    \n",
    "    print(\"c_r\",c_r.shape)\n",
    "    print(\"t_r\",t_r.shape)\n",
    "   \n",
    "    d = concatenate(inputs=[c_r , t_r])\n",
    "    \n",
    "    print(\"d\",d.shape)\n",
    "    \n",
    "    out= Dense(300, activation='relu')(d)\n",
    "\n",
    "    \n",
    "    out= Dense(1, activation='sigmoid',W_regularizer=regularizers.l2(0.001))(out)\n",
    "\n",
    "\n",
    "    print(\"out\", out.shape)\n",
    "    \n",
    "    IAN_model= Model(inputs=[input_context,input_target],outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    IAN_model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    print( IAN_model.summary())\n",
    "    \n",
    "    return  IAN_model\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target  (?, 5)\n",
      "Context (?, 11)\n",
      "Target_embedding (?, 5, 300)\n",
      "Context_embedding (?, 11, 300)\n",
      "hc (?, ?, 300)\n",
      "ht (?, ?, 300)\n",
      "C_AVG (?, 300)\n",
      "t_avg (?, 300)\n",
      "c_r (?, 300)\n",
      "t_r (?, 300)\n",
      "d (?, 600)\n",
      "out (?, 1)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Context (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Target (InputLayer)             (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 11, 300)      997200      Context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 5, 300)       997200      Target[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_C (LSTM)                   [(None, 11, 300), (N 721200      embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_T (LSTM)                   [(None, 5, 300), (No 721200      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "POOL_T (GlobalAveragePooling1D) (None, 300)          0           LSTM_T[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "POOL_C (GlobalAveragePooling1D) (None, 300)          0           LSTM_C[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_4 (RepeatVector)  (None, 11, 300)      0           POOL_T[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_5 (RepeatVector)  (None, 5, 300)       0           POOL_C[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     multiple             0           LSTM_C[0][0]                     \n",
      "                                                                 repeat_vector_4[0][0]            \n",
      "                                                                 LSTM_T[0][0]                     \n",
      "                                                                 repeat_vector_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                multiple             601         concatenate_3[3][0]              \n",
      "                                                                 concatenate_3[4][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  multiple             0           dense_13[3][0]                   \n",
      "                                                                 dense_13[4][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 300)       0           attention_weights[3][0]          \n",
      "                                                                 LSTM_C[0][0]                     \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 LSTM_T[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 300)          0           dot_1[3][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 300)          0           dot_1[4][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 600)          0           lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 300)          180300      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            301         dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,618,002\n",
      "Trainable params: 1,623,602\n",
      "Non-trainable params: 1,994,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:57: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_regularizer=<keras.reg..., activation=\"sigmoid\")`\n"
     ]
    }
   ],
   "source": [
    "IAN_model=Model_IAN(learning_rate=0.01,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_IAN= [input_context,input_t_IAN]\n",
    "testX_IAN=[v_sentence,v_target_IAN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 10s 8ms/step - loss: 0.2205 - mean_absolute_error: 0.4191 - val_loss: 0.2868 - val_mean_absolute_error: 0.4898\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2341 - mean_absolute_error: 0.4391 - val_loss: 0.2861 - val_mean_absolute_error: 0.4898\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2335 - mean_absolute_error: 0.4391 - val_loss: 0.2857 - val_mean_absolute_error: 0.4898\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2332 - mean_absolute_error: 0.4391 - val_loss: 0.2854 - val_mean_absolute_error: 0.4898\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2330 - mean_absolute_error: 0.4391 - val_loss: 0.2853 - val_mean_absolute_error: 0.4898\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2328 - mean_absolute_error: 0.4391 - val_loss: 0.2852 - val_mean_absolute_error: 0.4898\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2327 - mean_absolute_error: 0.4391 - val_loss: 0.2851 - val_mean_absolute_error: 0.4898\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2327 - mean_absolute_error: 0.4391 - val_loss: 0.2850 - val_mean_absolute_error: 0.4898\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2326 - mean_absolute_error: 0.4391 - val_loss: 0.2850 - val_mean_absolute_error: 0.4898\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2326 - mean_absolute_error: 0.4391 - val_loss: 0.2850 - val_mean_absolute_error: 0.4898\n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2326 - mean_absolute_error: 0.4391 - val_loss: 0.2850 - val_mean_absolute_error: 0.4898\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2326 - mean_absolute_error: 0.4391 - val_loss: 0.2850 - val_mean_absolute_error: 0.4898\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2326 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 14/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2326 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 15/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 16/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 17/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 18/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 19/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 20/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 21/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2422 - mean_absolute_error: 0.4481 - val_loss: 0.2872 - val_mean_absolute_error: 0.4898\n",
      "Epoch 22/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2354 - mean_absolute_error: 0.4391 - val_loss: 0.2877 - val_mean_absolute_error: 0.4898\n",
      "Epoch 23/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2350 - mean_absolute_error: 0.4391 - val_loss: 0.2869 - val_mean_absolute_error: 0.4898\n",
      "Epoch 24/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2342 - mean_absolute_error: 0.4391 - val_loss: 0.2862 - val_mean_absolute_error: 0.4898\n",
      "Epoch 25/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2336 - mean_absolute_error: 0.4391 - val_loss: 0.2858 - val_mean_absolute_error: 0.4898\n",
      "Epoch 26/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2332 - mean_absolute_error: 0.4391 - val_loss: 0.2855 - val_mean_absolute_error: 0.4898\n",
      "Epoch 27/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2330 - mean_absolute_error: 0.4391 - val_loss: 0.2853 - val_mean_absolute_error: 0.4898\n",
      "Epoch 28/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2328 - mean_absolute_error: 0.4391 - val_loss: 0.2851 - val_mean_absolute_error: 0.4898\n",
      "Epoch 29/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2327 - mean_absolute_error: 0.4391 - val_loss: 0.2851 - val_mean_absolute_error: 0.4898\n",
      "Epoch 30/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2327 - mean_absolute_error: 0.4391 - val_loss: 0.2850 - val_mean_absolute_error: 0.4898\n",
      "Epoch 31/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2326 - mean_absolute_error: 0.4391 - val_loss: 0.2850 - val_mean_absolute_error: 0.4898\n",
      "Epoch 32/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2326 - mean_absolute_error: 0.4391 - val_loss: 0.2850 - val_mean_absolute_error: 0.4898\n",
      "Epoch 33/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2326 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 34/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 35/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 36/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 37/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 38/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 39/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 40/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 41/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 42/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 43/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 44/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2325 - mean_absolute_error: 0.4391 - val_loss: 0.2849 - val_mean_absolute_error: 0.4898\n",
      "Epoch 45/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2246 - mean_absolute_error: 0.4295 - val_loss: 0.2856 - val_mean_absolute_error: 0.4898\n",
      "Epoch 46/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2370 - mean_absolute_error: 0.4391 - val_loss: 0.2927 - val_mean_absolute_error: 0.4898\n",
      "Epoch 47/50\n",
      "1173/1173 [==============================] - 5s 4ms/step - loss: 0.2413 - mean_absolute_error: 0.4391 - val_loss: 0.2943 - val_mean_absolute_error: 0.4898\n",
      "Epoch 48/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2420 - mean_absolute_error: 0.4391 - val_loss: 0.2945 - val_mean_absolute_error: 0.4898\n",
      "Epoch 49/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2421 - mean_absolute_error: 0.4391 - val_loss: 0.2945 - val_mean_absolute_error: 0.4898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.2421 - mean_absolute_error: 0.4391 - val_loss: 0.2945 - val_mean_absolute_error: 0.4898\n"
     ]
    }
   ],
   "source": [
    "IAN_model = IAN_model.fit(x=trainX_IAN,y=trainY, epochs=50,batch_size=64,validation_data=(testX_IAN,testY))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IAN_model.save('IAN_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'>\n",
    "After 50 epochs :\n",
    "loss: 0.2421 - mean_absolute_error: 0.4391 - val_loss: 0.2945 - val_mean_absolute_error: 0.4898\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAN with One word target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Model_IAN_1(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag, em_dim):\n",
    "    \n",
    "    input_context = Input(shape=(max_length,),name='Context')\n",
    "    input_target = Input(shape=(11,),name='Target')\n",
    "    \n",
    "    print(\"Target \",input_target.shape)\n",
    "    print(\"Context\" ,input_context.shape)\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "\n",
    "    context=embedding(input_context)\n",
    "    target= embedding(input_target)\n",
    "    print(\"Target_embedding\",target.shape)\n",
    "    print(\"Context_embedding\", context.shape)\n",
    " \n",
    "    H_c , _ , _ = LSTM( lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"LSTM_C\")(context)\n",
    "    H_t , _ , _ = LSTM( lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"LSTM_T\")(target)\n",
    "\n",
    "    print(\"hc\", H_c.shape)\n",
    "    print(\"ht\",H_t.shape)\n",
    "    \n",
    "    c_avg = GlobalAveragePooling1D(name='POOL_C')(H_c)  \n",
    "    t_avg = GlobalAveragePooling1D(name='POOL_T')(H_t)\n",
    "    \n",
    "    print(\"C_AVG\", c_avg.shape)\n",
    "    print(\"t_avg\",t_avg.shape)\n",
    "    \n",
    "\n",
    "    c_r = Lambda(lambda x: one_step_attention(x[0],x[1],11))([H_c,t_avg])\n",
    "    t_r = Lambda(lambda x: one_step_attention(x[0],x[1],11))([H_t,c_avg])\n",
    "    \n",
    "    print(\"c_r\",c_r.shape)\n",
    "    print(\"t_r\",t_r.shape)\n",
    "   \n",
    "    d = concatenate(inputs=[c_r , t_r])\n",
    "    \n",
    "    print(\"d\",d.shape)\n",
    "    \n",
    "    out= Dense(300, activation='relu')(d)\n",
    "\n",
    "    \n",
    "    out= Dense(1, activation='sigmoid',W_regularizer=regularizers.l2(0.001))(out)\n",
    "\n",
    "\n",
    "    print(\"out\", out.shape)\n",
    "    \n",
    "    IAN_model= Model(inputs=[input_context,input_target],outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=0.001)\n",
    "    \n",
    "    IAN_model.compile(loss='mse', optimizer=optimizer, metrics=['cosine'])\n",
    "    \n",
    "    print( IAN_model.summary())\n",
    "    \n",
    "    return  IAN_model\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target  (?, 11)\n",
      "Context (?, 11)\n",
      "Target_embedding (?, 11, 300)\n",
      "Context_embedding (?, 11, 300)\n",
      "hc (?, ?, 300)\n",
      "ht (?, ?, 300)\n",
      "C_AVG (?, 300)\n",
      "t_avg (?, 300)\n",
      "c_r (?, 300)\n",
      "t_r (?, 300)\n",
      "d (?, 600)\n",
      "out (?, 1)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Context (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Target (InputLayer)             (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 11, 300)      1696500     Context[0][0]                    \n",
      "                                                                 Target[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_C (LSTM)                   [(None, 11, 300), (N 721200      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_T (LSTM)                   [(None, 11, 300), (N 721200      embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "POOL_T (GlobalAveragePooling1D) (None, 300)          0           LSTM_T[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "POOL_C (GlobalAveragePooling1D) (None, 300)          0           LSTM_C[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 300)          0           LSTM_C[0][0]                     \n",
      "                                                                 POOL_T[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 300)          0           LSTM_T[0][0]                     \n",
      "                                                                 POOL_C[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 600)          0           lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 300)          180300      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            301         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,319,501\n",
      "Trainable params: 1,623,001\n",
      "Non-trainable params: 1,696,500\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_regularizer=<keras.reg...)`\n"
     ]
    }
   ],
   "source": [
    "max_length=11\n",
    "IAN_model_1=Model_IAN_1(learning_rate=0.01,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4039 samples, validate on 93 samples\n",
      "Epoch 1/100\n",
      "4039/4039 [==============================] - 61s 15ms/step - loss: 0.0379 - cosine_proximity: -0.9993 - val_loss: 0.6268 - val_cosine_proximity: -0.0538\n",
      "Epoch 2/100\n",
      "4039/4039 [==============================] - 49s 12ms/step - loss: 0.0233 - cosine_proximity: -0.9993 - val_loss: 0.5789 - val_cosine_proximity: -0.0538\n",
      "Epoch 3/100\n",
      "4039/4039 [==============================] - 49s 12ms/step - loss: 0.0205 - cosine_proximity: -0.9993 - val_loss: 0.5697 - val_cosine_proximity: -0.0538 - loss: 0.0205 - cosine_proximity: -0.\n",
      "Epoch 4/100\n",
      "4039/4039 [==============================] - 48s 12ms/step - loss: 0.0179 - cosine_proximity: -0.9993 - val_loss: 0.5601 - val_cosine_proximity: -0.0538\n",
      "Epoch 5/100\n",
      "4039/4039 [==============================] - 47s 12ms/step - loss: 0.0161 - cosine_proximity: -0.9993 - val_loss: 0.5897 - val_cosine_proximity: -0.0538\n",
      "Epoch 6/100\n",
      "4039/4039 [==============================] - 52s 13ms/step - loss: 0.0146 - cosine_proximity: -0.9993 - val_loss: 0.6382 - val_cosine_proximity: -0.0538\n",
      "Epoch 7/100\n",
      "4039/4039 [==============================] - 53s 13ms/step - loss: 0.0134 - cosine_proximity: -0.9993 - val_loss: 0.5738 - val_cosine_proximity: -0.0538\n",
      "Epoch 8/100\n",
      "4039/4039 [==============================] - 50s 12ms/step - loss: 0.0119 - cosine_proximity: -0.9993 - val_loss: 0.5790 - val_cosine_proximity: -0.0538\n",
      "Epoch 9/100\n",
      "4039/4039 [==============================] - 54s 13ms/step - loss: 0.0111 - cosine_proximity: -0.9993 - val_loss: 0.5483 - val_cosine_proximity: -0.0538\n",
      "Epoch 10/100\n",
      "4039/4039 [==============================] - 50s 12ms/step - loss: 0.0099 - cosine_proximity: -0.9993 - val_loss: 0.5909 - val_cosine_proximity: -0.0538\n",
      "Epoch 11/100\n",
      "4039/4039 [==============================] - 49s 12ms/step - loss: 0.0090 - cosine_proximity: -0.9993 - val_loss: 0.5928 - val_cosine_proximity: -0.0538\n",
      "Epoch 12/100\n",
      "4039/4039 [==============================] - 47s 12ms/step - loss: 0.0076 - cosine_proximity: -0.9993 - val_loss: 0.5383 - val_cosine_proximity: -0.0538\n",
      "Epoch 13/100\n",
      "4039/4039 [==============================] - 49s 12ms/step - loss: 0.0068 - cosine_proximity: -0.9993 - val_loss: 0.5820 - val_cosine_proximity: -0.0538\n",
      "Epoch 14/100\n",
      "2144/4039 [==============>...............] - ETA: 25s - loss: 0.0065 - cosine_proximity: -1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4f9ebdf45f91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mIAN_model_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIAN_model_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSENTENCE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSENTIMENT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlystop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'h_sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'h_target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'h_sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "IAN_model_1 = IAN_model_1.fit(x=[SENTENCE,TARGET],y=SENTIMENT, epochs=100,batch_size=8,callbacks=[earlystop],validation_data=([a['h_sentence'],a['h_target']],a['h_sentiment']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def rescale(series,old_range,new_range):\n",
    "    m = interp1d(old_range,new_range)\n",
    "    return [float(m(x)) for x in series]\n",
    "\n",
    "def sk_mse(y_true,y_pred):\n",
    "     return np.mean(np.square(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentiment = IAN_model_1.predict([a['h_sentence'],a['h_target']])\n",
    "pred_sentiment = rescale(pred_sentiment,[0,1],[-1,1])\n",
    "outputs= sk_mse(a['h_sentiment'],pred_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2906313786623694"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'>\n",
    "After 50 epochs:\n",
    " loss: 0.0048 - mean_absolute_error: 0.0405 - val_loss: 0.0547 - val_mean_absolute_error: 0.1867\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=11\n",
    "def cnn_4(dropout,learning_rate,em,em_dim,em_trainable_flag,num_filters=200):\n",
    "    \n",
    "        # channel 1\n",
    "        inputs1 = Input(shape=(length,))\n",
    "        embedding1 = Embedding(vocab_size, 300)(inputs1)\n",
    "        \n",
    "       # q = Bidirectional(LSTM(327,return_sequences=True,recurrent_dropout=dropout))(embedding1)\n",
    "       # q = Bidirectional(LSTM(327,return_sequences=True,recurrent_dropout=dropout))(q)\n",
    "        \n",
    "        conv1 = Conv1D(filters=num_filters, kernel_size=1, activation='relu')(embedding1)\n",
    "        drop1 = Dropout(0.2)(conv1)\n",
    "        pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "        flat1 = Flatten()(pool1)\n",
    "        #q1 = Bidirectional(LSTM(300,return_sequences=True,recurrent_dropout=dropout))(pool1)\n",
    "\n",
    "        \n",
    "        # channel 2\n",
    "        #inputs2 = Input(shape=(length,))\n",
    "        embedding2 = Embedding(vocab_size, 300)(inputs1)\n",
    "        conv2 = Conv1D(filters=num_filters, kernel_size=2, activation='relu')(embedding2)\n",
    "        drop2 = Dropout(0.2)(conv2)\n",
    "        pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "        flat2 = Flatten()(pool2)\n",
    "        #q2 = Bidirectional(LSTM(300,return_sequences=True,recurrent_dropout=dropout))(pool2)\n",
    "\n",
    "        # channel 3\n",
    "        #inputs3 = Input(shape=(length,))\n",
    "        embedding3 = Embedding(vocab_size, 300)(inputs1)\n",
    "        conv3 = Conv1D(filters=num_filters, kernel_size=3, activation='relu')(embedding3)\n",
    "        drop3 = Dropout(0.2)(conv3)\n",
    "        pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "        flat3 = Flatten()(pool3)\n",
    "        #q3 = Bidirectional(LSTM(300,return_sequences=False,recurrent_dropout=dropout))(pool3)\n",
    "\n",
    "        \n",
    "        # merge\n",
    "        merged = concatenate([flat1, flat2, flat3])\n",
    "        # interpretation\n",
    "        \n",
    "        dense1 = Dense(327, activation='relu')(merged)\n",
    "        outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "        model = Model(inputs=inputs1, outputs=outputs)\n",
    "        # compile\n",
    "        optimizer=Adam(lr=learning_rate)\n",
    "        model.compile(loss='mse', optimizer=optimizer, metrics=['cosine'])\n",
    "        # summarize\n",
    "        #print(model.summary())\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_model_4 = cnn_4(dropout=0.2,\n",
    "                     learning_rate=0.001,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4039 samples, validate on 93 samples\n",
      "Epoch 1/100\n",
      "4039/4039 [==============================] - 54s 13ms/step - loss: 0.0283 - cosine_proximity: -0.9993 - val_loss: 0.5202 - val_cosine_proximity: -0.0538\n",
      "Epoch 2/100\n",
      "2296/4039 [================>.............] - ETA: 22s - loss: 0.0096 - cosine_proximity: -0.9996"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-ec06dd068339>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcnn_model_4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSENTENCE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSENTIMENT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlystop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'h_sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'h_sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn_model_4.fit(x=SENTENCE,y=SENTIMENT, epochs=100,batch_size=8,callbacks=[earlystop],validation_data=(a['h_sentence'],a['h_sentiment']))\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentiment = IAN_model_1.predict([a['h_sentence'],a['h_target']])\n",
    "pred_sentiment = rescale(pred_sentiment,[0,1],[-1,1])\n",
    "outputs= sk_mse(a['h_sentiment'],pred_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model2(dropout,learning_rate,em,em_dim,lstm_out, n_hidden_layer,em_trainable_flag,n_filters=150):\n",
    "   \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "    \n",
    "    c=Dropout(0.5)(context)\n",
    "    \n",
    "    c=Conv1D(n_filters,kernel_size=3,activation='relu')(context)\n",
    "    \n",
    "    m = Bidirectional(LSTM(150,return_sequences=False,recurrent_dropout=dropout))(c)\n",
    "    \n",
    "    #a=  Attention()(m)\n",
    "                   \n",
    "    a=Dense(400,activation='relu')(m)\n",
    "        \n",
    "    out=Dense(1,activation='sigmoid')(a)\n",
    "    \n",
    "    model= Model(inputs=input_context ,outputs=out)\n",
    "    \n",
    "\n",
    "    optimizer = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Context (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "embedding_37 (Embedding)     (None, 11, 300)           997200    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 9, 150)            135150    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 300)               361200    \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 400)               120400    \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,614,351\n",
      "Trainable params: 617,151\n",
      "Non-trainable params: 997,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn_model_1 = define_model2(dropout=0.2,\n",
    "                     learning_rate=0.01,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.0415 - mean_absolute_error: 0.1799 - val_loss: 0.0465 - val_mean_absolute_error: 0.1793\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0399 - mean_absolute_error: 0.1728 - val_loss: 0.0461 - val_mean_absolute_error: 0.1790\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0393 - mean_absolute_error: 0.1707 - val_loss: 0.0464 - val_mean_absolute_error: 0.1785\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0386 - mean_absolute_error: 0.1709 - val_loss: 0.0465 - val_mean_absolute_error: 0.1780\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0376 - mean_absolute_error: 0.1662 - val_loss: 0.0461 - val_mean_absolute_error: 0.1774\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0363 - mean_absolute_error: 0.1651 - val_loss: 0.0475 - val_mean_absolute_error: 0.1775\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0340 - mean_absolute_error: 0.1590 - val_loss: 0.0473 - val_mean_absolute_error: 0.1771\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0308 - mean_absolute_error: 0.1495 - val_loss: 0.0490 - val_mean_absolute_error: 0.1786\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0267 - mean_absolute_error: 0.1357 - val_loss: 0.0490 - val_mean_absolute_error: 0.1785\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0229 - mean_absolute_error: 0.1215 - val_loss: 0.0518 - val_mean_absolute_error: 0.1825\n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0192 - mean_absolute_error: 0.1090 - val_loss: 0.0509 - val_mean_absolute_error: 0.1812\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0168 - mean_absolute_error: 0.1014 - val_loss: 0.0496 - val_mean_absolute_error: 0.1787\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0142 - mean_absolute_error: 0.0920 - val_loss: 0.0503 - val_mean_absolute_error: 0.1789\n",
      "Epoch 14/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0128 - mean_absolute_error: 0.0867 - val_loss: 0.0489 - val_mean_absolute_error: 0.1771\n",
      "Epoch 15/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0100 - mean_absolute_error: 0.0765 - val_loss: 0.0509 - val_mean_absolute_error: 0.1795\n",
      "Epoch 16/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0090 - mean_absolute_error: 0.0733 - val_loss: 0.0517 - val_mean_absolute_error: 0.1810\n",
      "Epoch 17/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0076 - mean_absolute_error: 0.0667 - val_loss: 0.0516 - val_mean_absolute_error: 0.1801\n",
      "Epoch 18/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0066 - mean_absolute_error: 0.0623 - val_loss: 0.0502 - val_mean_absolute_error: 0.1774\n",
      "Epoch 19/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0057 - mean_absolute_error: 0.0576 - val_loss: 0.0514 - val_mean_absolute_error: 0.1796\n",
      "Epoch 20/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0052 - mean_absolute_error: 0.0539 - val_loss: 0.0511 - val_mean_absolute_error: 0.1791\n",
      "Epoch 21/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0045 - mean_absolute_error: 0.0502 - val_loss: 0.0498 - val_mean_absolute_error: 0.1769\n",
      "Epoch 22/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0040 - mean_absolute_error: 0.0484 - val_loss: 0.0494 - val_mean_absolute_error: 0.1767\n",
      "Epoch 23/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0037 - mean_absolute_error: 0.0442 - val_loss: 0.0514 - val_mean_absolute_error: 0.1798\n",
      "Epoch 24/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0035 - mean_absolute_error: 0.0439 - val_loss: 0.0505 - val_mean_absolute_error: 0.1781\n",
      "Epoch 25/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0033 - mean_absolute_error: 0.0423 - val_loss: 0.0536 - val_mean_absolute_error: 0.1827\n",
      "Epoch 26/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0029 - mean_absolute_error: 0.0390 - val_loss: 0.0535 - val_mean_absolute_error: 0.1825\n",
      "Epoch 27/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0027 - mean_absolute_error: 0.0375 - val_loss: 0.0529 - val_mean_absolute_error: 0.1814\n",
      "Epoch 28/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0024 - mean_absolute_error: 0.0359 - val_loss: 0.0516 - val_mean_absolute_error: 0.1788\n",
      "Epoch 29/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0024 - mean_absolute_error: 0.0338 - val_loss: 0.0497 - val_mean_absolute_error: 0.1759\n",
      "Epoch 30/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0025 - mean_absolute_error: 0.0356 - val_loss: 0.0511 - val_mean_absolute_error: 0.1782\n",
      "Epoch 31/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0022 - mean_absolute_error: 0.0335 - val_loss: 0.0528 - val_mean_absolute_error: 0.1812\n",
      "Epoch 32/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0021 - mean_absolute_error: 0.0318 - val_loss: 0.0517 - val_mean_absolute_error: 0.1791\n",
      "Epoch 33/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0019 - mean_absolute_error: 0.0304 - val_loss: 0.0522 - val_mean_absolute_error: 0.1797\n",
      "Epoch 34/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0018 - mean_absolute_error: 0.0293 - val_loss: 0.0512 - val_mean_absolute_error: 0.1779\n",
      "Epoch 35/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0018 - mean_absolute_error: 0.0286 - val_loss: 0.0508 - val_mean_absolute_error: 0.1775\n",
      "Epoch 36/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0017 - mean_absolute_error: 0.0288 - val_loss: 0.0521 - val_mean_absolute_error: 0.1797\n",
      "Epoch 37/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0018 - mean_absolute_error: 0.0290 - val_loss: 0.0553 - val_mean_absolute_error: 0.1857\n",
      "Epoch 38/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0016 - mean_absolute_error: 0.0277 - val_loss: 0.0545 - val_mean_absolute_error: 0.1842\n",
      "Epoch 39/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0018 - mean_absolute_error: 0.0292 - val_loss: 0.0524 - val_mean_absolute_error: 0.1801\n",
      "Epoch 40/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0019 - mean_absolute_error: 0.0315 - val_loss: 0.0569 - val_mean_absolute_error: 0.1887\n",
      "Epoch 41/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0016 - mean_absolute_error: 0.0282 - val_loss: 0.0522 - val_mean_absolute_error: 0.1800\n",
      "Epoch 42/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0014 - mean_absolute_error: 0.0255 - val_loss: 0.0507 - val_mean_absolute_error: 0.1768\n",
      "Epoch 43/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0013 - mean_absolute_error: 0.0248 - val_loss: 0.0513 - val_mean_absolute_error: 0.1780\n",
      "Epoch 44/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0013 - mean_absolute_error: 0.0240 - val_loss: 0.0529 - val_mean_absolute_error: 0.1805\n",
      "Epoch 45/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0012 - mean_absolute_error: 0.0221 - val_loss: 0.0527 - val_mean_absolute_error: 0.1806\n",
      "Epoch 46/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0013 - mean_absolute_error: 0.0244 - val_loss: 0.0527 - val_mean_absolute_error: 0.1806\n",
      "Epoch 47/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0012 - mean_absolute_error: 0.0227 - val_loss: 0.0549 - val_mean_absolute_error: 0.1848\n",
      "Epoch 48/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0013 - mean_absolute_error: 0.0244 - val_loss: 0.0514 - val_mean_absolute_error: 0.1776\n",
      "Epoch 49/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0012 - mean_absolute_error: 0.0219 - val_loss: 0.0526 - val_mean_absolute_error: 0.1804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 0.0011 - mean_absolute_error: 0.0210 - val_loss: 0.0539 - val_mean_absolute_error: 0.1829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fca8bdc1d0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_1.fit(x=input_context,y=trainY, epochs=50,batch_size=64,validation_data=(v_sentence,v_sentiment))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relu and sigmoid : <font color='red'>\n",
    "After 50 epochs:\n",
    "    loss: 0.0011 - mean_absolute_error: 0.0210 - val_loss: 0.0539 - val_mean_absolute_error: 0.1829\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN +LSTM +ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model3(dropout,learning_rate,em,em_dim,lstm_out, n_hidden_layer,em_trainable_flag,n_filters=150):\n",
    "   \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "    \n",
    "    c=Dropout(0.5)(context)\n",
    "    \n",
    "    c=Conv1D(n_filters,kernel_size=3,activation='relu')(context)\n",
    "    \n",
    "    m = Bidirectional(LSTM(150,return_sequences=True,recurrent_dropout=dropout))(c)\n",
    "    \n",
    "    a=  Attention()(m)\n",
    "                   \n",
    "    a=Dense(400,activation='relu')(a)\n",
    "        \n",
    "    out=Dense(1,activation='sigmoid')(a)\n",
    "    \n",
    "    model= Model(inputs=input_context ,outputs=out)\n",
    "    \n",
    "\n",
    "    optimizer = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Context (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 11, 300)           997200    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 9, 150)            135150    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 9, 300)            361200    \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 300)               309       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               120400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,614,660\n",
      "Trainable params: 617,460\n",
      "Non-trainable params: 997,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "max_length=11\n",
    "cnn_model_3 = define_model3(dropout=0.2,\n",
    "                     learning_rate=0.01,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0416 - mean_absolute_error: 0.1813 - val_loss: 0.0459 - val_mean_absolute_error: 0.1795\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0400 - mean_absolute_error: 0.1735 - val_loss: 0.0461 - val_mean_absolute_error: 0.1793\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0396 - mean_absolute_error: 0.1728 - val_loss: 0.0461 - val_mean_absolute_error: 0.1792\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0392 - mean_absolute_error: 0.1715 - val_loss: 0.0462 - val_mean_absolute_error: 0.1791\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0385 - mean_absolute_error: 0.1701 - val_loss: 0.0461 - val_mean_absolute_error: 0.1787\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0374 - mean_absolute_error: 0.1678 - val_loss: 0.0464 - val_mean_absolute_error: 0.1781\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0357 - mean_absolute_error: 0.1625 - val_loss: 0.0459 - val_mean_absolute_error: 0.1772\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0329 - mean_absolute_error: 0.1549 - val_loss: 0.0469 - val_mean_absolute_error: 0.1774\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0288 - mean_absolute_error: 0.1425 - val_loss: 0.0466 - val_mean_absolute_error: 0.1761\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0250 - mean_absolute_error: 0.1281 - val_loss: 0.0495 - val_mean_absolute_error: 0.1799\n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0214 - mean_absolute_error: 0.1159 - val_loss: 0.0507 - val_mean_absolute_error: 0.1823\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0189 - mean_absolute_error: 0.1077 - val_loss: 0.0534 - val_mean_absolute_error: 0.1864\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0168 - mean_absolute_error: 0.1009 - val_loss: 0.0459 - val_mean_absolute_error: 0.1735\n",
      "Epoch 14/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0143 - mean_absolute_error: 0.0914 - val_loss: 0.0478 - val_mean_absolute_error: 0.1762\n",
      "Epoch 15/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0129 - mean_absolute_error: 0.0888 - val_loss: 0.0536 - val_mean_absolute_error: 0.1864\n",
      "Epoch 16/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0112 - mean_absolute_error: 0.0812 - val_loss: 0.0493 - val_mean_absolute_error: 0.1785\n",
      "Epoch 17/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0095 - mean_absolute_error: 0.0731 - val_loss: 0.0493 - val_mean_absolute_error: 0.1781\n",
      "Epoch 18/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0080 - mean_absolute_error: 0.0679 - val_loss: 0.0500 - val_mean_absolute_error: 0.1790\n",
      "Epoch 19/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0073 - mean_absolute_error: 0.0642 - val_loss: 0.0514 - val_mean_absolute_error: 0.1816\n",
      "Epoch 20/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0066 - mean_absolute_error: 0.0608 - val_loss: 0.0512 - val_mean_absolute_error: 0.1809\n",
      "Epoch 21/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0059 - mean_absolute_error: 0.0570 - val_loss: 0.0533 - val_mean_absolute_error: 0.1837\n",
      "Epoch 22/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0051 - mean_absolute_error: 0.0533 - val_loss: 0.0557 - val_mean_absolute_error: 0.1875\n",
      "Epoch 23/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0048 - mean_absolute_error: 0.0519 - val_loss: 0.0548 - val_mean_absolute_error: 0.1861\n",
      "Epoch 24/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0041 - mean_absolute_error: 0.0478 - val_loss: 0.0518 - val_mean_absolute_error: 0.1805\n",
      "Epoch 25/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0039 - mean_absolute_error: 0.0452 - val_loss: 0.0505 - val_mean_absolute_error: 0.1778\n",
      "Epoch 26/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0037 - mean_absolute_error: 0.0446 - val_loss: 0.0506 - val_mean_absolute_error: 0.1782\n",
      "Epoch 27/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0034 - mean_absolute_error: 0.0419 - val_loss: 0.0501 - val_mean_absolute_error: 0.1772\n",
      "Epoch 28/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0033 - mean_absolute_error: 0.0425 - val_loss: 0.0531 - val_mean_absolute_error: 0.1820\n",
      "Epoch 29/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0030 - mean_absolute_error: 0.0391 - val_loss: 0.0543 - val_mean_absolute_error: 0.1847\n",
      "Epoch 30/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0029 - mean_absolute_error: 0.0399 - val_loss: 0.0532 - val_mean_absolute_error: 0.1819\n",
      "Epoch 31/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0025 - mean_absolute_error: 0.0352 - val_loss: 0.0545 - val_mean_absolute_error: 0.1842\n",
      "Epoch 32/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0026 - mean_absolute_error: 0.0362 - val_loss: 0.0555 - val_mean_absolute_error: 0.1865\n",
      "Epoch 33/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0023 - mean_absolute_error: 0.0345 - val_loss: 0.0552 - val_mean_absolute_error: 0.1860\n",
      "Epoch 34/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0022 - mean_absolute_error: 0.0331 - val_loss: 0.0547 - val_mean_absolute_error: 0.1852\n",
      "Epoch 35/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0021 - mean_absolute_error: 0.0326 - val_loss: 0.0527 - val_mean_absolute_error: 0.1810\n",
      "Epoch 36/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0019 - mean_absolute_error: 0.0303 - val_loss: 0.0521 - val_mean_absolute_error: 0.1795\n",
      "Epoch 37/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0019 - mean_absolute_error: 0.0307 - val_loss: 0.0527 - val_mean_absolute_error: 0.1808\n",
      "Epoch 38/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0018 - mean_absolute_error: 0.0297 - val_loss: 0.0536 - val_mean_absolute_error: 0.1823\n",
      "Epoch 39/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0018 - mean_absolute_error: 0.0284 - val_loss: 0.0527 - val_mean_absolute_error: 0.1805\n",
      "Epoch 40/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0017 - mean_absolute_error: 0.0280 - val_loss: 0.0528 - val_mean_absolute_error: 0.1803\n",
      "Epoch 41/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0016 - mean_absolute_error: 0.0274 - val_loss: 0.0529 - val_mean_absolute_error: 0.1807\n",
      "Epoch 42/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0017 - mean_absolute_error: 0.0286 - val_loss: 0.0552 - val_mean_absolute_error: 0.1846\n",
      "Epoch 43/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0016 - mean_absolute_error: 0.0277 - val_loss: 0.0560 - val_mean_absolute_error: 0.1860\n",
      "Epoch 44/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0017 - mean_absolute_error: 0.0277 - val_loss: 0.0522 - val_mean_absolute_error: 0.1795\n",
      "Epoch 45/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0018 - mean_absolute_error: 0.0284 - val_loss: 0.0520 - val_mean_absolute_error: 0.1792\n",
      "Epoch 46/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0016 - mean_absolute_error: 0.0272 - val_loss: 0.0519 - val_mean_absolute_error: 0.1783\n",
      "Epoch 47/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0016 - mean_absolute_error: 0.0269 - val_loss: 0.0536 - val_mean_absolute_error: 0.1811\n",
      "Epoch 48/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0014 - mean_absolute_error: 0.0253 - val_loss: 0.0527 - val_mean_absolute_error: 0.1803\n",
      "Epoch 49/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0014 - mean_absolute_error: 0.0244 - val_loss: 0.0538 - val_mean_absolute_error: 0.1825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0013 - mean_absolute_error: 0.0234 - val_loss: 0.0536 - val_mean_absolute_error: 0.1818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25bc4e04d68>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_3.fit(x=input_context,y=trainY, epochs=50,batch_size=64,validation_data=(v_sentence,v_sentiment))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 50 epochs : loss: 0.0013 - mean_absolute_error: 0.0234 - val_loss: 0.0536 - val_mean_absolute_error: 0.1818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 CHANNEL CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model2(dropout,learning_rate,em,em_dim,em_trainable_flag,n_filters=100):\n",
    "    \n",
    "\n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    inputs1= embedding(input_context)\n",
    "    \n",
    "    \n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=3, activation='relu')(inputs1)\n",
    "    drop1 = Dropout(dropout)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    conv2 = Conv1D(filters=n_filters, kernel_size=4, activation='relu')(inputs1)\n",
    "    drop2 = Dropout(dropout)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    \n",
    "    conv3 = Conv1D(filters=n_filters, kernel_size=5, activation='relu')(inputs1)\n",
    "    drop3 = Dropout(dropout)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    \n",
    "    conv4 = Conv1D(filters=n_filters, kernel_size=2, activation='relu')(inputs1)\n",
    "    drop4 = Dropout(dropout)(conv4)\n",
    "    pool4 = MaxPooling1D(pool_size=2)(drop4)\n",
    "    flat4 = Flatten()(pool4)\n",
    "    \n",
    "    conv5 = Conv1D(filters=n_filters, kernel_size=6, activation='relu')(inputs1)\n",
    "    drop5 = Dropout(dropout)(conv5)\n",
    "    pool5 = MaxPooling1D(pool_size=2)(drop5)\n",
    "    flat5 = Flatten()(pool5)\n",
    "    # merge\n",
    "    \n",
    "    merged = concatenate([flat1, flat2, flat3,flat4,flat5])\n",
    "    # interpretation\n",
    "    dense1 = Dense(400, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=input_context, outputs=outputs)\n",
    "    \n",
    "    # compile\n",
    "    \n",
    "    optimizer = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_2 = cnn_model2(dropout=0.5,\n",
    "                     learning_rate=0.001,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1173 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0441 - mean_absolute_error: 0.1794 - val_loss: 0.0472 - val_mean_absolute_error: 0.1807\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 1s 962us/step - loss: 0.0380 - mean_absolute_error: 0.1667 - val_loss: 0.0481 - val_mean_absolute_error: 0.1798\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 1s 944us/step - loss: 0.0348 - mean_absolute_error: 0.1592 - val_loss: 0.0468 - val_mean_absolute_error: 0.1783\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 1s 973us/step - loss: 0.0311 - mean_absolute_error: 0.1488 - val_loss: 0.0467 - val_mean_absolute_error: 0.1777\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 1s 942us/step - loss: 0.0281 - mean_absolute_error: 0.1408 - val_loss: 0.0468 - val_mean_absolute_error: 0.1771\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 1s 971us/step - loss: 0.0259 - mean_absolute_error: 0.1341 - val_loss: 0.0472 - val_mean_absolute_error: 0.1769\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 1s 939us/step - loss: 0.0238 - mean_absolute_error: 0.1255 - val_loss: 0.0481 - val_mean_absolute_error: 0.1765\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 1s 979us/step - loss: 0.0217 - mean_absolute_error: 0.1203 - val_loss: 0.0477 - val_mean_absolute_error: 0.1756\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 1s 940us/step - loss: 0.0195 - mean_absolute_error: 0.1128 - val_loss: 0.0487 - val_mean_absolute_error: 0.1769\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 1s 942us/step - loss: 0.0182 - mean_absolute_error: 0.1089 - val_loss: 0.0467 - val_mean_absolute_error: 0.1735\n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 1s 940us/step - loss: 0.0158 - mean_absolute_error: 0.1015 - val_loss: 0.0494 - val_mean_absolute_error: 0.1768\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 1s 928us/step - loss: 0.0145 - mean_absolute_error: 0.0956 - val_loss: 0.0497 - val_mean_absolute_error: 0.1774\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 1s 910us/step - loss: 0.0136 - mean_absolute_error: 0.0926 - val_loss: 0.0471 - val_mean_absolute_error: 0.1739\n",
      "Epoch 14/50\n",
      "1173/1173 [==============================] - 1s 921us/step - loss: 0.0130 - mean_absolute_error: 0.0905 - val_loss: 0.0488 - val_mean_absolute_error: 0.1760\n",
      "Epoch 15/50\n",
      "1173/1173 [==============================] - 1s 914us/step - loss: 0.0118 - mean_absolute_error: 0.0848 - val_loss: 0.0467 - val_mean_absolute_error: 0.1728\n",
      "Epoch 16/50\n",
      "1173/1173 [==============================] - 1s 914us/step - loss: 0.0110 - mean_absolute_error: 0.0820 - val_loss: 0.0475 - val_mean_absolute_error: 0.1740\n",
      "Epoch 17/50\n",
      "1173/1173 [==============================] - 1s 911us/step - loss: 0.0101 - mean_absolute_error: 0.0788 - val_loss: 0.0495 - val_mean_absolute_error: 0.1761\n",
      "Epoch 18/50\n",
      "1173/1173 [==============================] - 1s 899us/step - loss: 0.0099 - mean_absolute_error: 0.0765 - val_loss: 0.0486 - val_mean_absolute_error: 0.1743\n",
      "Epoch 19/50\n",
      "1173/1173 [==============================] - 1s 959us/step - loss: 0.0090 - mean_absolute_error: 0.0744 - val_loss: 0.0485 - val_mean_absolute_error: 0.1744\n",
      "Epoch 20/50\n",
      "1173/1173 [==============================] - 1s 998us/step - loss: 0.0089 - mean_absolute_error: 0.0739 - val_loss: 0.0517 - val_mean_absolute_error: 0.1788\n",
      "Epoch 21/50\n",
      "1173/1173 [==============================] - 1s 935us/step - loss: 0.0085 - mean_absolute_error: 0.0713 - val_loss: 0.0472 - val_mean_absolute_error: 0.1722\n",
      "Epoch 22/50\n",
      "1173/1173 [==============================] - 1s 917us/step - loss: 0.0077 - mean_absolute_error: 0.0677 - val_loss: 0.0476 - val_mean_absolute_error: 0.1727\n",
      "Epoch 23/50\n",
      "1173/1173 [==============================] - 1s 918us/step - loss: 0.0082 - mean_absolute_error: 0.0713 - val_loss: 0.0501 - val_mean_absolute_error: 0.1762\n",
      "Epoch 24/50\n",
      "1173/1173 [==============================] - 1s 989us/step - loss: 0.0072 - mean_absolute_error: 0.0660 - val_loss: 0.0497 - val_mean_absolute_error: 0.1755\n",
      "Epoch 25/50\n",
      "1173/1173 [==============================] - 1s 970us/step - loss: 0.0074 - mean_absolute_error: 0.0663 - val_loss: 0.0488 - val_mean_absolute_error: 0.1741\n",
      "Epoch 26/50\n",
      "1173/1173 [==============================] - 1s 994us/step - loss: 0.0069 - mean_absolute_error: 0.0661 - val_loss: 0.0477 - val_mean_absolute_error: 0.1716\n",
      "Epoch 27/50\n",
      "1173/1173 [==============================] - 1s 960us/step - loss: 0.0063 - mean_absolute_error: 0.0625 - val_loss: 0.0487 - val_mean_absolute_error: 0.1731\n",
      "Epoch 28/50\n",
      "1173/1173 [==============================] - 1s 939us/step - loss: 0.0069 - mean_absolute_error: 0.0646 - val_loss: 0.0479 - val_mean_absolute_error: 0.1721\n",
      "Epoch 29/50\n",
      "1173/1173 [==============================] - 1s 983us/step - loss: 0.0063 - mean_absolute_error: 0.0617 - val_loss: 0.0485 - val_mean_absolute_error: 0.1731\n",
      "Epoch 30/50\n",
      "1173/1173 [==============================] - 1s 992us/step - loss: 0.0060 - mean_absolute_error: 0.0604 - val_loss: 0.0466 - val_mean_absolute_error: 0.1695\n",
      "Epoch 31/50\n",
      "1173/1173 [==============================] - 1s 987us/step - loss: 0.0059 - mean_absolute_error: 0.0596 - val_loss: 0.0487 - val_mean_absolute_error: 0.1728\n",
      "Epoch 32/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0055 - mean_absolute_error: 0.0572 - val_loss: 0.0491 - val_mean_absolute_error: 0.1733\n",
      "Epoch 33/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0051 - mean_absolute_error: 0.0557 - val_loss: 0.0491 - val_mean_absolute_error: 0.1735\n",
      "Epoch 34/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0053 - mean_absolute_error: 0.0564 - val_loss: 0.0482 - val_mean_absolute_error: 0.1720\n",
      "Epoch 35/50\n",
      "1173/1173 [==============================] - 1s 941us/step - loss: 0.0052 - mean_absolute_error: 0.0552 - val_loss: 0.0479 - val_mean_absolute_error: 0.1714\n",
      "Epoch 36/50\n",
      "1173/1173 [==============================] - 1s 906us/step - loss: 0.0049 - mean_absolute_error: 0.0544 - val_loss: 0.0475 - val_mean_absolute_error: 0.1706\n",
      "Epoch 37/50\n",
      "1173/1173 [==============================] - 1s 905us/step - loss: 0.0052 - mean_absolute_error: 0.0555 - val_loss: 0.0506 - val_mean_absolute_error: 0.1759\n",
      "Epoch 38/50\n",
      "1173/1173 [==============================] - 1s 964us/step - loss: 0.0050 - mean_absolute_error: 0.0546 - val_loss: 0.0481 - val_mean_absolute_error: 0.1721\n",
      "Epoch 39/50\n",
      "1173/1173 [==============================] - 1s 972us/step - loss: 0.0050 - mean_absolute_error: 0.0547 - val_loss: 0.0513 - val_mean_absolute_error: 0.1773\n",
      "Epoch 40/50\n",
      "1173/1173 [==============================] - 1s 877us/step - loss: 0.0047 - mean_absolute_error: 0.0538 - val_loss: 0.0492 - val_mean_absolute_error: 0.1738\n",
      "Epoch 41/50\n",
      "1173/1173 [==============================] - 1s 896us/step - loss: 0.0043 - mean_absolute_error: 0.0507 - val_loss: 0.0499 - val_mean_absolute_error: 0.1750\n",
      "Epoch 42/50\n",
      "1173/1173 [==============================] - 1s 905us/step - loss: 0.0045 - mean_absolute_error: 0.0512 - val_loss: 0.0515 - val_mean_absolute_error: 0.1773\n",
      "Epoch 43/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0043 - mean_absolute_error: 0.0503 - val_loss: 0.0512 - val_mean_absolute_error: 0.1764\n",
      "Epoch 44/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0045 - mean_absolute_error: 0.0511 - val_loss: 0.0474 - val_mean_absolute_error: 0.1712\n",
      "Epoch 45/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0042 - mean_absolute_error: 0.0501 - val_loss: 0.0481 - val_mean_absolute_error: 0.1718\n",
      "Epoch 46/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0039 - mean_absolute_error: 0.0486 - val_loss: 0.0486 - val_mean_absolute_error: 0.1721\n",
      "Epoch 47/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0038 - mean_absolute_error: 0.0476 - val_loss: 0.0488 - val_mean_absolute_error: 0.1724\n",
      "Epoch 48/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0039 - mean_absolute_error: 0.0481 - val_loss: 0.0483 - val_mean_absolute_error: 0.1722\n",
      "Epoch 49/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0038 - mean_absolute_error: 0.0479 - val_loss: 0.0500 - val_mean_absolute_error: 0.1742\n",
      "Epoch 50/50\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0040 - mean_absolute_error: 0.0488 - val_loss: 0.0497 - val_mean_absolute_error: 0.1733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25bb2e249e8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_2.fit(x=input_context,y=trainY, epochs=50,batch_size=64,validation_data=(v_sentence,v_sentiment))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "After 50 epochs:\n",
    "    loss: 0.0040 - mean_absolute_error: 0.0488 - val_loss: 0.0497 - val_mean_absolute_error: 0.1733\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASPECT MODELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    list_punctuation = list(punctuation)\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,'')\n",
    "    return s.lower()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        if token.isspace():\n",
    "            continue\n",
    "        elif  token.startswith('$'):\n",
    "            lda_tokens.append('Company Name')\n",
    "        elif token.startswith('http'):\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.startswith('@'):\n",
    "            lda_tokens.append('Mention')\n",
    "        else:\n",
    "            lda_tokens.append(remove_punctuation(token))\n",
    "\n",
    "    return lda_tokens\n",
    "\n",
    "nltk.download('wordnet')\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    try:\n",
    "        tokens = tokenize(text)\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "        tokens = [token for token in tokens if token not in en_stop]\n",
    "        tokens = [get_lemma(token) for token in tokens]\n",
    "    except: \n",
    "        for i in text: \n",
    "            tokens = tokenize(i)\n",
    "            tokens = [token for token in tokens if len(token) > 2]\n",
    "            tokens = [token for token in tokens if token not in en_stop]\n",
    "            tokens = [get_lemma(token) for token in tokens]\n",
    "\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "for line in dataset1['sentence']:\n",
    "    tokens = prepare_text_for_lda(line)\n",
    "    text_data.append(tokens)\n",
    "    \n",
    "print(text_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1=pickle.load(open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Data\\train_data_augmented.dat\",\"rb\"))\n",
    "head=pickle.load(open(r\"D:\\PythonCodes\\Sentiment-Analysis\\Data\\head.dat\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>$AMZN Going thru the roof needs a sell!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>$IWM relative weakness is pretty apparent but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>$FB bot some @78.47 breakout from the consolid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>$RCON some upside today. This thing is severe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strategy</td>\n",
       "      <td>Netflix Grows Efforts To Create Loyal Customer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         aspect                                           sentence\n",
       "0  Price Action            $AMZN Going thru the roof needs a sell!\n",
       "1  Price Action  $IWM relative weakness is pretty apparent but ...\n",
       "2  Price Action  $FB bot some @78.47 breakout from the consolid...\n",
       "3  Price Action  $RCON some upside today. This thing is severe ...\n",
       "4      Strategy  Netflix Grows Efforts To Create Loyal Customer..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>h_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Financial</td>\n",
       "      <td>Companies Severn Trent expects costs hit from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>BP to slash 4000 jobs globally as oil prices drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Appointment</td>\n",
       "      <td>Tesco UK personnel director quits supermarket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reputation</td>\n",
       "      <td>HSBC money laundering report's release delayed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Price Action</td>\n",
       "      <td>Landlord Hammerson's NAV rises on increased le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         aspect                                         h_sentence\n",
       "0     Financial  Companies Severn Trent expects costs hit from ...\n",
       "1   Appointment  BP to slash 4000 jobs globally as oil prices drop\n",
       "2   Appointment      Tesco UK personnel director quits supermarket\n",
       "3    Reputation  HSBC money laundering report's release delayed...\n",
       "4  Price Action  Landlord Hammerson's NAV rises on increased le..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_pred(pred):\n",
    "    return [label_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_=get_class_from_pred(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(dataset1['sentence'])\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train,aspect_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0126984126984127"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "X_test = vectorizer.transform(head['h_sentence'])\n",
    "predictions = classifier.predict(X_test)\n",
    "f1_score(h_aspect,predictions,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\simcy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.012"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed=7\n",
    "np.random.seed(seed)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 200) \n",
    "forest = forest.fit( X_train, aspect_ )\n",
    "predictions = forest.predict(X_test)\n",
    "f1_score(h_aspect,predictions,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_classes=27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 50 epochs : loss: 0.1233 - acc: 0.9625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_1(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag,em_dim):\n",
    "     \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    input_target = Input(shape=(max_length,),name='Target')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "    v_target = embedding(input_target)\n",
    "    \n",
    "    print(\"Target\" ,v_target.shape)\n",
    "    \n",
    "    H = LSTM (lstm_out, recurrent_dropout=dropout,return_sequences=True)(context)\n",
    "    H = LSTM (lstm_out, recurrent_dropout=dropout,return_sequences=True)(H)\n",
    "    H_all, H_last , _ = LSTM (lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"AT\")(H)\n",
    "    \n",
    "    concat = concatenate(inputs =[v_target,H_all])\n",
    "    print(\"Concat\",concat.shape)\n",
    "    \n",
    "    r=AttentionWithContext(name='Attention')([H_all,concat])\n",
    "    \n",
    "    pooling= GlobalAveragePooling1D()(H_all)\n",
    "    \n",
    "    out=Final2(name='Final')([r , pooling])\n",
    "    \n",
    "    out=Dense(int((2*lstm_out+27)/2),activation='relu')(out)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(out)\n",
    "    \n",
    "    AT_model= Model(inputs=[input_context,input_target],outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    AT_model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics = ['accuracy'])\n",
    "    \n",
    "    \n",
    "    print(AT_model.summary())\n",
    "    \n",
    "    return AT_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target (?, 11, 300)\n",
      "Concat (?, 11, 600)\n",
      "M1 (?, 11, 600)\n",
      "M (?, 11, 1)\n",
      "alpha (?, 11, 1)\n",
      "r (?, 300)\n",
      "H_all (?, 300)\n",
      "m2.shape (?, 300)\n",
      "m1  (?, 300)\n",
      "(?, 300) h_final\n",
      "OUT (?, 27)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Context (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 11, 300)      997200      Context[0][0]                    \n",
      "                                                                 Target[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 11, 300)      721200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 11, 300)      721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Target (InputLayer)             (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "AT (LSTM)                       [(None, 11, 300), (N 721200      lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 11, 600)      0           embedding_1[1][0]                \n",
      "                                                                 AT[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "Attention (AttentionWithContext (300, 1)             360600      AT[0][0]                         \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 300)          0           AT[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "Final (Final2)                  (None, 27)           188127      Attention[0][0]                  \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 313)          8764        Final[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 27)           8478        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,726,769\n",
      "Trainable params: 2,729,569\n",
      "Non-trainable params: 997,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "At_model = attention_1(learning_rate=0.00069,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 13s 11ms/step - loss: 2.8720 - acc: 0.3444\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 9s 8ms/step - loss: 2.4707 - acc: 0.3725\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 2.4170 - acc: 0.3751\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 2.3137 - acc: 0.3811\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 2.2491 - acc: 0.3879\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 2.1831 - acc: 0.3981\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 2.0921 - acc: 0.4152\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.9606 - acc: 0.4450\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.8773 - acc: 0.4766\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 9s 8ms/step - loss: 1.7606 - acc: 0.4953\n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.6761 - acc: 0.5243\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.6361 - acc: 0.5354\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.5590 - acc: 0.5516\n",
      "Epoch 14/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.5350 - acc: 0.5507\n",
      "Epoch 15/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.4826 - acc: 0.5678\n",
      "Epoch 16/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.3894 - acc: 0.5797\n",
      "Epoch 17/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.2739 - acc: 0.6053\n",
      "Epoch 18/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.2007 - acc: 0.6249\n",
      "Epoch 19/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.1685 - acc: 0.6343\n",
      "Epoch 20/50\n",
      "1173/1173 [==============================] - 9s 7ms/step - loss: 1.0945 - acc: 0.6522\n",
      "Epoch 21/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 1.0652 - acc: 0.6641\n",
      "Epoch 22/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 1.0709 - acc: 0.6590\n",
      "Epoch 23/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 1.0019 - acc: 0.6726\n",
      "Epoch 24/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.9351 - acc: 0.6999\n",
      "Epoch 25/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.8333 - acc: 0.7323\n",
      "Epoch 26/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.7655 - acc: 0.7596\n",
      "Epoch 27/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.7294 - acc: 0.7698\n",
      "Epoch 28/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6660 - acc: 0.7971\n",
      "Epoch 29/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6381 - acc: 0.7954\n",
      "Epoch 30/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6232 - acc: 0.8073\n",
      "Epoch 31/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.6230 - acc: 0.8099\n",
      "Epoch 32/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.5580 - acc: 0.8107\n",
      "Epoch 33/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.5021 - acc: 0.8355\n",
      "Epoch 34/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.5139 - acc: 0.8312\n",
      "Epoch 35/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.4503 - acc: 0.8576\n",
      "Epoch 36/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.3955 - acc: 0.8721\n",
      "Epoch 37/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.3865 - acc: 0.8824\n",
      "Epoch 38/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.3252 - acc: 0.8943\n",
      "Epoch 39/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.2809 - acc: 0.9105\n",
      "Epoch 40/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.3407 - acc: 0.8917\n",
      "Epoch 41/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.2515 - acc: 0.9207\n",
      "Epoch 42/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.2035 - acc: 0.9284\n",
      "Epoch 43/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.2122 - acc: 0.9250\n",
      "Epoch 44/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.1853 - acc: 0.9361\n",
      "Epoch 45/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.1796 - acc: 0.9446\n",
      "Epoch 46/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.2093 - acc: 0.9327\n",
      "Epoch 47/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.1986 - acc: 0.9369\n",
      "Epoch 48/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.1959 - acc: 0.9344\n",
      "Epoch 49/50\n",
      "1173/1173 [==============================] - 6s 5ms/step - loss: 0.1289 - acc: 0.9582\n",
      "Epoch 50/50\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.1625 - acc: 0.9497\n"
     ]
    }
   ],
   "source": [
    "attention1 = At_model.fit(x=trainX,y=aspect, epochs=50,batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "attention_1_json = At_model.to_json()\n",
    "with open(\"attention_1.json\", \"w\") as json_file:\n",
    "    json_file.write(attention_1_json)\n",
    "# serialize weights to HDF5\n",
    "At_model.save_weights(\"attention_1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Attention + bidirectional LSTM (Adding Attention to our model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_2(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag,em_dim):\n",
    "     \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "        \n",
    "    a = Bidirectional(LSTM(300, return_sequences=True,recurrent_dropout=dropout))(context)\n",
    "    \n",
    "    print(a.shape)\n",
    "\n",
    "    alpha = Attention()(a)\n",
    "    \n",
    "    x=Dense(int((2*lstm_out+27)/2),activation='relu')(alpha)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "    \n",
    "    model= Model(inputs=input_context ,outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics = ['accuracy'])\n",
    "    \n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 600)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Context (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 11, 300)           997200    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 11, 600)           1442400   \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 600)               611       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 313)               188113    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                8478      \n",
      "=================================================================\n",
      "Total params: 2,636,802\n",
      "Trainable params: 1,639,602\n",
      "Non-trainable params: 997,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "attention_2 = attention_2(learning_rate=0.00069,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1343/1343 [==============================] - 17s 13ms/step - loss: 2.6751 - acc: 0.3276\n",
      "Epoch 2/100\n",
      "1343/1343 [==============================] - 16s 12ms/step - loss: 2.2076 - acc: 0.4021\n",
      "Epoch 3/100\n",
      "1343/1343 [==============================] - 16s 12ms/step - loss: 1.8253 - acc: 0.4795\n",
      "Epoch 4/100\n",
      "1343/1343 [==============================] - 14s 11ms/step - loss: 1.4475 - acc: 0.5733\n",
      "Epoch 5/100\n",
      "1343/1343 [==============================] - 15s 11ms/step - loss: 1.1328 - acc: 0.6597\n",
      "Epoch 6/100\n",
      "1343/1343 [==============================] - 15s 11ms/step - loss: 0.8483 - acc: 0.7401\n",
      "Epoch 7/100\n",
      "1176/1343 [=========================>....] - ETA: 1s - loss: 0.6507 - acc: 0.8010"
     ]
    }
   ],
   "source": [
    "earlystopper = EarlyStopping(monitor='acc', patience=5, verbose=1)\n",
    "\n",
    "attention_2.fit(x=input_context,y=aspect, epochs=100,batch_size=8,callbacks=[earlystopper])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "attention_2_json = attention_2.to_json()\n",
    "with open(\"attention_2.json\", \"w\") as json_file:\n",
    "    json_file.write(attention_2_json)\n",
    "# serialize weights to HDF5\n",
    "attention_2.save_weights(\"attention_2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>\n",
    " After 50 epochs: loss: 0.0416 - acc: 0.9889\n",
    "  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_5(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag,em_dim):\n",
    "     \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "        \n",
    "    a = Bidirectional(LSTM(300, return_sequences=False,recurrent_dropout=dropout))(context)\n",
    "   \n",
    "    \n",
    "    x=Dense(int((2*lstm_out+27)/2),activation='relu')(a)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "    \n",
    "    model= Model(inputs=input_context ,outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics = ['accuracy'])\n",
    "    \n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Context (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "embedding_19 (Embedding)     (None, 11, 300)           997200    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 600)               1442400   \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 313)               188113    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 27)                8478      \n",
      "=================================================================\n",
      "Total params: 2,636,191\n",
      "Trainable params: 1,638,991\n",
      "Non-trainable params: 997,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "attention_5 = attention_5(learning_rate=0.00069,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 2.8591 - acc: 0.3495\n",
      "Epoch 2/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.4591 - acc: 0.3725\n",
      "Epoch 3/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.2765 - acc: 0.3845\n",
      "Epoch 4/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.0493 - acc: 0.4237\n",
      "Epoch 5/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 1.8466 - acc: 0.4902\n",
      "Epoch 6/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 1.6626 - acc: 0.5371\n",
      "Epoch 7/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 1.5048 - acc: 0.5695\n",
      "Epoch 8/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 1.3453 - acc: 0.6257\n",
      "Epoch 9/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 1.2105 - acc: 0.6581A: 3s - loss: 1.23\n",
      "Epoch 10/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 1.1141 - acc: 0.6829\n",
      "Epoch 11/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.9607 - acc: 0.7161\n",
      "Epoch 12/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.8664 - acc: 0.7442\n",
      "Epoch 13/100\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.7804 - acc: 0.7664\n",
      "Epoch 14/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.7023 - acc: 0.7852\n",
      "Epoch 15/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.6018 - acc: 0.8252\n",
      "Epoch 16/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.5445 - acc: 0.8338\n",
      "Epoch 17/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.4924 - acc: 0.8568\n",
      "Epoch 18/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.4339 - acc: 0.8619\n",
      "Epoch 19/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.3818 - acc: 0.8934\n",
      "Epoch 20/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.3363 - acc: 0.9045\n",
      "Epoch 21/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.3668 - acc: 0.8824\n",
      "Epoch 22/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2913 - acc: 0.9173\n",
      "Epoch 23/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.2654 - acc: 0.9301\n",
      "Epoch 24/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.2564 - acc: 0.9190A: 1s - loss: 0.2674 - ac\n",
      "Epoch 25/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.2157 - acc: 0.9446\n",
      "Epoch 26/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1831 - acc: 0.9471A: 2s - loss: 0.2032 \n",
      "Epoch 27/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1865 - acc: 0.9506A: 0s - loss: 0.1876 - acc: 0.95\n",
      "Epoch 28/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1854 - acc: 0.9446A: 2s - loss: 0.18\n",
      "Epoch 29/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1626 - acc: 0.9591\n",
      "Epoch 30/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1380 - acc: 0.9625\n",
      "Epoch 31/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1216 - acc: 0.9702\n",
      "Epoch 32/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1237 - acc: 0.9685\n",
      "Epoch 33/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1131 - acc: 0.9719\n",
      "Epoch 34/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0910 - acc: 0.9795\n",
      "Epoch 35/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1002 - acc: 0.9761\n",
      "Epoch 36/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1086 - acc: 0.9642\n",
      "Epoch 37/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0854 - acc: 0.9744\n",
      "Epoch 38/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0871 - acc: 0.9736\n",
      "Epoch 39/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0728 - acc: 0.9812\n",
      "Epoch 40/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0713 - acc: 0.9804\n",
      "Epoch 41/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0652 - acc: 0.9812\n",
      "Epoch 42/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0700 - acc: 0.9812\n",
      "Epoch 43/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0587 - acc: 0.9864\n",
      "Epoch 44/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0748 - acc: 0.9804\n",
      "Epoch 45/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0732 - acc: 0.9829\n",
      "Epoch 46/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0686 - acc: 0.9778\n",
      "Epoch 47/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0691 - acc: 0.9787\n",
      "Epoch 48/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0404 - acc: 0.9932\n",
      "Epoch 49/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0602 - acc: 0.9804\n",
      "Epoch 50/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0535 - acc: 0.9829\n",
      "Epoch 51/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0605 - acc: 0.9821\n",
      "Epoch 52/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0568 - acc: 0.9847\n",
      "Epoch 53/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0474 - acc: 0.9889\n",
      "Epoch 54/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0503 - acc: 0.9872\n",
      "Epoch 55/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0418 - acc: 0.9872\n",
      "Epoch 56/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0485 - acc: 0.9855A: 2s - loss: 0.04\n",
      "Epoch 57/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0362 - acc: 0.9855\n",
      "Epoch 58/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0479 - acc: 0.9855\n",
      "Epoch 59/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0441 - acc: 0.9872\n",
      "Epoch 60/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0356 - acc: 0.9915\n",
      "Epoch 61/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0313 - acc: 0.9889\n",
      "Epoch 62/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0267 - acc: 0.9923\n",
      "Epoch 63/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0243 - acc: 0.9923\n",
      "Epoch 64/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0402 - acc: 0.9889\n",
      "Epoch 65/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0536 - acc: 0.9855\n",
      "Epoch 66/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0404 - acc: 0.9906\n",
      "Epoch 67/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0454 - acc: 0.9829\n",
      "Epoch 68/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0295 - acc: 0.9906\n",
      "Epoch 69/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0367 - acc: 0.9881\n",
      "Epoch 70/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0290 - acc: 0.9889\n",
      "Epoch 71/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0351 - acc: 0.9872\n",
      "Epoch 72/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0236 - acc: 0.9923\n",
      "Epoch 73/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0246 - acc: 0.9906\n",
      "Epoch 74/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0346 - acc: 0.9889\n",
      "Epoch 75/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0279 - acc: 0.9940\n",
      "Epoch 76/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0311 - acc: 0.9881\n",
      "Epoch 77/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0212 - acc: 0.9906\n",
      "Epoch 78/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0242 - acc: 0.9889\n",
      "Epoch 79/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0297 - acc: 0.9889\n",
      "Epoch 80/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0289 - acc: 0.9906\n",
      "Epoch 81/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0287 - acc: 0.9881\n",
      "Epoch 82/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0461 - acc: 0.9855\n",
      "Epoch 83/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0290 - acc: 0.9906\n",
      "Epoch 84/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0332 - acc: 0.9881A: 1s - loss: 0.0314 - \n",
      "Epoch 85/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0313 - acc: 0.9855\n",
      "Epoch 86/100\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.0254 - acc: 0.9932\n",
      "Epoch 87/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0257 - acc: 0.9915\n",
      "Epoch 88/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0212 - acc: 0.9932\n",
      "Epoch 89/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0390 - acc: 0.9872\n",
      "Epoch 90/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0249 - acc: 0.9915A: 2s - loss: 0.0277 - \n",
      "Epoch 91/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0212 - acc: 0.9923\n",
      "Epoch 92/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0211 - acc: 0.9915\n",
      "Epoch 93/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0189 - acc: 0.9923\n",
      "Epoch 94/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0217 - acc: 0.9898\n",
      "Epoch 95/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0189 - acc: 0.9915\n",
      "Epoch 96/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0164 - acc: 0.9915\n",
      "Epoch 97/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0140 - acc: 0.9957A: 0s - loss: 0.0121 - acc: 0\n",
      "Epoch 98/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0180 - acc: 0.9940\n",
      "Epoch 99/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0219 - acc: 0.9932\n",
      "Epoch 100/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0216 - acc: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1583cea73c8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_5.fit(x=input_context,y=aspect, epochs=100,batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "attention_5_json = attention_5.to_json()\n",
    "with open(\"attention_5.json\", \"w\") as json_file:\n",
    "    json_file.write(attention_5_json)\n",
    "# serialize weights to HDF5\n",
    "attention_5.save_weights(\"attention_5.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ATAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_ATAE(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag, em_dim):\n",
    "    \n",
    "    \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    input_target = Input(shape=(max_length,),name='Target')\n",
    "  \n",
    "   \n",
    "    #print(input_target.shape)\n",
    "    #print(input_context.shape)\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "    v_target = embedding(input_target)\n",
    "    \n",
    "    #print(context.shape)\n",
    "    print(\"Target\" ,v_target.shape)\n",
    "\n",
    "    \n",
    "    concat = concatenate(inputs =[v_target,context])\n",
    "    print(concat.shape)\n",
    "    inputs = Dropout(dropout)(concat)\n",
    "    print(inputs.shape)   \n",
    "    \n",
    "    H = LSTM (lstm_out, recurrent_dropout=dropout,return_sequences=True)(inputs)\n",
    "    H = LSTM (lstm_out, recurrent_dropout=dropout,return_sequences=True)(H)\n",
    "    H_all, H_last , _ = LSTM (lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"ATAE\")(H)\n",
    "    \n",
    "    concat = concatenate(inputs =[v_target,H_all])\n",
    "    print(\"Concat\",concat.shape)\n",
    "    \n",
    "    r=AttentionWithContext(name='Attention')([H_all,concat])\n",
    "    \n",
    "    pooling= GlobalAveragePooling1D()(H_all)\n",
    "    \n",
    "    out=Final2(name='Final')([r , pooling])\n",
    "    \n",
    "    #out=Dense(int((2*lstm_out+27)/2),activation='relu')(out)\n",
    "    \n",
    "    out= Activation('tanh')(out)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(out)\n",
    "    \n",
    "    ATAE_model= Model(inputs=[input_context,input_target],outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    ATAE_model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics = ['accuracy'])\n",
    "    \n",
    "    \n",
    "    print(ATAE_model.summary())\n",
    "    \n",
    "    return ATAE_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target (?, 11, 300)\n",
      "(?, 11, 600)\n",
      "(?, 11, 600)\n",
      "Concat (?, 11, 600)\n",
      "M1 (?, 11, 600)\n",
      "M (?, 11, 1)\n",
      "alpha (?, 11, 1)\n",
      "r (?, 300)\n",
      "H_all (?, 300)\n",
      "m2.shape (?, 300)\n",
      "m1  (?, 300)\n",
      "(?, 300) h_final\n",
      "OUT (?, 27)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Target (InputLayer)             (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Context (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 11, 300)      997200      Context[0][0]                    \n",
      "                                                                 Target[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 11, 600)      0           embedding_10[1][0]               \n",
      "                                                                 embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 11, 600)      0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 11, 300)      1081200     dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 11, 300)      721200      lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ATAE (LSTM)                     [(None, 11, 300), (N 721200      lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11, 600)      0           embedding_10[1][0]               \n",
      "                                                                 ATAE[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "Attention (AttentionWithContext (300, 1)             360600      ATAE[0][0]                       \n",
      "                                                                 concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           ATAE[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "Final (Final2)                  (None, 27)           188127      Attention[0][0]                  \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 27)           0           Final[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 27)           756         activation_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,070,283\n",
      "Trainable params: 3,073,083\n",
      "Non-trainable params: 997,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "attention_3 =Model_ATAE(learning_rate=0.00069,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 16s 14ms/step - loss: 2.8237 - acc: 0.3495\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 10s 9ms/step - loss: 2.5327 - acc: 0.3734\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 10s 9ms/step - loss: 2.4118 - acc: 0.3743\n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 2.2763 - acc: 0.4143\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 10s 9ms/step - loss: 2.1513 - acc: 0.4390\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 2.0589 - acc: 0.4689\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 10s 9ms/step - loss: 1.9484 - acc: 0.4885\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 1.8696 - acc: 0.4936\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 1.7836 - acc: 0.5303\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 1.7108 - acc: 0.5439\n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 1.6292 - acc: 0.5644\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 1.5793 - acc: 0.5763\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 1.5069 - acc: 0.6147\n",
      "Epoch 14/50\n",
      "1173/1173 [==============================] - 11s 10ms/step - loss: 1.4407 - acc: 0.6198\n",
      "Epoch 15/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 1.3669 - acc: 0.6419\n",
      "Epoch 16/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 1.3258 - acc: 0.6496\n",
      "Epoch 17/50\n",
      "1173/1173 [==============================] - 11s 10ms/step - loss: 1.2840 - acc: 0.6607\n",
      "Epoch 18/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 1.2597 - acc: 0.6709\n",
      "Epoch 19/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 1.2070 - acc: 0.6837\n",
      "Epoch 20/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 1.1359 - acc: 0.7110\n",
      "Epoch 21/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 1.1093 - acc: 0.7127\n",
      "Epoch 22/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 1.1094 - acc: 0.7144\n",
      "Epoch 23/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 1.1357 - acc: 0.6991\n",
      "Epoch 24/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 1.0885 - acc: 0.7025\n",
      "Epoch 25/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 1.0337 - acc: 0.7357\n",
      "Epoch 26/50\n",
      "1173/1173 [==============================] - 12s 11ms/step - loss: 0.9689 - acc: 0.7442\n",
      "Epoch 27/50\n",
      "1173/1173 [==============================] - 12s 11ms/step - loss: 0.9489 - acc: 0.7511\n",
      "Epoch 28/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 0.9119 - acc: 0.7442\n",
      "Epoch 29/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 0.8728 - acc: 0.7715\n",
      "Epoch 30/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 0.8883 - acc: 0.7639\n",
      "Epoch 31/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 0.8800 - acc: 0.7587\n",
      "Epoch 32/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 0.8548 - acc: 0.7656\n",
      "Epoch 33/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 0.8555 - acc: 0.7732\n",
      "Epoch 34/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 0.7679 - acc: 0.7860\n",
      "Epoch 35/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 0.7545 - acc: 0.8073\n",
      "Epoch 36/50\n",
      "1173/1173 [==============================] - 10s 9ms/step - loss: 0.7915 - acc: 0.7903\n",
      "Epoch 37/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 0.7706 - acc: 0.7971\n",
      "Epoch 38/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 0.7256 - acc: 0.7988\n",
      "Epoch 39/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 0.7096 - acc: 0.7980\n",
      "Epoch 40/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 0.7524 - acc: 0.7903\n",
      "Epoch 41/50\n",
      "1173/1173 [==============================] - 12s 10ms/step - loss: 0.7043 - acc: 0.8031\n",
      "Epoch 42/50\n",
      "1173/1173 [==============================] - 8s 7ms/step - loss: 0.6313 - acc: 0.8363\n",
      "Epoch 43/50\n",
      "1173/1173 [==============================] - 8s 7ms/step - loss: 0.6369 - acc: 0.8269\n",
      "Epoch 44/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 0.5956 - acc: 0.8397\n",
      "Epoch 45/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 0.5909 - acc: 0.8321\n",
      "Epoch 46/50\n",
      "1173/1173 [==============================] - 10s 9ms/step - loss: 0.6179 - acc: 0.8295\n",
      "Epoch 47/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 0.5739 - acc: 0.8355\n",
      "Epoch 48/50\n",
      "1173/1173 [==============================] - 10s 9ms/step - loss: 0.5637 - acc: 0.8483\n",
      "Epoch 49/50\n",
      "1173/1173 [==============================] - 10s 9ms/step - loss: 0.5534 - acc: 0.8483\n",
      "Epoch 50/50\n",
      "1173/1173 [==============================] - 11s 9ms/step - loss: 0.5447 - acc: 0.8525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229e80d15c0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_3.fit(x=trainX,y=aspect, epochs=50,batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "attention_3_json = attention_3.to_json()\n",
    "with open(\"attention_3.json\", \"w\") as json_file:\n",
    "    json_file.write(attention_3_json)\n",
    "# serialize weights to HDF5\n",
    "attention_3.save_weights(\"attention_3.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After  30 epochs : loss: 0.7511 - acc: 0.7630  relu and no tanh after final layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Model_IAN_1_aspect(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag, em_dim):\n",
    "    \n",
    "    input_context = Input(shape=(max_length,),name='Context')\n",
    "    input_target = Input(shape=(11,),name='Target')\n",
    "    \n",
    "    print(\"Target \",input_target.shape)\n",
    "    print(\"Context\" ,input_context.shape)\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "\n",
    "    context=embedding(input_context)\n",
    "    target= embedding(input_target)\n",
    "    print(\"Target_embedding\",target.shape)\n",
    "    print(\"Context_embedding\", context.shape)\n",
    " \n",
    "    H_c , _ , _ = LSTM( lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"LSTM_C\")(context)\n",
    "    H_t , _ , _ = LSTM( lstm_out, recurrent_dropout=dropout,return_state=True,return_sequences=True,name=\"LSTM_T\")(target)\n",
    "\n",
    "    print(\"hc\", H_c.shape)\n",
    "    print(\"ht\",H_t.shape)\n",
    "    \n",
    "    c_avg = GlobalAveragePooling1D(name='POOL_C')(H_c)  \n",
    "    t_avg = GlobalAveragePooling1D(name='POOL_T')(H_t)\n",
    "    \n",
    "    print(\"C_AVG\", c_avg.shape)\n",
    "    print(\"t_avg\",t_avg.shape)\n",
    "    \n",
    "\n",
    "    #c_r = Lambda(lambda x: one_step_attention(x[0],x[1],11))([H_c,t_avg])\n",
    "    #t_r = Lambda(lambda x: one_step_attention(x[0],x[1],11))([H_t,c_avg])\n",
    "    \n",
    "    c_ = RepeatVector(11)(t_avg)\n",
    "    c_ = concatenator([H_c, c_])\n",
    "    c_ = densor(c_)\n",
    "    c_ = activator(c_)\n",
    "    c = dotor([c_, H_c])    \n",
    "    c_r= Lambda(lambda x: K.sum(x, axis=1))(c)\n",
    "    \n",
    "    t_ = RepeatVector(11)(c_avg)\n",
    "    t_ = concatenator([H_t, t_])\n",
    "    t_ = densor(t_)\n",
    "    t_ = activator(t_)\n",
    "    t = dotor([t_, H_t])    \n",
    "    t_r= Lambda(lambda x: K.sum(x, axis=1))(t)\n",
    "    \n",
    "    \n",
    "    print(\"c_r\",c_r.shape)\n",
    "    print(\"t_r\",t_r.shape)\n",
    "   \n",
    "    d = concatenate(inputs=[c_r , t_r])\n",
    "    \n",
    "    print(\"d\",d.shape)\n",
    "    \n",
    "    x=Dense(int((2*lstm_out+27)/2),activation='relu')(d)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "   \n",
    "    IAN_model= Model(inputs=[input_context,input_target],outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    IAN_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    print( IAN_model.summary())\n",
    "    \n",
    "    return  IAN_model\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target  (?, 11)\n",
      "Context (?, 11)\n",
      "Target_embedding (?, 11, 300)\n",
      "Context_embedding (?, 11, 300)\n",
      "hc (?, ?, 300)\n",
      "ht (?, ?, 300)\n",
      "C_AVG (?, 300)\n",
      "t_avg (?, 300)\n",
      "c_r (?, 300)\n",
      "t_r (?, 300)\n",
      "d (?, 600)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Context (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Target (InputLayer)             (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 11, 300)      997200      Context[0][0]                    \n",
      "                                                                 Target[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_C (LSTM)                   [(None, 11, 300), (N 721200      embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_T (LSTM)                   [(None, 11, 300), (N 721200      embedding_16[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "POOL_T (GlobalAveragePooling1D) (None, 300)          0           LSTM_T[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "POOL_C (GlobalAveragePooling1D) (None, 300)          0           LSTM_C[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_8 (RepeatVector)  (None, 11, 300)      0           POOL_T[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_9 (RepeatVector)  (None, 11, 300)      0           POOL_C[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     multiple             0           LSTM_C[0][0]                     \n",
      "                                                                 repeat_vector_8[0][0]            \n",
      "                                                                 LSTM_T[0][0]                     \n",
      "                                                                 repeat_vector_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                multiple             601         concatenate_3[7][0]              \n",
      "                                                                 concatenate_3[8][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  multiple             0           dense_13[7][0]                   \n",
      "                                                                 dense_13[8][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 300)       0           attention_weights[7][0]          \n",
      "                                                                 LSTM_C[0][0]                     \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 LSTM_T[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 300)          0           dot_1[7][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 300)          0           dot_1[8][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 600)          0           lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 313)          188113      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 27)           8478        dense_18[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,636,792\n",
      "Trainable params: 1,639,592\n",
      "Non-trainable params: 997,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "IAN_model_1=Model_IAN_1_aspect(learning_rate=0.01,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 3.7940 - acc: 0.2609\n",
      "Epoch 2/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.2930 - acc: 0.3811\n",
      "Epoch 3/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 1.9692 - acc: 0.4552\n",
      "Epoch 4/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 1.6770 - acc: 0.5226\n",
      "Epoch 5/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 1.4341 - acc: 0.5669\n",
      "Epoch 6/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 1.2345 - acc: 0.6360A: 2s - loss: 1.27\n",
      "Epoch 7/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.9692 - acc: 0.7084\n",
      "Epoch 8/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.7178 - acc: 0.7843\n",
      "Epoch 9/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.6107 - acc: 0.8099\n",
      "Epoch 10/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.4526 - acc: 0.8653\n",
      "Epoch 11/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.3085 - acc: 0.9028\n",
      "Epoch 12/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.2418 - acc: 0.9258\n",
      "Epoch 13/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1863 - acc: 0.9454\n",
      "Epoch 14/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1767 - acc: 0.9463\n",
      "Epoch 15/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1920 - acc: 0.9412\n",
      "Epoch 16/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1630 - acc: 0.9557A: 2s - loss: 0.0966\n",
      "Epoch 17/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2005 - acc: 0.9395\n",
      "Epoch 18/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1474 - acc: 0.9599\n",
      "Epoch 19/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1677 - acc: 0.9565A: 0s - loss: 0.1582 - acc: 0\n",
      "Epoch 20/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1386 - acc: 0.9625\n",
      "Epoch 21/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1064 - acc: 0.9719A: 2s - loss: 0.0970\n",
      "Epoch 22/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0957 - acc: 0.9770\n",
      "Epoch 23/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1055 - acc: 0.9736\n",
      "Epoch 24/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0675 - acc: 0.9812\n",
      "Epoch 25/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0879 - acc: 0.9829\n",
      "Epoch 26/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1582 - acc: 0.9625\n",
      "Epoch 27/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2124 - acc: 0.9454A: 1s - loss: 0.1656 - acc:\n",
      "Epoch 28/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1412 - acc: 0.9642\n",
      "Epoch 29/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1473 - acc: 0.9633\n",
      "Epoch 30/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1041 - acc: 0.9744\n",
      "Epoch 31/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1991 - acc: 0.9514\n",
      "Epoch 32/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2145 - acc: 0.9463\n",
      "Epoch 33/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1593 - acc: 0.9608\n",
      "Epoch 34/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1693 - acc: 0.9582\n",
      "Epoch 35/100\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.1767 - acc: 0.9608\n",
      "Epoch 36/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1302 - acc: 0.9650\n",
      "Epoch 37/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1432 - acc: 0.9642\n",
      "Epoch 38/100\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.1428 - acc: 0.9633\n",
      "Epoch 39/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1138 - acc: 0.9710\n",
      "Epoch 40/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0975 - acc: 0.9795\n",
      "Epoch 41/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0714 - acc: 0.9804\n",
      "Epoch 42/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1482 - acc: 0.9659\n",
      "Epoch 43/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1313 - acc: 0.9650\n",
      "Epoch 44/100\n",
      "1173/1173 [==============================] - 4s 4ms/step - loss: 0.1318 - acc: 0.9616\n",
      "Epoch 45/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0834 - acc: 0.9710\n",
      "Epoch 46/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0788 - acc: 0.9812\n",
      "Epoch 47/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0809 - acc: 0.9812\n",
      "Epoch 48/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1065 - acc: 0.9761\n",
      "Epoch 49/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0456 - acc: 0.9864\n",
      "Epoch 50/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0774 - acc: 0.9812\n",
      "Epoch 51/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1647 - acc: 0.9744\n",
      "Epoch 52/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1573 - acc: 0.9710\n",
      "Epoch 53/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1588 - acc: 0.9668\n",
      "Epoch 54/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1819 - acc: 0.9565\n",
      "Epoch 55/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1258 - acc: 0.9685\n",
      "Epoch 56/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1637 - acc: 0.9676\n",
      "Epoch 57/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1083 - acc: 0.9676\n",
      "Epoch 58/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1023 - acc: 0.9685\n",
      "Epoch 59/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0911 - acc: 0.9838\n",
      "Epoch 60/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1067 - acc: 0.9761\n",
      "Epoch 61/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1031 - acc: 0.9753\n",
      "Epoch 62/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1245 - acc: 0.9676\n",
      "Epoch 63/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1283 - acc: 0.9702\n",
      "Epoch 64/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1133 - acc: 0.9736\n",
      "Epoch 65/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0989 - acc: 0.9702\n",
      "Epoch 66/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1391 - acc: 0.9702\n",
      "Epoch 67/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1110 - acc: 0.9676\n",
      "Epoch 68/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1438 - acc: 0.9693\n",
      "Epoch 69/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1994 - acc: 0.9582A: 2s - loss: 0.0944\n",
      "Epoch 70/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1943 - acc: 0.9548\n",
      "Epoch 71/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2350 - acc: 0.9514\n",
      "Epoch 72/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.2192 - acc: 0.9471\n",
      "Epoch 73/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1654 - acc: 0.9642\n",
      "Epoch 74/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1741 - acc: 0.9582\n",
      "Epoch 75/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1603 - acc: 0.9659\n",
      "Epoch 76/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1862 - acc: 0.9668\n",
      "Epoch 77/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1091 - acc: 0.9710A: 2s - loss: 0.0863 -\n",
      "Epoch 78/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1478 - acc: 0.9702\n",
      "Epoch 79/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1314 - acc: 0.9744\n",
      "Epoch 80/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1057 - acc: 0.9778\n",
      "Epoch 81/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1587 - acc: 0.9702\n",
      "Epoch 82/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2262 - acc: 0.9676\n",
      "Epoch 83/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1951 - acc: 0.9557\n",
      "Epoch 84/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2501 - acc: 0.9378A: 2s - loss: 0.1297 -\n",
      "Epoch 85/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1730 - acc: 0.9488\n",
      "Epoch 86/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1966 - acc: 0.9480\n",
      "Epoch 87/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1518 - acc: 0.9676\n",
      "Epoch 88/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.2360 - acc: 0.9591\n",
      "Epoch 89/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.2061 - acc: 0.9531\n",
      "Epoch 90/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2421 - acc: 0.9378\n",
      "Epoch 91/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1682 - acc: 0.9514\n",
      "Epoch 92/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.2879 - acc: 0.9403\n",
      "Epoch 93/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1973 - acc: 0.9429\n",
      "Epoch 94/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1922 - acc: 0.9548\n",
      "Epoch 95/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1564 - acc: 0.9548\n",
      "Epoch 96/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1238 - acc: 0.9702\n",
      "Epoch 97/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.1141 - acc: 0.9693\n",
      "Epoch 98/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.1219 - acc: 0.9702\n",
      "Epoch 99/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 0.0700 - acc: 0.9787\n",
      "Epoch 100/100\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 0.0677 - acc: 0.9753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1583ce90438>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IAN_model_1.fit(x=trainX,y=aspect, epochs=100,batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "ian_json = IAN_model_1.to_json()\n",
    "with open(\"ian.json\", \"w\") as json_file:\n",
    "    json_file.write(ian_json)\n",
    "# serialize weights to HDF5\n",
    "IAN_model_1.save_weights(\"ian.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After 50 epochs: loss: 0.0464 - acc: 0.9864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+BIDIRECTIONAL LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model2(dropout,learning_rate,em,em_dim,lstm_out, n_hidden_layer,em_trainable_flag,n_filters=150):\n",
    "   \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "    \n",
    "    c=Dropout(0.5)(context)\n",
    "    \n",
    "    c=Conv1D(n_filters,kernel_size=3,activation='relu')(context)\n",
    "    \n",
    "    m = Bidirectional(LSTM(150,return_sequences=False,recurrent_dropout=dropout))(c)\n",
    "                   \n",
    "    x=Dense(int((2*lstm_out+27)/2),activation='relu')(m)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "   \n",
    "    model= Model(inputs=input_context,outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Context (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 11, 300)           997200    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 9, 150)            135150    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 300)               361200    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 313)               94213     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 27)                8478      \n",
      "=================================================================\n",
      "Total params: 1,596,241\n",
      "Trainable params: 599,041\n",
      "Non-trainable params: 997,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn_model_1 = define_model2(dropout=0.2,\n",
    "                     learning_rate=0.01,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1173/1173 [==============================] - 3s 3ms/step - loss: 2.8320 - acc: 0.3197\n",
      "Epoch 2/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 2.2987 - acc: 0.3708\n",
      "Epoch 3/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 2.0465 - acc: 0.4135\n",
      "Epoch 4/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 1.7025 - acc: 0.5013\n",
      "Epoch 5/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 1.3283 - acc: 0.6172\n",
      "Epoch 6/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.9997 - acc: 0.7366\n",
      "Epoch 7/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.7416 - acc: 0.7766\n",
      "Epoch 8/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.4969 - acc: 0.8457\n",
      "Epoch 9/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.3789 - acc: 0.8917\n",
      "Epoch 10/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.3212 - acc: 0.9028\n",
      "Epoch 11/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.2388 - acc: 0.9258\n",
      "Epoch 12/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.2040 - acc: 0.9488\n",
      "Epoch 13/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.1147 - acc: 0.9727\n",
      "Epoch 14/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0580 - acc: 0.9795\n",
      "Epoch 15/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.1194 - acc: 0.9616\n",
      "Epoch 16/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0899 - acc: 0.9736\n",
      "Epoch 17/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0557 - acc: 0.9821\n",
      "Epoch 18/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0734 - acc: 0.9821\n",
      "Epoch 19/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0280 - acc: 0.9872\n",
      "Epoch 20/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0230 - acc: 0.9923\n",
      "Epoch 21/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0614 - acc: 0.9847\n",
      "Epoch 22/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0752 - acc: 0.9761\n",
      "Epoch 23/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0942 - acc: 0.9744\n",
      "Epoch 24/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.1215 - acc: 0.9633\n",
      "Epoch 25/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0748 - acc: 0.9804\n",
      "Epoch 26/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0491 - acc: 0.9864\n",
      "Epoch 27/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0464 - acc: 0.9898\n",
      "Epoch 28/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0718 - acc: 0.9795\n",
      "Epoch 29/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.1590 - acc: 0.9599\n",
      "Epoch 30/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.1521 - acc: 0.9531\n",
      "Epoch 31/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.1966 - acc: 0.9497\n",
      "Epoch 32/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.1383 - acc: 0.9616\n",
      "Epoch 33/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0827 - acc: 0.9753\n",
      "Epoch 34/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0679 - acc: 0.9795\n",
      "Epoch 35/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0727 - acc: 0.9761\n",
      "Epoch 36/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0414 - acc: 0.9864\n",
      "Epoch 37/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0187 - acc: 0.9923\n",
      "Epoch 38/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0132 - acc: 0.9915\n",
      "Epoch 39/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0133 - acc: 0.9923\n",
      "Epoch 40/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0092 - acc: 0.9949\n",
      "Epoch 41/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0091 - acc: 0.9923\n",
      "Epoch 42/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0075 - acc: 0.9949\n",
      "Epoch 43/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0088 - acc: 0.9932\n",
      "Epoch 44/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0082 - acc: 0.9949\n",
      "Epoch 45/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0078 - acc: 0.9940\n",
      "Epoch 46/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0073 - acc: 0.9940\n",
      "Epoch 47/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0084 - acc: 0.9932\n",
      "Epoch 48/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0074 - acc: 0.9949\n",
      "Epoch 49/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0074 - acc: 0.9949\n",
      "Epoch 50/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0076 - acc: 0.9923\n",
      "Epoch 51/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0076 - acc: 0.9949\n",
      "Epoch 52/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0077 - acc: 0.9949\n",
      "Epoch 53/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0079 - acc: 0.9957\n",
      "Epoch 54/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0075 - acc: 0.9940\n",
      "Epoch 55/100\n",
      "1173/1173 [==============================] - ETA: 0s - loss: 0.0068 - acc: 0.994 - 1s 1ms/step - loss: 0.0074 - acc: 0.9940\n",
      "Epoch 56/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0084 - acc: 0.9949\n",
      "Epoch 57/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0082 - acc: 0.9957\n",
      "Epoch 58/100\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0077 - acc: 0.9949\n",
      "Epoch 59/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0071 - acc: 0.9940\n",
      "Epoch 60/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0081 - acc: 0.9923\n",
      "Epoch 61/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0076 - acc: 0.9932\n",
      "Epoch 62/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0080 - acc: 0.9949\n",
      "Epoch 63/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0073 - acc: 0.9940\n",
      "Epoch 64/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0075 - acc: 0.9949\n",
      "Epoch 65/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0081 - acc: 0.9949\n",
      "Epoch 66/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0072 - acc: 0.9949\n",
      "Epoch 67/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0078 - acc: 0.9932\n",
      "Epoch 68/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0074 - acc: 0.9940\n",
      "Epoch 69/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0077 - acc: 0.9949\n",
      "Epoch 70/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0085 - acc: 0.9932\n",
      "Epoch 71/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0073 - acc: 0.9949\n",
      "Epoch 72/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0070 - acc: 0.9949\n",
      "Epoch 73/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0078 - acc: 0.9932\n",
      "Epoch 74/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0073 - acc: 0.9940\n",
      "Epoch 75/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0074 - acc: 0.9949\n",
      "Epoch 76/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0072 - acc: 0.9940\n",
      "Epoch 77/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0082 - acc: 0.9949\n",
      "Epoch 78/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0084 - acc: 0.9949\n",
      "Epoch 79/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0084 - acc: 0.9957\n",
      "Epoch 80/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0078 - acc: 0.9940\n",
      "Epoch 81/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0076 - acc: 0.9932\n",
      "Epoch 82/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0073 - acc: 0.9923\n",
      "Epoch 83/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0079 - acc: 0.9957\n",
      "Epoch 84/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0082 - acc: 0.9923\n",
      "Epoch 85/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0075 - acc: 0.9957\n",
      "Epoch 86/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0071 - acc: 0.9957\n",
      "Epoch 87/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0080 - acc: 0.9940\n",
      "Epoch 88/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0071 - acc: 0.9957\n",
      "Epoch 89/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0078 - acc: 0.9940\n",
      "Epoch 90/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0070 - acc: 0.9949\n",
      "Epoch 91/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0071 - acc: 0.9966\n",
      "Epoch 92/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0080 - acc: 0.9940\n",
      "Epoch 93/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0074 - acc: 0.9940\n",
      "Epoch 94/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0081 - acc: 0.9957\n",
      "Epoch 95/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0076 - acc: 0.9957\n",
      "Epoch 96/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0076 - acc: 0.9940\n",
      "Epoch 97/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0074 - acc: 0.9957\n",
      "Epoch 98/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0073 - acc: 0.9957\n",
      "Epoch 99/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0071 - acc: 0.9957\n",
      "Epoch 100/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0069 - acc: 0.9957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1581b04cb00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_1.fit(x=input_context,y=aspect, epochs=100,batch_size=64)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "cnn_model_1_json = cnn_model_1.to_json()\n",
    "with open(\"cnn_model_1.json\", \"w\") as json_file:\n",
    "    json_file.write(cnn_model_1_json)\n",
    "# serialize weights to HDF5\n",
    "cnn_model_1.save_weights(\"cnn_model_1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After 50 epochs: loss: 0.0078 - acc: 0.9949"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN +LSTM +ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model3(dropout,learning_rate,em,em_dim,lstm_out, n_hidden_layer,em_trainable_flag,n_filters=150):\n",
    "   \n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    context= embedding(input_context)\n",
    "    \n",
    "    c=Dropout(0.5)(context)\n",
    "    \n",
    "    c=Conv1D(n_filters,kernel_size=3,activation='relu')(context)\n",
    "    \n",
    "    m = Bidirectional(LSTM(150,return_sequences=True,recurrent_dropout=dropout))(c)\n",
    "    \n",
    "    a=  Attention()(m)\n",
    "                    \n",
    "    x=Dense(int((2*lstm_out+27)/2),activation='relu')(a)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "   \n",
    "    model= Model(inputs=input_context,outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Context (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 11, 300)           997200    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 9, 150)            135150    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 9, 300)            361200    \n",
      "_________________________________________________________________\n",
      "attention_2 (Attention)      (None, 300)               309       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 313)               94213     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 27)                8478      \n",
      "=================================================================\n",
      "Total params: 1,596,550\n",
      "Trainable params: 599,350\n",
      "Non-trainable params: 997,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn_model_3 = define_model3(dropout=0.2,\n",
    "                     learning_rate=0.001,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1343/1343 [==============================] - 3s 2ms/step - loss: 2.9045 - acc: 0.3053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x254603daba8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_3.fit(x=input_context,y=aspect, epochs=1,batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  96, 1129,   72, ...,    0,    0,    0],\n",
       "       [ 336,   28, 1680, ...,    0,    0,    0],\n",
       "       [ 347,  299,  359, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [3264, 1754,    0, ...,    0,    0,    0],\n",
       "       [ 247,  211,  131, ...,    0,    0,    0],\n",
       "       [ 485,   87,   53, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After 50 epochs: loss: 0.0075 - acc: 0.9940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "cnn_model_3_json = cnn_model_3.to_json()\n",
    "with open(\"cnn_model_3.json\", \"w\") as json_file:\n",
    "    json_file.write(cnn_model_3_json)\n",
    "# serialize weights to HDF5\n",
    "cnn_model_3.save_weights(\"cnn_model_3.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 CHANNEL CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model2(dropout,learning_rate,em,em_dim,em_trainable_flag,n_filters=100):\n",
    "    \n",
    "\n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    inputs1= embedding(input_context)\n",
    "    \n",
    "    \n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=3, activation='relu')(inputs1)\n",
    "    drop1 = Dropout(dropout)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    conv2 = Conv1D(filters=n_filters, kernel_size=4, activation='relu')(inputs1)\n",
    "    drop2 = Dropout(dropout)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    \n",
    "    conv3 = Conv1D(filters=n_filters, kernel_size=5, activation='relu')(inputs1)\n",
    "    drop3 = Dropout(dropout)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    \n",
    "    conv4 = Conv1D(filters=n_filters, kernel_size=2, activation='relu')(inputs1)\n",
    "    drop4 = Dropout(dropout)(conv4)\n",
    "    pool4 = MaxPooling1D(pool_size=2)(drop4)\n",
    "    flat4 = Flatten()(pool4)\n",
    "    \n",
    "    conv5 = Conv1D(filters=n_filters, kernel_size=6, activation='relu')(inputs1)\n",
    "    drop5 = Dropout(dropout)(conv5)\n",
    "    pool5 = MaxPooling1D(pool_size=2)(drop5)\n",
    "    flat5 = Flatten()(pool5)\n",
    "    # merge\n",
    "    \n",
    "    merged = concatenate([flat1, flat2, flat3,flat4,flat5])\n",
    "    # interpretation\n",
    "                   \n",
    "    x=Dense(400,activation='relu')(merged)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "   \n",
    "    model= Model(inputs=input_context,outputs=out)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Context (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 11, 300)      997200      Context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 9, 100)       90100       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 8, 100)       120100      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 7, 100)       150100      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 10, 100)      60100       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 6, 100)       180100      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 9, 100)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 8, 100)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 7, 100)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 10, 100)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 6, 100)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 4, 100)       0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 4, 100)       0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 3, 100)       0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 5, 100)       0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 3, 100)       0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 400)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 400)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 300)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 500)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 300)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1900)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 400)          760400      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 27)           10827       dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,368,927\n",
      "Trainable params: 1,371,727\n",
      "Non-trainable params: 997,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn_model_2 = cnn_model2(dropout=0.5,\n",
    "                     learning_rate=0.001,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1173/1173 [==============================] - 4s 3ms/step - loss: 2.5779 - acc: 0.3529\n",
      "Epoch 2/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 1.9138 - acc: 0.4791\n",
      "Epoch 3/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 1.3330 - acc: 0.6479A: 0s - loss: 1.3158 - acc: \n",
      "Epoch 4/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.8359 - acc: 0.8031\n",
      "Epoch 5/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.4989 - acc: 0.8934\n",
      "Epoch 6/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.2781 - acc: 0.9412\n",
      "Epoch 7/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.1506 - acc: 0.9770\n",
      "Epoch 8/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0916 - acc: 0.9906\n",
      "Epoch 9/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0859 - acc: 0.9906\n",
      "Epoch 10/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0642 - acc: 0.9889A: 0s - loss: 0.0651 - acc: \n",
      "Epoch 11/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0556 - acc: 0.9923\n",
      "Epoch 12/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0451 - acc: 0.9949\n",
      "Epoch 13/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0409 - acc: 0.9940\n",
      "Epoch 14/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0487 - acc: 0.9940\n",
      "Epoch 15/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0539 - acc: 0.9923\n",
      "Epoch 16/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0300 - acc: 0.9940\n",
      "Epoch 17/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0387 - acc: 0.9940\n",
      "Epoch 18/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0241 - acc: 0.9940\n",
      "Epoch 19/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0210 - acc: 0.9949\n",
      "Epoch 20/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0316 - acc: 0.9957\n",
      "Epoch 21/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0374 - acc: 0.9940\n",
      "Epoch 22/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0342 - acc: 0.9932\n",
      "Epoch 23/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0447 - acc: 0.9923\n",
      "Epoch 24/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0470 - acc: 0.9923A: 1s - loss: 0.01\n",
      "Epoch 25/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0322 - acc: 0.9957A: 0s - loss: 0.0197 - acc: \n",
      "Epoch 26/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0336 - acc: 0.9932\n",
      "Epoch 27/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0281 - acc: 0.9940\n",
      "Epoch 28/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0275 - acc: 0.9957\n",
      "Epoch 29/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0260 - acc: 0.9923\n",
      "Epoch 30/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0291 - acc: 0.9949A: 0s - loss: 0.0295 - acc: 0.994\n",
      "Epoch 31/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0143 - acc: 0.9940\n",
      "Epoch 32/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0236 - acc: 0.9957\n",
      "Epoch 33/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0322 - acc: 0.9940A: 0s - loss: 0.0340 - acc: 0.99\n",
      "Epoch 34/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0270 - acc: 0.9940\n",
      "Epoch 35/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0133 - acc: 0.9974\n",
      "Epoch 36/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0100 - acc: 0.9966\n",
      "Epoch 37/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0247 - acc: 0.9940\n",
      "Epoch 38/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0252 - acc: 0.9940\n",
      "Epoch 39/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0232 - acc: 0.9932\n",
      "Epoch 40/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0139 - acc: 0.9949\n",
      "Epoch 41/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0235 - acc: 0.9940\n",
      "Epoch 42/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0224 - acc: 0.9940\n",
      "Epoch 43/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0209 - acc: 0.9966\n",
      "Epoch 44/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0129 - acc: 0.9957\n",
      "Epoch 45/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0121 - acc: 0.9957\n",
      "Epoch 46/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0107 - acc: 0.9957\n",
      "Epoch 47/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0201 - acc: 0.9932A: 1s - loss: 0.0\n",
      "Epoch 48/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0190 - acc: 0.9915\n",
      "Epoch 49/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0146 - acc: 0.9940\n",
      "Epoch 50/50\n",
      "1173/1173 [==============================] - 2s 1ms/step - loss: 0.0139 - acc: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229d1eefac8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_2.fit(x=input_context,y=aspect, epochs=100,batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "cnn_model_2_json = cnn_model_2.to_json()\n",
    "with open(\"cnn_model_2.json\", \"w\") as json_file:\n",
    "    json_file.write(cnn_model_2_json)\n",
    "# serialize weights to HDF5\n",
    "cnn_model_2.save_weights(\"cnn_model_2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After 50 epochs:  loss: 0.0214 - acc: 0.9957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_4(dropout,learning_rate,em,em_dim,em_trainable_flag,num_filters=100):\n",
    "    \n",
    "  \n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "\n",
    "    input_context= Input(shape=(max_length,),name='Context')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)\n",
    "    inputs1= embedding(input_context)\n",
    "    \n",
    "    reshape = Reshape((max_length,em_dim,1))(inputs1)\n",
    "\n",
    "    c0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], em_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    c1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], em_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    c2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], em_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "    DP0 = MaxPooling2D(pool_size=(3, 1), strides=(1,1), padding='valid')(c0)\n",
    "    DP1 = MaxPooling2D(pool_size=(4, 1), strides=(1,1), padding='valid')(c1)\n",
    "    DP2 = MaxPooling2D(pool_size=(5, 1), strides=(1,1), padding='valid')(c2)\n",
    "\n",
    "    CT = Concatenate(axis=1)([DP0, DP1, DP2])\n",
    "\n",
    "    flatten = Flatten()(CT)\n",
    "    dropout = Dropout(dropout)(flatten)\n",
    "\n",
    "    x=Dense(400,activation='relu')(dropout)\n",
    "        \n",
    "    out=Dense(27,activation='softmax')(x)\n",
    "   \n",
    "    model= Model(inputs=input_context,outputs=out)\n",
    "    \n",
    "\n",
    "    \n",
    "    optimizer = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    #optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Context (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 11, 300)      997200      Context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 11, 300, 1)   0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 9, 1, 100)    90100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 1, 100)    120100      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 1, 100)    150100      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 7, 1, 100)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 5, 1, 100)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 3, 1, 100)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 15, 1, 100)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1500)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1500)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 400)          600400      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 27)           10827       dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,968,727\n",
      "Trainable params: 971,527\n",
      "Non-trainable params: 997,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn_model_4 = cnn_4(dropout=0.2,\n",
    "                     learning_rate=0.001,\n",
    "                     em='embedding_matrix',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1173/1173 [==============================] - 2s 2ms/step - loss: 2.8760 - acc: 0.3120\n",
      "Epoch 2/100\n",
      "1173/1173 [==============================] - 1s 736us/step - loss: 2.5601 - acc: 0.3725\n",
      "Epoch 3/100\n",
      "1173/1173 [==============================] - 1s 725us/step - loss: 2.4505 - acc: 0.3743\n",
      "Epoch 4/100\n",
      "1173/1173 [==============================] - 1s 840us/step - loss: 2.3569 - acc: 0.3768\n",
      "Epoch 5/100\n",
      "1173/1173 [==============================] - 1s 887us/step - loss: 2.2672 - acc: 0.3811\n",
      "Epoch 6/100\n",
      "1173/1173 [==============================] - 1s 804us/step - loss: 2.1626 - acc: 0.4024\n",
      "Epoch 7/100\n",
      "1173/1173 [==============================] - 1s 752us/step - loss: 2.0597 - acc: 0.4186\n",
      "Epoch 8/100\n",
      "1173/1173 [==============================] - 1s 712us/step - loss: 1.9546 - acc: 0.4595\n",
      "Epoch 9/100\n",
      "1173/1173 [==============================] - 1s 782us/step - loss: 1.8488 - acc: 0.4876\n",
      "Epoch 10/100\n",
      "1173/1173 [==============================] - 1s 788us/step - loss: 1.7447 - acc: 0.5200\n",
      "Epoch 11/100\n",
      "1173/1173 [==============================] - 1s 699us/step - loss: 1.6409 - acc: 0.5550\n",
      "Epoch 12/100\n",
      "1173/1173 [==============================] - 1s 679us/step - loss: 1.5398 - acc: 0.5678\n",
      "Epoch 13/100\n",
      "1173/1173 [==============================] - 1s 699us/step - loss: 1.4374 - acc: 0.6198\n",
      "Epoch 14/100\n",
      "1173/1173 [==============================] - 1s 815us/step - loss: 1.3388 - acc: 0.6607\n",
      "Epoch 15/100\n",
      "1173/1173 [==============================] - 1s 842us/step - loss: 1.2573 - acc: 0.6786 0s - loss: 1.2658 - acc: 0.67\n",
      "Epoch 16/100\n",
      "1173/1173 [==============================] - 1s 772us/step - loss: 1.1700 - acc: 0.7067\n",
      "Epoch 17/100\n",
      "1173/1173 [==============================] - 1s 875us/step - loss: 1.0688 - acc: 0.7494\n",
      "Epoch 18/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.9924 - acc: 0.7690\n",
      "Epoch 19/100\n",
      "1173/1173 [==============================] - 1s 882us/step - loss: 0.9093 - acc: 0.8065\n",
      "Epoch 20/100\n",
      "1173/1173 [==============================] - 1s 778us/step - loss: 0.8370 - acc: 0.8252\n",
      "Epoch 21/100\n",
      "1173/1173 [==============================] - 1s 744us/step - loss: 0.7630 - acc: 0.8431\n",
      "Epoch 22/100\n",
      "1173/1173 [==============================] - 1s 898us/step - loss: 0.6930 - acc: 0.8755\n",
      "Epoch 23/100\n",
      "1173/1173 [==============================] - 1s 725us/step - loss: 0.6408 - acc: 0.8832\n",
      "Epoch 24/100\n",
      "1173/1173 [==============================] - 1s 781us/step - loss: 0.5867 - acc: 0.9011\n",
      "Epoch 25/100\n",
      "1173/1173 [==============================] - 1s 755us/step - loss: 0.5333 - acc: 0.9054\n",
      "Epoch 26/100\n",
      "1173/1173 [==============================] - 1s 867us/step - loss: 0.4877 - acc: 0.9224\n",
      "Epoch 27/100\n",
      "1173/1173 [==============================] - 1s 785us/step - loss: 0.4442 - acc: 0.9403\n",
      "Epoch 28/100\n",
      "1173/1173 [==============================] - 1s 744us/step - loss: 0.4001 - acc: 0.9514\n",
      "Epoch 29/100\n",
      "1173/1173 [==============================] - 1s 732us/step - loss: 0.3719 - acc: 0.9514\n",
      "Epoch 30/100\n",
      "1173/1173 [==============================] - 1s 859us/step - loss: 0.3331 - acc: 0.9650\n",
      "Epoch 31/100\n",
      "1173/1173 [==============================] - 1s 712us/step - loss: 0.3078 - acc: 0.9616\n",
      "Epoch 32/100\n",
      "1173/1173 [==============================] - 1s 708us/step - loss: 0.2834 - acc: 0.9710\n",
      "Epoch 33/100\n",
      "1173/1173 [==============================] - 1s 710us/step - loss: 0.2663 - acc: 0.9693\n",
      "Epoch 34/100\n",
      "1173/1173 [==============================] - 1s 706us/step - loss: 0.2363 - acc: 0.9753\n",
      "Epoch 35/100\n",
      "1173/1173 [==============================] - 1s 767us/step - loss: 0.2216 - acc: 0.9753\n",
      "Epoch 36/100\n",
      "1173/1173 [==============================] - 1s 696us/step - loss: 0.2067 - acc: 0.9744\n",
      "Epoch 37/100\n",
      "1173/1173 [==============================] - 1s 719us/step - loss: 0.1986 - acc: 0.9787\n",
      "Epoch 38/100\n",
      "1173/1173 [==============================] - 1s 710us/step - loss: 0.1792 - acc: 0.9847\n",
      "Epoch 39/100\n",
      "1173/1173 [==============================] - 1s 703us/step - loss: 0.1616 - acc: 0.9855\n",
      "Epoch 40/100\n",
      "1173/1173 [==============================] - 1s 867us/step - loss: 0.1526 - acc: 0.9898\n",
      "Epoch 41/100\n",
      "1173/1173 [==============================] - 1s 766us/step - loss: 0.1451 - acc: 0.9872\n",
      "Epoch 42/100\n",
      "1173/1173 [==============================] - 1s 746us/step - loss: 0.1344 - acc: 0.9872\n",
      "Epoch 43/100\n",
      "1173/1173 [==============================] - 1s 849us/step - loss: 0.1261 - acc: 0.9872\n",
      "Epoch 44/100\n",
      "1173/1173 [==============================] - 1s 836us/step - loss: 0.1248 - acc: 0.9915\n",
      "Epoch 45/100\n",
      "1173/1173 [==============================] - 1s 732us/step - loss: 0.1113 - acc: 0.9906\n",
      "Epoch 46/100\n",
      "1173/1173 [==============================] - 1s 718us/step - loss: 0.1072 - acc: 0.9898\n",
      "Epoch 47/100\n",
      "1173/1173 [==============================] - 1s 706us/step - loss: 0.1017 - acc: 0.9898\n",
      "Epoch 48/100\n",
      "1173/1173 [==============================] - 1s 867us/step - loss: 0.0948 - acc: 0.9906\n",
      "Epoch 49/100\n",
      "1173/1173 [==============================] - 1s 775us/step - loss: 0.0872 - acc: 0.9915\n",
      "Epoch 50/100\n",
      "1173/1173 [==============================] - 1s 780us/step - loss: 0.0915 - acc: 0.9898\n",
      "Epoch 51/100\n",
      "1173/1173 [==============================] - 1s 871us/step - loss: 0.0839 - acc: 0.9915\n",
      "Epoch 52/100\n",
      "1173/1173 [==============================] - 1s 971us/step - loss: 0.0741 - acc: 0.9923\n",
      "Epoch 53/100\n",
      "1173/1173 [==============================] - 1s 809us/step - loss: 0.0740 - acc: 0.9915\n",
      "Epoch 54/100\n",
      "1173/1173 [==============================] - 1s 717us/step - loss: 0.0699 - acc: 0.9915\n",
      "Epoch 55/100\n",
      "1173/1173 [==============================] - 1s 789us/step - loss: 0.0653 - acc: 0.9923\n",
      "Epoch 56/100\n",
      "1173/1173 [==============================] - 1s 767us/step - loss: 0.0604 - acc: 0.9940\n",
      "Epoch 57/100\n",
      "1173/1173 [==============================] - 1s 709us/step - loss: 0.0598 - acc: 0.9923\n",
      "Epoch 58/100\n",
      "1173/1173 [==============================] - 1s 721us/step - loss: 0.0570 - acc: 0.9932\n",
      "Epoch 59/100\n",
      "1173/1173 [==============================] - 1s 742us/step - loss: 0.0558 - acc: 0.9923\n",
      "Epoch 60/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0525 - acc: 0.9932\n",
      "Epoch 61/100\n",
      "1173/1173 [==============================] - 1s 1ms/step - loss: 0.0552 - acc: 0.9906\n",
      "Epoch 62/100\n",
      "1173/1173 [==============================] - 1s 812us/step - loss: 0.0494 - acc: 0.9940 1s - loss: 0.0358 - \n",
      "Epoch 63/100\n",
      "1173/1173 [==============================] - 1s 877us/step - loss: 0.0500 - acc: 0.9949\n",
      "Epoch 64/100\n",
      "1173/1173 [==============================] - 1s 679us/step - loss: 0.0502 - acc: 0.9940\n",
      "Epoch 65/100\n",
      "1173/1173 [==============================] - 1s 794us/step - loss: 0.0465 - acc: 0.9940\n",
      "Epoch 66/100\n",
      "1173/1173 [==============================] - 1s 762us/step - loss: 0.0471 - acc: 0.9898\n",
      "Epoch 67/100\n",
      "1173/1173 [==============================] - 1s 697us/step - loss: 0.0444 - acc: 0.9923\n",
      "Epoch 68/100\n",
      "1173/1173 [==============================] - 1s 683us/step - loss: 0.0414 - acc: 0.9957\n",
      "Epoch 69/100\n",
      "1173/1173 [==============================] - 1s 692us/step - loss: 0.0406 - acc: 0.9923\n",
      "Epoch 70/100\n",
      "1173/1173 [==============================] - 1s 692us/step - loss: 0.0394 - acc: 0.9932\n",
      "Epoch 71/100\n",
      "1173/1173 [==============================] - 1s 736us/step - loss: 0.0384 - acc: 0.9949\n",
      "Epoch 72/100\n",
      "1173/1173 [==============================] - 1s 794us/step - loss: 0.0413 - acc: 0.9932\n",
      "Epoch 73/100\n",
      "1173/1173 [==============================] - 1s 679us/step - loss: 0.0373 - acc: 0.9949\n",
      "Epoch 74/100\n",
      "1173/1173 [==============================] - 1s 698us/step - loss: 0.0326 - acc: 0.9949\n",
      "Epoch 75/100\n",
      "1173/1173 [==============================] - 1s 703us/step - loss: 0.0372 - acc: 0.9949\n",
      "Epoch 76/100\n",
      "1173/1173 [==============================] - 1s 685us/step - loss: 0.0315 - acc: 0.9949\n",
      "Epoch 77/100\n",
      "1173/1173 [==============================] - 1s 781us/step - loss: 0.0360 - acc: 0.9940\n",
      "Epoch 78/100\n",
      "1173/1173 [==============================] - 1s 714us/step - loss: 0.0325 - acc: 0.9940\n",
      "Epoch 79/100\n",
      "1173/1173 [==============================] - 1s 842us/step - loss: 0.0307 - acc: 0.9940\n",
      "Epoch 80/100\n",
      "1173/1173 [==============================] - 1s 695us/step - loss: 0.0298 - acc: 0.9932\n",
      "Epoch 81/100\n",
      "1173/1173 [==============================] - 1s 684us/step - loss: 0.0297 - acc: 0.9949\n",
      "Epoch 82/100\n",
      "1173/1173 [==============================] - 1s 724us/step - loss: 0.0370 - acc: 0.9940\n",
      "Epoch 83/100\n",
      "1173/1173 [==============================] - 1s 708us/step - loss: 0.0300 - acc: 0.9940\n",
      "Epoch 84/100\n",
      "1173/1173 [==============================] - 1s 688us/step - loss: 0.0276 - acc: 0.9949\n",
      "Epoch 85/100\n",
      "1173/1173 [==============================] - 1s 700us/step - loss: 0.0310 - acc: 0.9923\n",
      "Epoch 86/100\n",
      "1173/1173 [==============================] - 1s 666us/step - loss: 0.0318 - acc: 0.9915\n",
      "Epoch 87/100\n",
      "1173/1173 [==============================] - 1s 685us/step - loss: 0.0287 - acc: 0.9940\n",
      "Epoch 88/100\n",
      "1173/1173 [==============================] - 1s 786us/step - loss: 0.0268 - acc: 0.9932\n",
      "Epoch 89/100\n",
      "1173/1173 [==============================] - 1s 681us/step - loss: 0.0318 - acc: 0.9923\n",
      "Epoch 90/100\n",
      "1173/1173 [==============================] - 1s 690us/step - loss: 0.0246 - acc: 0.9940\n",
      "Epoch 91/100\n",
      "1173/1173 [==============================] - 1s 683us/step - loss: 0.0253 - acc: 0.9949\n",
      "Epoch 92/100\n",
      "1173/1173 [==============================] - 1s 710us/step - loss: 0.0281 - acc: 0.9940\n",
      "Epoch 93/100\n",
      "1173/1173 [==============================] - 1s 684us/step - loss: 0.0247 - acc: 0.9923\n",
      "Epoch 94/100\n",
      "1173/1173 [==============================] - 1s 686us/step - loss: 0.0248 - acc: 0.9949\n",
      "Epoch 95/100\n",
      "1173/1173 [==============================] - 1s 686us/step - loss: 0.0230 - acc: 0.9957\n",
      "Epoch 96/100\n",
      "1173/1173 [==============================] - 1s 669us/step - loss: 0.0228 - acc: 0.9949\n",
      "Epoch 97/100\n",
      "1173/1173 [==============================] - 1s 698us/step - loss: 0.0260 - acc: 0.9923\n",
      "Epoch 98/100\n",
      "1173/1173 [==============================] - 1s 693us/step - loss: 0.0247 - acc: 0.9932\n",
      "Epoch 99/100\n",
      "1173/1173 [==============================] - 1s 692us/step - loss: 0.0267 - acc: 0.9915\n",
      "Epoch 100/100\n",
      "1173/1173 [==============================] - 1s 688us/step - loss: 0.0321 - acc: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1582ec91ba8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_4.fit(x=input_context,y=aspect, epochs=100,batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After 50 epochs:  loss: 0.0837 - acc: 0.9906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "cnn_model_4_json = cnn_model_4.to_json()\n",
    "with open(\"cnn_model_4.json\", \"w\") as json_file:\n",
    "    json_file.write(cnn_model_4_json)\n",
    "# serialize weights to HDF5\n",
    "cnn_model_4.save_weights(\"cnn_model_4.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
